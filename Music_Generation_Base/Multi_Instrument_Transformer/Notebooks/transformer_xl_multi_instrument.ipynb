{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_xl_final_multi_lakh.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgErROWE11qg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import music21\n",
        "import os\n",
        "#import midifile \n",
        "# pre_process\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from enum import Enum\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import *\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "#import modules\n",
        "#import XL_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JRlvMEHDGAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#specifying data paths \n",
        "path = 'debussy'\n",
        "\n",
        "BPB = 4 # beats per bar\n",
        "TIMESIG = f'{BPB}/4' # default time signature\n",
        "PIANO_RANGE = (21, 108)\n",
        "NOTE_RANGE = (1,127)\n",
        "VALTSEP = -1 # separator value for numpy encoding\n",
        "VALTCONT = -2 # numpy value for TCONT - needed for compressing chord array\n",
        "\n",
        "SAMPLE_FREQ = 4\n",
        "NOTE_SIZE = 128\n",
        "DUR_SIZE = (10*BPB*SAMPLE_FREQ)+1 # Max length - 8 bars. Or 16 beats/quarternotes\n",
        "MAX_NOTE_DUR = (8*BPB*SAMPLE_FREQ)\n",
        "\n",
        "\n",
        "#tokenizing\n",
        "BOS = 'xxbos'\n",
        "PAD = 'xxpad'\n",
        "EOS = 'xxeos'\n",
        "#MASK = 'xxmask' # Used for BERT masked language modeling. \n",
        "#CSEQ = 'xxcseq' # Used for Seq2Seq translation - denotes start of chord sequence\n",
        "#MSEQ = 'xxmseq' # Used for Seq2Seq translation - denotes start of melody sequence\n",
        "#S2SCLS = 'xxs2scls' # deprecated\n",
        "#NSCLS = 'xxnscls' # deprecated\n",
        "SEP = 'xxsep'\n",
        "IN = 'xxni'     #null instrument\n",
        "\n",
        "SPECIAL_TOKS = [BOS, PAD, EOS, SEP,IN]\n",
        "\n",
        "NOTE_TOKS = [f'n{i}' for i in range(NOTE_SIZE)] \n",
        "DUR_TOKS = [f'd{i}' for i in range(DUR_SIZE)]\n",
        "NOTE_START, NOTE_END = NOTE_TOKS[0], NOTE_TOKS[-1]\n",
        "DUR_START, DUR_END = DUR_TOKS[0], DUR_TOKS[-1]\n",
        "\n",
        "MTEMPO_SIZE = 10\n",
        "MTEMPO_OFF = 'mt0'\n",
        "MTEMPO_TOKS = [f'mt{i}' for i in range(MTEMPO_SIZE)]\n",
        "\n",
        "SEQType = Enum('SEQType', 'Mask, Sentence, Melody, Chords, Empty')\n",
        "\n",
        "ACCEP_INS = dict()\n",
        "ACCEP_INS['Piano'] = 0 \n",
        "ACCEP_INS['Acoustic Bass'] = 1\n",
        "ACCEP_INS['Acoustic Guitar'] = 2 \n",
        "ACCEP_INS['Violin'] = 3 \n",
        "ACCEP_INS['Electric Guitar'] = 4 \n",
        "ACCEP_INS['Electric Bass'] = 5 \n",
        "ACCEP_INS['Saxophone'] = 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OesYYo1qDPX9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "d87ee057-e2eb-4fa3-873c-aecc820dd3d7"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Sep  5 11:16:20 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    25W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRpRWOGgDajP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "152f49d1-ed9b-412a-ef24-d70e9082d19c"
      },
      "source": [
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.4 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slksmnTo2HiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from enum import Enum\n",
        "import music21\n",
        "\n",
        "PIANO_TYPES = list(range(24)) + list(range(80, 96)) # Piano, Synths\n",
        "PLUCK_TYPES = list(range(24, 40)) + list(range(104, 112)) # Guitar, Bass, Ethnic\n",
        "BRIGHT_TYPES = list(range(40, 56)) + list(range(56, 80))\n",
        "\n",
        "PIANO_RANGE = (21, 109) # https://en.wikipedia.org/wiki/Scientific_pitch_notation\n",
        "\n",
        "\n",
        "#Using enums in python\n",
        "class Track(Enum):\n",
        "    PIANO = 0 # discrete instruments - keyboard, woodwinds\n",
        "    PLUCK = 1 # continuous instruments with pitch bend: violin, trombone, synths\n",
        "    BRIGHT = 2\n",
        "    PERC = 3\n",
        "    UNDEF = 4\n",
        "    \n",
        "ype2inst = {\n",
        "    # use print_music21_instruments() to see supported types\n",
        "    Track.PIANO: 0, # Piano\n",
        "    Track.PLUCK: 24, # Guitar\n",
        "    Track.BRIGHT: 40, # Violin\n",
        "    Track.PERC: 114, # Steel Drum\n",
        "}\n",
        "\n",
        "# INFO_TYPES = set(['TIME_SIGNATURE', 'KEY_SIGNATURE'])\n",
        "INFO_TYPES = set(['TIME_SIGNATURE', 'KEY_SIGNATURE', 'SET_TEMPO'])\n",
        "\n",
        "def file2mf(fp):\n",
        "    mf = music21.midi.MidiFile()\n",
        "    if isinstance(fp, bytes):\n",
        "        mf.readstr(fp)\n",
        "    else:\n",
        "        mf.open(fp)\n",
        "        mf.read()\n",
        "        mf.close()\n",
        "    return mf\n",
        "\n",
        "def mf2stream(mf): return music21.midi.translate.midiFileToStream(mf)\n",
        "\n",
        "def is_empty_midi(fp):\n",
        "    if fp is None: return False\n",
        "    mf = file2mf(fp)\n",
        "    return not any([t.hasNotes() for t in mf.tracks])\n",
        "\n",
        "def num_piano_tracks(fp):\n",
        "    music_file = file2mf(fp)\n",
        "    note_tracks = [t for t in music_file.tracks if t.hasNotes() and get_track_type(t) == Track.PIANO]\n",
        "    return len(note_tracks)\n",
        "\n",
        "def is_channel(t, c_val):\n",
        "    return any([c == c_val for c in t.getChannels()])\n",
        "\n",
        "def track_sort(t): # sort by 1. variation of pitch, 2. number of notes\n",
        "    return len(unique_track_notes(t)), len(t.events)\n",
        "\n",
        "def is_piano_note(pitch):\n",
        "    return (pitch >= PIANO_RANGE[0]) and (pitch < PIANO_RANGE[1])\n",
        "\n",
        "def unique_track_notes(t):\n",
        "    return { e.pitch for e in t.events if e.pitch is not None }\n",
        "\n",
        "def compress_midi_file(fp, cutoff=6, min_variation=3, supported_types=set([Track.PIANO, Track.PLUCK, Track.BRIGHT])):\n",
        "    music_file = file2mf(fp)\n",
        "    \n",
        "    info_tracks = [t for t in music_file.tracks if not t.hasNotes()]\n",
        "    note_tracks = [t for t in music_file.tracks if t.hasNotes()]\n",
        "    \n",
        "    if len(note_tracks) > cutoff:\n",
        "        note_tracks = sorted(note_tracks, key=track_sort, reverse=True)\n",
        "        \n",
        "    supported_tracks = []\n",
        "    for idx,t in enumerate(note_tracks):\n",
        "        if len(supported_tracks) >= cutoff: break\n",
        "        track_type = get_track_type(t)\n",
        "        if track_type not in supported_types: continue\n",
        "        pitch_set = unique_track_notes(t)\n",
        "        if (len(pitch_set) < min_variation): continue # must have more than x unique notes\n",
        "        if not all(map(is_piano_note, pitch_set)): continue # must not contain midi notes outside of piano range\n",
        "#         if track_type == Track.UNDEF: print('Could not designate track:', fp, t)\n",
        "        change_track_instrument(t, type2inst[track_type])\n",
        "        supported_tracks.append(t)\n",
        "    if not supported_tracks: return None\n",
        "    music_file.tracks = info_tracks + supported_tracks\n",
        "    return music_file\n",
        "\n",
        "def get_track_type(t):\n",
        "    if is_channel(t, 10): return Track.PERC\n",
        "    i = get_track_instrument(t)\n",
        "    if i in PIANO_TYPES: return Track.PIANO\n",
        "    if i in PLUCK_TYPES: return Track.PLUCK\n",
        "    if i in BRIGHT_TYPES: return Track.BRIGHT\n",
        "    return Track.UNDEF\n",
        "\n",
        "def get_track_instrument(t):\n",
        "    for idx,e in enumerate(t.events):\n",
        "        if e.type == 'PROGRAM_CHANGE': return e.data\n",
        "    return None\n",
        "\n",
        "def change_track_instrument(t, value):\n",
        "    for idx,e in enumerate(t.events):\n",
        "        if e.type == 'PROGRAM_CHANGE': e.data = value\n",
        "\n",
        "def print_music21_instruments():\n",
        "    for i in range(200):\n",
        "        try: print(i, music21.instrument.instrumentFromMidiProgram(i))\n",
        "        except: pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew5brC192Jc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def file2stream(fp):\n",
        "    if isinstance(fp, music21.midi.MidiFile): return music21.midi.translate.midiFileToStream(fp)\n",
        "    return music21.converter.parse(fp)\n",
        "\n",
        "def npenc2stream(arr,rev_uniq_ins,bpm=120):\n",
        "    \"Converts numpy encoding to music21 stream\"\n",
        "    chordarr = npenc2chordarr(np.array(arr)) # 1.\n",
        "    return chordarr2stream(chordarr,rev_uniq_ins,bpm=bpm) # 2.\n",
        "\n",
        "# 2.\n",
        "def stream2chordarr(s, note_size=NOTE_SIZE, sample_freq=SAMPLE_FREQ, max_note_dur=MAX_NOTE_DUR):\n",
        "    \"Converts music21.Stream to 1-hot numpy array\"\n",
        "    # assuming 4/4 time\n",
        "    # note x instrument x pitch\n",
        "    # FYI: midi middle C value=60\n",
        "    \n",
        "    # (AS) TODO: need to order by instruments most played and filter out percussion or include the channel\n",
        "    highest_time = max(s.flat.getElementsByClass('Note').highestTime, s.flat.getElementsByClass('Chord').highestTime)\n",
        "    maxTimeStep = round(highest_time * sample_freq)+1\n",
        "    score_arr = np.zeros((maxTimeStep, len(s.parts), NOTE_SIZE))\n",
        "\n",
        "    def note_data(pitch, note):\n",
        "        return (pitch.midi, int(round(note.offset*sample_freq)), int(round(note.duration.quarterLength*sample_freq)))\n",
        "    ins=dict()\n",
        "    for idx,part in enumerate(s.parts):\n",
        "        notes=[]\n",
        "        iterate = False\n",
        "        for elem in part.flat:\n",
        "            if isinstance(elem,music21.instrument.Instrument):\n",
        "                if elem.instrumentName in ACCEP_INS.keys():\n",
        "                    ins[idx] = elem.instrumentName \n",
        "                    iterate = True\n",
        "                else :\n",
        "                    break\n",
        "            if isinstance(elem, music21.note.Note):\n",
        "                notes.append(note_data(elem.pitch, elem))\n",
        "            if isinstance(elem, music21.chord.Chord):\n",
        "                for p in elem.pitches:\n",
        "                    notes.append(note_data(p, elem)) \n",
        "        # sort notes by offset (1), duration (2) so that hits are not overwritten and longer notes have priority\n",
        "        \n",
        "        notes_sorted = sorted(notes, key=lambda x: (x[1], x[2])) \n",
        "        if(iterate == True):\n",
        "            for n in notes_sorted:\n",
        "                if n is None: continue\n",
        "                pitch,offset,duration = n\n",
        "                if max_note_dur is not None and duration > max_note_dur: duration = max_note_dur\n",
        "                score_arr[offset,idx, pitch] = duration\n",
        "                score_arr[offset+1:offset+duration, idx, pitch] = VALTCONT      # Continue holding not\n",
        "        \n",
        "    return score_arr,ins\n",
        "\n",
        "def chordarr2npenc(chordarr, skip_last_rest=True):\n",
        "    # combine instruments\n",
        "    result = []\n",
        "    wait_count = 0\n",
        "    for idx,timestep in enumerate(chordarr):\n",
        "        flat_time = timestep2npenc(timestep)\n",
        "        if len(flat_time) == 0:\n",
        "            wait_count += 1\n",
        "        else:\n",
        "            # pitch, octave, duration, instrument\n",
        "            if wait_count > 0: result.append([VALTSEP, wait_count,-2])\n",
        "            result.extend(flat_time)\n",
        "            wait_count = 1\n",
        "    if wait_count > 0 and not skip_last_rest: result.append([VALTSEP, wait_count,-2])\n",
        "    return np.array(result,dtype = int)\n",
        "\n",
        "#   return np.array(result, dtype=int).reshape(-1, 2) # reshaping. Just in case result is empty\n",
        "\n",
        "# Note: not worrying about overlaps - as notes will still play. just look tied\n",
        "# http://web.mit.edu/music21/doc/moduleReference/moduleStream.html#music21.stream.Stream.getOverlaps\n",
        "def timestep2npenc(timestep, note_range=NOTE_RANGE, enc_type='full'):\n",
        "    # inst x pitch\n",
        "    notes = []\n",
        "    for i,n in zip(*timestep.nonzero()):\n",
        "        d = timestep[i,n]\n",
        "        if d < 0: continue # only supporting short duration encoding for now\n",
        "        if n < note_range[0] or n >= note_range[1]: continue # must be within midi range\n",
        "        notes.append([n,d,i])\n",
        "        \n",
        "    notes = sorted(notes, key=lambda x: x[0], reverse=True) # sort by note (highest to lowest)\n",
        "    \n",
        "    if enc_type is None: \n",
        "        # note, duration\n",
        "        return [n[:2] for n in notes] \n",
        "    if enc_type == 'parts':\n",
        "        # note, duration, part\n",
        "        return [n for n in notes]\n",
        "    if enc_type == 'full':\n",
        "        # note_class, duration , instrument\n",
        "        return [[n, d, i] for n,d,i in notes] \n",
        "\n",
        "###################Decoding Phase##########################################################\n",
        "\n",
        "# 1.\n",
        "def npenc2chordarr(npenc,note_size=NOTE_SIZE):\n",
        "    num_instruments = 1 if npenc.shape[1] <= 2 else npenc.max(axis=0)[-1]\n",
        "    max_len = npenc_len(npenc)\n",
        "    # score_arr = (steps, inst, note)\n",
        "    score_arr = np.zeros((max_len, num_instruments + 1, note_size))\n",
        "    \n",
        "    idx = 0\n",
        "    for step in npenc:\n",
        "        n,d,i = (step.tolist()+[0])[:3] # or n,d,i\n",
        "        if n < VALTSEP: continue # special token\n",
        "        if n == VALTSEP:\n",
        "            idx += d\n",
        "            continue\n",
        "        score_arr[idx,i,n] = d\n",
        "    return score_arr\n",
        "\n",
        "def npenc_len(npenc):\n",
        "    duration = 0\n",
        "    for t in npenc:\n",
        "        if t[0] == VALTSEP: duration += t[1]\n",
        "    return duration + 1\n",
        "\n",
        "\n",
        "# 2.\n",
        "def chordarr2stream(arr,rev_uniq_ins,sample_freq=SAMPLE_FREQ, bpm=120):\n",
        "    duration = music21.duration.Duration(1. / sample_freq)\n",
        "    stream = music21.stream.Score()\n",
        "    stream.append(music21.meter.TimeSignature(TIMESIG))\n",
        "    stream.append(music21.tempo.MetronomeMark(number=bpm))\n",
        "    stream.append(music21.key.KeySignature(0))\n",
        "    for inst in range(arr.shape[1]):\n",
        "        p = partarr2stream(arr[:,inst,:],inst,rev_uniq_ins,duration)\n",
        "        stream.append(p)\n",
        "    stream = stream.transpose(0)\n",
        "    return stream\n",
        "\n",
        "# 2b.\n",
        "def partarr2stream(partarr,inst,rev_uniq_ins,duration):\n",
        "    \"convert instrument part to music21 chords\"\n",
        "#    part = music21.stream.Part()\n",
        "#    part.append(music21.instrument.Piano())\n",
        "#    part_append_duration_notes(partarr, duration, part) # notes already have duration calculated\n",
        "    l = len(rev_uniq_ins) \n",
        "    inst = inst%l\n",
        "    part = music21.stream.Part()\n",
        "    if(rev_uniq_ins[inst] == 'Piano'):\n",
        "        part.append(music21.instrument.Piano())\n",
        "    elif(rev_uniq_ins[inst] == 'Trumpet'):\n",
        "        part.append(music21.instrument.Trumpet())\n",
        "    elif(rev_uniq_ins[inst] == 'Tenor Saxophone'):\n",
        "        part.append(music21.instrument.TenorSaxophone())\n",
        "    elif(rev_uniq_ins[inst] == 'Vibraphone'):\n",
        "        part.append(music21.instrument.Vibraphone())\n",
        "    elif(rev_uniq_ins[inst] == 'Baritone Saxophone'):\n",
        "        part.append(music21.instrument.BaritoneSaxophone())\n",
        "    elif(rev_uniq_ins[inst] == 'Acoustic Bass'):\n",
        "        part.append(music21.instrument.AcousticBass())\n",
        "    elif(rev_uniq_ins[inst] == 'Trombone'):\n",
        "        part.append(music21.instrument.Trombone())\n",
        "    elif(rev_uniq_ins[inst] == 'Flute'):\n",
        "        part.append(music21.instrument.Flute())\n",
        "    elif(rev_uniq_ins[inst] == 'Saxophone'):\n",
        "        part.append(music21.instrument.Saxophone())\n",
        "    elif(rev_uniq_ins[inst] == 'Electric Bass'):\n",
        "        part.append(music21.instrument.ElectricBass())\n",
        "    elif(rev_uniq_ins[inst] == 'Electric Guitar'):\n",
        "        part.append(music21.instrument.ElectricGuitar())\n",
        "    elif(rev_uniq_ins[inst] == 'Acoustic Guitar'):\n",
        "        part.append(music21.instrument.AcousticGuitar())\n",
        "    else:\n",
        "        part.append(music21.instrument.Piano())\n",
        "    part_append_duration_notes(partarr, duration, part)\n",
        "    \n",
        "\n",
        "    return part\n",
        "\n",
        "def part_append_duration_notes(partarr, duration, stream):\n",
        "    \"convert instrument part to music21 chords\"\n",
        "    for tidx,t in enumerate(partarr):\n",
        "        note_idxs = np.where(t > 0)[0] # filter out any negative values (continuous mode)\n",
        "        if len(note_idxs) == 0: continue\n",
        "        notes = []\n",
        "        for nidx in note_idxs:\n",
        "            note = music21.note.Note(nidx)\n",
        "            note.duration = music21.duration.Duration(partarr[tidx,nidx]*duration.quarterLength)\n",
        "            notes.append(note)\n",
        "        for g in group_notes_by_duration(notes):\n",
        "            if len(g) == 1:\n",
        "                stream.insert(tidx*duration.quarterLength, g[0])\n",
        "            else:\n",
        "                chord = music21.chord.Chord(g)\n",
        "                stream.insert(tidx*duration.quarterLength, chord)\n",
        "    return stream\n",
        "\n",
        "from itertools import groupby\n",
        "#  combining notes with different durations into a single chord may overwrite conflicting durations. Example: aylictal/still-waters-run-deep\n",
        "def group_notes_by_duration(notes):\n",
        "    \"separate notes into chord groups\"\n",
        "    keyfunc = lambda n: n.duration.quarterLength\n",
        "    notes = sorted(notes, key=keyfunc)\n",
        "    return [list(g) for k,g in groupby(notes, keyfunc)]\n",
        "\n",
        "\n",
        "# Midi -> npenc Conversion helpers\n",
        "def is_valid_npenc(npenc, note_range=PIANO_RANGE, max_dur=DUR_SIZE, \n",
        "                   min_notes=32, input_path=None, verbose=True):\n",
        "    if len(npenc) < min_notes:\n",
        "        if verbose: print('Sequence too short:', len(npenc), input_path)\n",
        "        return False\n",
        "    if (npenc[:,1] >= max_dur).any(): \n",
        "        if verbose: print(f'npenc exceeds max {max_dur} duration:', npenc[:,1].max(), input_path)\n",
        "        return False\n",
        "    # https://en.wikipedia.org/wiki/Scientific_pitch_notation - 88 key range - 21 = A0, 108 = C8\n",
        "    if ((npenc[...,0] > VALTSEP) & ((npenc[...,0] < note_range[0]) | (npenc[...,0] >= note_range[1]))).any(): \n",
        "        print(f'npenc out of piano note range {note_range}:', input_path)\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# seperates overlapping notes to different tracks\n",
        "def remove_overlaps(stream, separate_chords=True):\n",
        "    if not separate_chords:\n",
        "        return stream.flat.makeVoices().voicesToParts()\n",
        "    return separate_melody_chord(stream)\n",
        "\n",
        "# seperates notes and chords to different tracks\n",
        "def separate_melody_chord(stream):\n",
        "    new_stream = music21.stream.Score()\n",
        "    if stream.timeSignature: new_stream.append(stream.timeSignature)\n",
        "    new_stream.append(stream.metronomeMarkBoundaries()[0][-1])\n",
        "    if stream.keySignature: new_stream.append(stream.keySignature)\n",
        "    \n",
        "    melody_part = music21.stream.Part(stream.flat.getElementsByClass('Note'))\n",
        "    melody_part.insert(0, stream.getInstrument())\n",
        "    chord_part = music21.stream.Part(stream.flat.getElementsByClass('Chord'))\n",
        "    chord_part.insert(0, stream.getInstrument())\n",
        "    new_stream.append(melody_part)\n",
        "    new_stream.append(chord_part)\n",
        "    return new_stream\n",
        "    \n",
        " # processing functions for sanitizing data\n",
        "\n",
        "def compress_chordarr(chordarr):\n",
        "    return shorten_chordarr_rests(trim_chordarr_rests(chordarr))\n",
        "\n",
        "def trim_chordarr_rests(arr, max_rests=4, sample_freq=SAMPLE_FREQ):\n",
        "    # max rests is in quarter notes\n",
        "    # max 1 bar between song start and end\n",
        "    start_idx = 0\n",
        "    max_sample = max_rests*sample_freq\n",
        "    for idx,t in enumerate(arr):\n",
        "        if (t != 0).any(): break\n",
        "        start_idx = idx+1\n",
        "        \n",
        "    end_idx = 0\n",
        "    for idx,t in enumerate(reversed(arr)):\n",
        "        if (t != 0).any(): break\n",
        "        end_idx = idx+1\n",
        "    start_idx = start_idx - start_idx % max_sample\n",
        "    end_idx = end_idx - end_idx % max_sample\n",
        "#     if start_idx > 0 or end_idx > 0: print('Trimming rests. Start, end:', start_idx, len(arr)-end_idx, end_idx)\n",
        "    return arr[start_idx:(len(arr)-end_idx)]\n",
        "\n",
        "def shorten_chordarr_rests(arr, max_rests=8, sample_freq=SAMPLE_FREQ):\n",
        "    # max rests is in quarter notes\n",
        "    # max 2 bar pause\n",
        "    rest_count = 0\n",
        "    result = []\n",
        "    max_sample = max_rests*sample_freq\n",
        "    for timestep in arr:\n",
        "        if (timestep==0).all(): \n",
        "            rest_count += 1\n",
        "        else:\n",
        "            if rest_count > max_sample:\n",
        "#                 old_count = rest_count\n",
        "                rest_count = (rest_count % sample_freq) + max_sample\n",
        "#                 print(f'Compressing rests: {old_count} -> {rest_count}')\n",
        "            for i in range(rest_count): result.append(np.zeros(timestep.shape))\n",
        "            rest_count = 0\n",
        "            result.append(timestep)\n",
        "    for i in range(rest_count): result.append(np.zeros(timestep.shape))\n",
        "    return np.array(result)\n",
        "\n",
        "# sequence 2 sequence convenience functions\n",
        "\n",
        "def stream2npenc_parts(stream, sort_pitch=True):\n",
        "    chordarr = stream2chordarr(stream)\n",
        "    _,num_parts,_ = chordarr.shape\n",
        "    parts = [part_enc(chordarr, i) for i in range(num_parts)]\n",
        "    return sorted(parts, key=avg_pitch, reverse=True) if sort_pitch else parts\n",
        "\n",
        "def chordarr_combine_parts(parts):\n",
        "    max_ts = max([p.shape[0] for p in parts])\n",
        "    parts_padded = [pad_part_to(p, max_ts) for p in parts]\n",
        "    chordarr_comb = np.concatenate(parts_padded, axis=1)\n",
        "    return chordarr_comb\n",
        "\n",
        "def pad_part_to(p, target_size):\n",
        "    pad_width = ((0,target_size-p.shape[0]),(0,0),(0,0))\n",
        "    return np.pad(p, pad_width, 'constant')\n",
        "\n",
        "def part_enc(chordarr, part):\n",
        "    partarr = chordarr[:,part:part+1,:]\n",
        "    npenc = chordarr2npenc(partarr)\n",
        "    return npenc\n",
        "\n",
        "def avg_tempo(t, sep_idx=VALTSEP):\n",
        "    avg = t[t[:, 0] == sep_idx][:, 1].sum()/t.shape[0]\n",
        "    avg = int(round(avg/SAMPLE_FREQ))\n",
        "    return 'mt'+str(min(avg, MTEMPO_SIZE-1))\n",
        "\n",
        "def avg_pitch(t, sep_idx=VALTSEP):\n",
        "    return t[t[:, 0] > sep_idx][:, 0].mean()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sQ5xy212MlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embedding_lookup(lookup_table, x):\n",
        "    return tf.compat.v1.nn.embedding_lookup(lookup_table, x)\n",
        "\n",
        "\n",
        "def normal_embedding_lookup(x, n_token, d_embed, d_proj, initializer,\n",
        "                            proj_initializer, scope='normal_embed', **kwargs):\n",
        "    emb_scale = d_proj ** 0.5\n",
        "    with tf.compat.v1.variable_scope(scope):\n",
        "        lookup_table = tf.compat.v1.get_variable('lookup_table', [n_token, d_embed], initializer=initializer)\n",
        "        y = embedding_lookup(lookup_table, x)\n",
        "        if d_proj != d_embed:\n",
        "            proj_W = tf.compat.v1.get_variable('proj_W', [d_embed, d_proj], initializer=proj_initializer)\n",
        "            y = tf.einsum('ibe,ed->ibd', y, proj_W)\n",
        "        else:\n",
        "            proj_W = None\n",
        "        ret_params = [lookup_table, proj_W]\n",
        "    y *= emb_scale\n",
        "    return y, ret_params\n",
        "\n",
        "\n",
        "def normal_softmax(hidden, target, n_token, params, scope='normal_softmax', **kwargs):\n",
        "    def _logit(x, W, b, proj):\n",
        "        y = x\n",
        "        if proj is not None:\n",
        "            y = tf.einsum('ibd,ed->ibe', y, proj)\n",
        "        return tf.einsum('ibd,nd->ibn', y, W) + b\n",
        "\n",
        "    params_W, params_projs = params[0], params[1]\n",
        "\n",
        "    with tf.compat.v1.variable_scope(scope):\n",
        "        softmax_b = tf.compat.v1.get_variable('bias', [n_token], initializer=tf.zeros_initializer())\n",
        "        output = _logit(hidden, params_W, softmax_b, params_projs)\n",
        "        nll = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target, logits=output)\n",
        "    return nll, output\n",
        "\n",
        "\n",
        "def positional_embedding(pos_seq, inv_freq, bsz=None):\n",
        "    sinusoid_inp = tf.einsum('i,j->ij', pos_seq, inv_freq)\n",
        "    pos_emb = tf.concat([tf.sin(sinusoid_inp), tf.cos(sinusoid_inp)], -1)\n",
        "    if bsz is not None:\n",
        "        return tf.tile(pos_emb[:, None, :], [1, bsz, 1])\n",
        "    else:\n",
        "        return pos_emb[:, None, :]\n",
        "\n",
        "\n",
        "def positionwise_FF(inp, d_model, d_inner, dropout, kernel_initializer,\n",
        "                    scope='ff', is_training=True):\n",
        "    output = inp\n",
        "    with tf.compat.v1.variable_scope(scope):\n",
        "        output = tf.keras.layers.Dense(d_inner, activation=tf.nn.relu, \n",
        "                                       kernel_initializer=kernel_initializer, name='layer_1')(inp)\n",
        "        output = tf.keras.layers.Dropout(dropout, name='drop_1')(output, training=is_training)\n",
        "        output = tf.keras.layers.Dense(d_model, activation=tf.nn.relu, \n",
        "                                       kernel_initializer=kernel_initializer, name='layer_2')(output)\n",
        "        output = tf.keras.layers.Dropout(dropout, name='drop_2')(output, training=is_training)\n",
        "        output = tf.keras.layers.LayerNormalization(axis=-1)(output + inp)\n",
        "    return output\n",
        "\n",
        "\n",
        "def _create_mask(qlen, mlen, same_length=False):\n",
        "    attn_mask = tf.ones([qlen, qlen])\n",
        "    mask_u = tf.linalg.band_part(attn_mask, 0, -1)\n",
        "    mask_dia = tf.linalg.band_part(attn_mask, 0, 0)\n",
        "    attn_mask_pad = tf.zeros([qlen, mlen])\n",
        "    ret = tf.concat([attn_mask_pad, mask_u - mask_dia], 1)\n",
        "    if same_length:\n",
        "        mask_l = tf.matrix_band_part(attn_mask, -1, 0)\n",
        "        ret = tf.concat([ret[:, :qlen] + mask_l - mask_dia, ret[:, qlen:]], 1)\n",
        "    return ret\n",
        "\n",
        "\n",
        "def _cache_mem(curr_out, prev_mem, mem_len=None):\n",
        "    if mem_len is None or prev_mem is None:\n",
        "        new_mem = curr_out\n",
        "    elif mem_len == 0:\n",
        "        return prev_mem\n",
        "    else:\n",
        "        new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:]\n",
        "    return tf.stop_gradient(new_mem)\n",
        "\n",
        "\n",
        "def rel_shift(x):\n",
        "    x_size = tf.shape(x)\n",
        "    x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])\n",
        "    x = tf.reshape(x, [x_size[1] + 1, x_size[0], x_size[2], x_size[3]])\n",
        "    x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n",
        "    x = tf.reshape(x, x_size)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rel_multihead_attn(w, r, r_w_bias, r_r_bias, attn_mask, mems, d_model,\n",
        "                       n_head, d_head, dropout, dropatt, is_training,\n",
        "                       kernel_initializer, scope='rel_attn'):\n",
        "    scale = 1 / (d_head ** 0.5)\n",
        "    with tf.compat.v1.variable_scope(scope):\n",
        "        qlen = tf.shape(w)[0]\n",
        "        rlen = tf.shape(r)[0]\n",
        "        bsz = tf.shape(w)[1]\n",
        "\n",
        "        cat = tf.concat([mems, w], 0) if mems is not None and mems.shape.ndims > 1 else w\n",
        "\n",
        "        w_heads = tf.keras.layers.Dense(3 * n_head * d_head, use_bias=False, \n",
        "                                        kernel_initializer=kernel_initializer, name='qkv')(cat)\n",
        "        r_head_k = tf.keras.layers.Dense(n_head * d_head, use_bias=False,\n",
        "                                         kernel_initializer=kernel_initializer, name='r')(r)\n",
        "        \n",
        "        w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, -1)\n",
        "        w_head_q = w_head_q[-qlen:]\n",
        "\n",
        "        klen = tf.shape(w_head_k)[0]\n",
        "\n",
        "        w_head_q = tf.reshape(w_head_q, [qlen, bsz, n_head, d_head])\n",
        "        w_head_k = tf.reshape(w_head_k, [klen, bsz, n_head, d_head])\n",
        "        w_head_v = tf.reshape(w_head_v, [klen, bsz, n_head, d_head])\n",
        "\n",
        "        r_head_k = tf.reshape(r_head_k, [rlen, n_head, d_head])\n",
        "\n",
        "        rw_head_q = w_head_q + r_w_bias\n",
        "        rr_head_q = w_head_q + r_r_bias\n",
        "\n",
        "        AC = tf.einsum('ibnd,jbnd->ijbn', rw_head_q, w_head_k)\n",
        "        BD = tf.einsum('ibnd,jnd->ijbn', rr_head_q, r_head_k)\n",
        "        BD = rel_shift(BD)\n",
        "\n",
        "        attn_score = (AC + BD) * scale\n",
        "        attn_mask_t = attn_mask[:, :, None, None]\n",
        "        attn_score = attn_score * (1 - attn_mask_t) - 1e30 * attn_mask_t\n",
        "\n",
        "        attn_prob = tf.nn.softmax(attn_score, 1)\n",
        "        attn_prob = tf.keras.layers.Dropout(dropatt)(attn_prob, training=is_training)\n",
        "\n",
        "        attn_vec = tf.einsum('ijbn,jbnd->ibnd', attn_prob, w_head_v)\n",
        "        size_t = tf.shape(attn_vec)\n",
        "        attn_vec = tf.reshape(attn_vec, [size_t[0], size_t[1], n_head * d_head])\n",
        "\n",
        "        attn_out = tf.keras.layers.Dense(d_model, use_bias=False, \n",
        "                                         kernel_initializer=kernel_initializer, name='o')(attn_vec)\n",
        "        attn_out = tf.keras.layers.Dropout(dropout)(attn_out, training=is_training)\n",
        "        output = tf.keras.layers.LayerNormalization(axis=-1)(attn_out + w)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "def transformer(dec_inp, target, mems, n_token, n_layer, d_model, d_embed,\n",
        "                n_head, d_head, d_inner, dropout, dropatt,\n",
        "                initializer, is_training, proj_initializer=None,\n",
        "                mem_len=None, cutoffs=[], div_val=1, tie_projs=[],\n",
        "                same_length=False, clamp_len=-1,\n",
        "                input_perms=None, target_perms=None, head_target=None,\n",
        "                untie_r=False, proj_same_dim=True,\n",
        "                scope='transformer'):\n",
        "    \"\"\"\n",
        "    cutoffs: a list of python int. Cutoffs for adaptive softmax.\n",
        "    tie_projs: a list of python bools. Whether to tie the projections.\n",
        "    perms: a list of tensors. Each tensor should of size [len, bsz, bin_size].\n",
        "        Only used in the adaptive setting.\n",
        "    \"\"\"\n",
        "\n",
        "    new_mems = []\n",
        "    with tf.compat.v1.variable_scope(scope, reuse= tf.compat.v1.AUTO_REUSE):\n",
        "        if untie_r:\n",
        "            r_w_bias = tf.compat.v1.get_variable('r_w_bias', [n_layer, n_head, d_head], initializer=initializer)\n",
        "            r_r_bias = tf.compat.v1.get_variable('r_r_bias', [n_layer, n_head, d_head], initializer=initializer)\n",
        "        else:\n",
        "            r_w_bias = tf.compat.v1.get_variable('r_w_bias', [n_head, d_head], initializer=initializer)\n",
        "            r_r_bias = tf.compat.v1.get_variable('r_r_bias', [n_head, d_head], initializer=initializer)\n",
        "\n",
        "        qlen = tf.shape(dec_inp)[0]\n",
        "        mlen = tf.shape(mems[0])[0] if mems is not None else 0\n",
        "        klen = qlen + mlen\n",
        "\n",
        "        if proj_initializer is None:\n",
        "            proj_initializer = initializer\n",
        "\n",
        "        embeddings, shared_params = normal_embedding_lookup(\n",
        "            x=dec_inp,\n",
        "            n_token=n_token,\n",
        "            d_embed=d_embed,\n",
        "            d_proj=d_model,\n",
        "            initializer=initializer,\n",
        "            proj_initializer=proj_initializer)\n",
        "        \n",
        "        attn_mask = _create_mask(qlen, mlen, same_length)\n",
        "        \n",
        "        pos_seq = tf.range(klen - 1, -1, -1.0)\n",
        "        if clamp_len > 0:\n",
        "            pos_seq = tf.minimum(pos_seq, clamp_len)\n",
        "        inv_freq = 1 / (10000 ** (tf.range(0, d_model, 2.0) / d_model))\n",
        "        pos_emb = positional_embedding(pos_seq, inv_freq)\n",
        "\n",
        "        output = tf.keras.layers.Dropout(rate=dropout)(embeddings, training=is_training)\n",
        "        pos_emb = tf.keras.layers.Dropout(rate=dropout)(pos_emb, training=is_training)\n",
        "\n",
        "        if mems is None:\n",
        "            mems = [None] * n_layer\n",
        "\n",
        "        for i in range(n_layer):\n",
        "            # cache new mems\n",
        "            new_mems.append(_cache_mem(output, mems[i], mem_len))\n",
        "\n",
        "            with tf.compat.v1.variable_scope('layer_{}'.format(i)):\n",
        "                output = rel_multihead_attn(\n",
        "                    w=output,\n",
        "                    r=pos_emb,\n",
        "                    r_w_bias=r_w_bias if not untie_r else r_w_bias[i],\n",
        "                    r_r_bias=r_r_bias if not untie_r else r_r_bias[i],\n",
        "                    attn_mask=attn_mask,\n",
        "                    mems=mems[i],\n",
        "                    d_model=d_model,\n",
        "                    n_head=n_head,\n",
        "                    d_head=d_head,\n",
        "                    dropout=dropout,\n",
        "                    dropatt=dropatt,\n",
        "                    is_training=is_training,\n",
        "                    kernel_initializer=initializer)\n",
        "\n",
        "                output = positionwise_FF(\n",
        "                    inp=output,\n",
        "                    d_model=d_model,\n",
        "                    d_inner=d_inner,\n",
        "                    dropout=dropout,\n",
        "                    kernel_initializer=initializer,\n",
        "                    is_training=is_training)\n",
        "\n",
        "        # apply Dropout\n",
        "        output = tf.keras.layers.Dropout(dropout)(output, training=is_training)\n",
        "\n",
        "        loss, logits = normal_softmax(\n",
        "            hidden=output,\n",
        "            target=target,\n",
        "            n_token=n_token,\n",
        "            params=shared_params)\n",
        "\n",
        "        return loss, logits, new_mems"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1d_eZNZ2QTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerXL(object):\n",
        "    ########################################\n",
        "    # initialize\n",
        "    ########################################\n",
        "    def __init__(self, vocab_size, checkpoint=None, is_training=False, training_seqs=None):\n",
        "        # load dictionary\n",
        "        self.event2word = vocab_size\n",
        "        # model settings\n",
        "        self.x_len = 512      #input sequence length\n",
        "        self.mem_len = 512    #\n",
        "        self.n_layer = 6\n",
        "        self.d_embed = 768\n",
        "        self.d_model = 768\n",
        "        self.dropout = 0.1    ##\n",
        "        self.n_head = 12\n",
        "        self.d_head = self.d_model // self.n_head\n",
        "        self.d_ff = 3072\n",
        "        self.n_token = (self.event2word)\n",
        "        self.learning_rate = 1e-4      ##\n",
        "        self.group_size = 3\n",
        "        self.entry_len = self.group_size * self.x_len\n",
        "        # mode\n",
        "        self.is_training = is_training\n",
        "        self.training_seqs = training_seqs\n",
        "        self.checkpoint = checkpoint\n",
        "        if self.is_training: # train from scratch or finetune\n",
        "            self.batch_size = 8        \n",
        "        else: # inference\n",
        "            self.batch_size = 1\n",
        "        # load model\n",
        "        self.load_model()\n",
        "\n",
        "    ########################################\n",
        "    # load model\n",
        "    ########################################\n",
        "    \n",
        "    def load_model(self):\n",
        "        tf.compat.v1.disable_eager_execution()\n",
        "        # placeholders ---> train\n",
        "        self.x = tf.compat.v1.placeholder(tf.int32, shape=[self.batch_size, None])\n",
        "        self.y = tf.compat.v1.placeholder(tf.int32, shape=[self.batch_size, None])\n",
        "        self.mems_i = [tf.compat.v1.placeholder(tf.float32, [self.mem_len, self.batch_size, self.d_model]) for _ in range(self.n_layer)]\n",
        "        # placeholders ---> test\n",
        "        self.x_t = tf.compat.v1.placeholder(tf.int32, shape=[1, None])\n",
        "        self.y_t = tf.compat.v1.placeholder(tf.int32, shape=[1, None])\n",
        "        self.mems_it = [tf.compat.v1.placeholder(tf.float32, [self.mem_len, 1, self.d_model]) for _ in range(self.n_layer)]\n",
        "        # model\n",
        "        self.global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "\n",
        "        # initialize parameters\n",
        "        initializer = tf.compat.v1.initializers.random_normal(stddev=0.02, seed=None)\n",
        "        proj_initializer = tf.compat.v1.initializers.random_normal(stddev=0.01, seed=None)\n",
        "        \n",
        "        with tf.compat.v1.variable_scope(tf.compat.v1.get_variable_scope()):\n",
        "            xx = tf.transpose(self.x, [1, 0])\n",
        "            yy = tf.transpose(self.y, [1, 0])\n",
        "            loss, self.logits, self.new_mem = transformer(\n",
        "                dec_inp=xx,\n",
        "                target=yy,\n",
        "                mems=self.mems_i,\n",
        "                n_token=self.n_token,\n",
        "                n_layer=self.n_layer,\n",
        "                d_model=self.d_model,\n",
        "                d_embed=self.d_embed,\n",
        "                n_head=self.n_head,\n",
        "                d_head=self.d_head,\n",
        "                d_inner=self.d_ff,\n",
        "                dropout=self.dropout,\n",
        "                dropatt=self.dropout,\n",
        "                initializer=initializer,\n",
        "                proj_initializer=proj_initializer,\n",
        "                is_training=self.is_training,\n",
        "                mem_len=self.mem_len,\n",
        "                cutoffs=[],\n",
        "                div_val=-1,\n",
        "                tie_projs=[],\n",
        "                same_length=False,\n",
        "                clamp_len=-1,\n",
        "                input_perms=None,\n",
        "                target_perms=None,\n",
        "                head_target=None,\n",
        "                untie_r=False,\n",
        "                proj_same_dim=True)\n",
        "        self.avg_loss = tf.reduce_mean(loss)\n",
        "        # vars\n",
        "        all_vars = tf.compat.v1.trainable_variables()\n",
        "        print ('num parameters:', np.sum([np.prod(v.get_shape().as_list()) for v in all_vars]))\n",
        "        grads = tf.gradients(self.avg_loss, all_vars)\n",
        "        grads_and_vars = list(zip(grads, all_vars))\n",
        "        # gradient clipping\n",
        "        def ClipIfNotNone(grad):\n",
        "            if grad is None:\n",
        "                return grad\n",
        "            return tf.clip_by_norm(grad, 100.)\n",
        "        \n",
        "        grads_and_vars = [(ClipIfNotNone(grad), var) for grad, var in grads_and_vars]\n",
        "        all_trainable_vars = tf.reduce_sum([tf.reduce_prod(v.shape) for v in tf.compat.v1.trainable_variables()])\n",
        "        # optimizer\n",
        "        #warmup_steps = 0\n",
        "        # increase the learning rate linearly\n",
        "        #if warmup_steps > 0:\n",
        "        #    warmup_lr = tf.compat.v1.to_float(self.global_step) / tf.compat.v1.to_float(warmup_steps) \\\n",
        "        #          * self.learning_rate\n",
        "        #else:\n",
        "        #    warmup_lr = 0.0\n",
        "\n",
        "        decay_lr = tf.compat.v1.train.cosine_decay(\n",
        "            self.learning_rate,\n",
        "            global_step=self.global_step,\n",
        "            decay_steps=200000,\n",
        "            alpha=0.004)\n",
        "        \n",
        "        #lr_decay_warmup = tf.where(self.global_step < warmup_steps,\n",
        "        #                    warmup_lr, decay_lr)\n",
        "        #decay_lr = tf.compat.v1.train.cosine_decay_warmup(     ##\n",
        "        #     self.learning_rate,\n",
        "        #     global_step=self.global_step,\n",
        "        #     decay_steps=200000,\n",
        "        #     warmup_steps=16000,\n",
        "        #     alpha=0.004\n",
        "        #)\n",
        "        \n",
        "        #try:\n",
        "            #self.optimizer = tfa.optimizers.LAMB(learning_rate=decay_lr)\n",
        "            #print('LAMBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB')\n",
        "        #except:\n",
        "            #self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=decay_lr)\n",
        "            #print('ADAMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM')\n",
        "            #pass\n",
        "        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=decay_lr)\n",
        "        self.train_op = self.optimizer.apply_gradients(grads_and_vars, self.global_step)\n",
        "        # saver\n",
        "        self.saver = tf.compat.v1.train.Saver(max_to_keep=100)\n",
        "        config = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n",
        "        config.gpu_options.allow_growth = True\n",
        "        self.sess = tf.compat.v1.Session(config=config)\n",
        "        # load pre-trained checkpoint or note\n",
        "        if self.checkpoint:\n",
        "            self.saver.restore(self.sess, self.checkpoint)\n",
        "        else:\n",
        "            self.sess.run(tf.compat.v1.global_variables_initializer())\n",
        "            \n",
        "    \n",
        "            \n",
        "    ########################################\n",
        "    # train\n",
        "    ########################################\n",
        "    def train(self, training_data, output_checkpoint_folder):\n",
        "        # check output folder\n",
        "        if not os.path.exists(output_checkpoint_folder):\n",
        "            os.mkdir(output_checkpoint_folder)\n",
        "        # shuffle\n",
        "        index = np.arange(len(training_data))\n",
        "        np.random.shuffle(index)\n",
        "        training_data = training_data[index]\n",
        "        num_batches = len(training_data) // self.batch_size\n",
        "        st = time.time()\n",
        "        for e in range(1000):\n",
        "            total_loss = []\n",
        "            for i in range(num_batches):\n",
        "                segments = training_data[self.batch_size*i:self.batch_size*(i+1)]\n",
        "                batch_m = [np.zeros((self.mem_len, self.batch_size, self.d_model), dtype=np.float32) for _ in range(self.n_layer)]\n",
        "                for j in range(self.group_size):\n",
        "                    batch_x = segments[:, j, 0, :]\n",
        "                    batch_y = segments[:, j, 1, :]\n",
        "                    # prepare feed dict\n",
        "                    feed_dict = {self.x: batch_x, self.y: batch_y}\n",
        "                    for m, m_np in zip(self.mems_i, batch_m):\n",
        "                        feed_dict[m] = m_np\n",
        "                    # run\n",
        "                    _, gs_, loss_, new_mem_ = self.sess.run([self.train_op, self.global_step, self.avg_loss, self.new_mem], feed_dict=feed_dict)\n",
        "                    batch_m = new_mem_\n",
        "                    total_loss.append(loss_)\n",
        "                    # print ('Current lr: {}'.format(self.sess.run(self.optimizer._lr)))\n",
        "                    print('>>> Epoch: {}, Step: {}, Loss: {:.5f}, Time: {:.2f}'.format(e, gs_, loss_, time.time()-st))\n",
        "                    print('i : ',i,' j : ',j)\n",
        "                    if not i % 500:\n",
        "                        self.saver.save(self.sess, '{}/model-{:03d}-{:.3f}'.format(output_checkpoint_folder, e, np.mean(total_loss)))\n",
        "                    \n",
        "\n",
        "            print ('[epoch {} avg loss] {:.5f}'.format(e, np.mean(total_loss)))\n",
        "            if not e % 6:\n",
        "                self.saver.save(self.sess, '{}/model-{:03d}-{:.3f}'.format(output_checkpoint_folder, e, np.mean(total_loss)))\n",
        "            # stop\n",
        "            if np.mean(total_loss) <= 0.0001:\n",
        "                break\n",
        "\n",
        "    ########################################\n",
        "    # search strategy: temperature (re-shape)\n",
        "    ########################################\n",
        "    def temperature(self, logits, temperature):\n",
        "        probs = np.exp(logits / temperature) / np.sum(np.exp(logits / temperature))\n",
        "        return probs\n",
        "\n",
        "\n",
        "    ########################################\n",
        "    # search strategy: nucleus (truncate)\n",
        "    ########################################\n",
        "    def nucleus(self, probs, p):\n",
        "        probs /= sum(probs)\n",
        "        sorted_probs = np.sort(probs)[::-1]\n",
        "        sorted_index = np.argsort(probs)[::-1]\n",
        "        cusum_sorted_probs = np.cumsum(sorted_probs)\n",
        "        after_threshold = cusum_sorted_probs > p\n",
        "        if sum(after_threshold) > 0:\n",
        "            last_index = np.where(after_threshold)[0][-1]\n",
        "            candi_index = sorted_index[:last_index]\n",
        "        else:\n",
        "            candi_index = sorted_index[:3] # just assign a value\n",
        "        candi_probs = [probs[i] for i in candi_index]\n",
        "        candi_probs /= sum(candi_probs)\n",
        "        word = np.random.choice(candi_index, size=1, p=candi_probs)[0]\n",
        "        return word\n",
        "\n",
        "    ########################################\n",
        "    # evaluate (for batch size = 1)\n",
        "    ########################################\n",
        "    def evaluate(self, notes, num_notes, k, strategies, use_structure=False, init_mem = None):\n",
        "\n",
        "      batch_size = 1\n",
        "      # initialize mem\n",
        "      if init_mem is None:\n",
        "          batch_m = [np.zeros((self.mem_len, batch_size, self.d_model), dtype=np.float32) for _ in range(self.n_layer)]\n",
        "          print('new memmmmm')\n",
        "      else:\n",
        "          batch_m = init_mem \n",
        "\n",
        "      initial_flag = True\n",
        "      fail = 0\n",
        "      i = 0\n",
        "\n",
        "      while i < num_notes:\n",
        "            if fail>200:\n",
        "              print('Fail : ',fail)\n",
        "              #continue\n",
        "\n",
        "            # prepare input\n",
        "            if initial_flag:\n",
        "                temp_x = np.zeros((batch_size, len(notes[0])))\n",
        "                for b in range(batch_size):\n",
        "                    for z, t in enumerate(notes[b]):\n",
        "                        temp_x[b][z] = t\n",
        "                initial_flag = False\n",
        "            else:\n",
        "                temp_x = np.zeros((batch_size, 1))\n",
        "                for b in range(batch_size):\n",
        "                    temp_x[b][0] = notes[b][-1]\n",
        "\n",
        "            # prepare feed dict\n",
        "            # inside a feed dict\n",
        "            # placeholder : data\n",
        "            # put input into feed_dict\n",
        "            feed_dict = {self.x: temp_x}\n",
        "\n",
        "            # put memeory into feed_dict\n",
        "            for m, m_np in zip(self.mems_i, batch_m):\n",
        "                feed_dict[m] = m_np\n",
        "            \n",
        "            # model (prediction)\n",
        "            _logits, _new_mem = self.sess.run([self.logits, self.new_mem], feed_dict=feed_dict)\n",
        "            #print('mem : ',_new_mem,' shape : ',len(_new_mem))\n",
        "            #print('shape : ',_logits.shape)\n",
        "            logits = _logits[-1, 0]\n",
        "\n",
        "            # temperature or not\n",
        "            if k == 0:\n",
        "              ran = float((np.random.randint(14,16))/10)\n",
        "            else:\n",
        "              ran = float((np.random.randint(7,10))/10)\n",
        "            \n",
        "            probs = self.temperature(logits=logits, temperature=ran)\n",
        "\n",
        "            # sampling\n",
        "            # note : the generated tokenized event\n",
        "            #ran_n = float((np.random.randint(90,98))/100)\n",
        "            note = self.nucleus(probs=probs, p=0.90)\n",
        "            \n",
        "\n",
        "            if note not in tokenizer.index_word:\n",
        "              continue\n",
        "\n",
        "            if (tokenizer.index_word[int(notes[0][-1])])[0] == 'n' and (tokenizer.index_word[int(note)])[0] != 'd':\n",
        "              print((tokenizer.index_word[int(notes[0][-1])]),' : ', tokenizer.index_word[int(note)])\n",
        "              fail += 1\n",
        "              continue\n",
        "            if (tokenizer.index_word[int(notes[0][-1])])[0] == 'd' and ((tokenizer.index_word[int(note)])[0] != 'i' and (tokenizer.index_word[int(note)]) != 'xxni'):\n",
        "              fail += 1\n",
        "              print((tokenizer.index_word[int(notes[0][-1])]),' : ',tokenizer.index_word[int(note)])\n",
        "              continue\n",
        "            if ((tokenizer.index_word[int(notes[0][-1])])[0] == 'i' or tokenizer.index_word[int(notes[0][-1])] == 'xxni') and ((tokenizer.index_word[int(note)])[0] != 'n' and (tokenizer.index_word[int(note)]) != 'xxsep'):\n",
        "              fail += 1\n",
        "              print((tokenizer.index_word[int(notes[0][-1])]),' : ',tokenizer.index_word[int(note)])\n",
        "              continue\n",
        "            if (tokenizer.index_word[int(notes[0][-1])]) == 'xxsep' and ((tokenizer.index_word[int(note)])[0] != 'd' and (tokenizer.index_word[int(note)])[0] != 'n'):\n",
        "              fail += 1\n",
        "              print((tokenizer.index_word[int(notes[0][-1])]),' : ',tokenizer.index_word[int(note)])\n",
        "              continue\n",
        "            \n",
        "            \n",
        "\n",
        "            # add new event to record sequence\n",
        "            notes = np.append(notes[0], note)\n",
        "            notes = np.reshape(notes, (1, len(notes)))\n",
        "            #print('notes : ',notes.shape)\n",
        "            \n",
        "            # re-new mem\n",
        "            batch_m = _new_mem\n",
        "            fail = 0\n",
        "            i += 1\n",
        "\n",
        "      return notes[0]\n",
        "\n",
        "    ########################################\n",
        "        # predict (for batch size = 1)\n",
        "    ########################################\n",
        "    def predict(self, notes, num_notes, k, strategies, use_structure=False):\n",
        "      prediction = self.evaluate(notes, num_notes, k, strategies, use_structure)\n",
        "\n",
        "      predicted_sentence = []\n",
        "  \n",
        "      for i in prediction:\n",
        "          # print('helllllo',int(i))\n",
        "          i = int(i)\n",
        "          if i < len(tokenizer.word_index) and i>0:\n",
        "              predicted_sentence.append(tokenizer.index_word[i])\n",
        "      return predicted_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU5daS2L2UUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_midi_dir(root_dir):\n",
        "    all_midi = []\n",
        "    for dirName, _, fileList in os.walk(root_dir):\n",
        "        for fname in fileList:\n",
        "            if '.mid' in fname:\n",
        "                all_midi.append(dirName + '/' + fname)\n",
        "\n",
        "    return all_midi\n",
        "\n",
        "\n",
        "    \n",
        "def get_data(notes_chords, sequence_length):\n",
        "    \n",
        "    # sequence_length = 100\n",
        "    notes_input = []\n",
        "    notes_output = []\n",
        "    shift = 1\n",
        "    \n",
        "    for i in range(0, len(notes_chords) - sequence_length, 1):\n",
        "        temp_input = ''\n",
        "        temp_output = ''\n",
        "        for j in range(i,i + sequence_length):\n",
        "            temp_input += notes_chords[j] + ' '\n",
        "        notes_input.append(temp_input)\n",
        "        for j in range(i+shift,i + sequence_length+shift):\n",
        "            temp_output += notes_chords[j] + ' '\n",
        "        notes_output.append(temp_output)\n",
        "\n",
        "\n",
        "    n_patterns = len(notes_input)\n",
        "    # notes_normalized_input = np.reshape(notes_input, (n_patterns, sequence_length))\n",
        "    # notes_normalized_input =  notes_normalized_input / float(n_vocab)\n",
        "    #notes_output = np.array(notes_output)\n",
        "\n",
        "\n",
        "    return (notes_input, notes_output)\n",
        "\n",
        "\n",
        "########################################\n",
        "    # Prepare data\n",
        "########################################\n",
        "        \n",
        "def xl_data(input_, output, group_size):\n",
        "        training_data = []\n",
        "    \n",
        "        pairs = []\n",
        "        for i in range(0, len(input_)):\n",
        "            x, y = input_[i], output[i]\n",
        "            \n",
        "            pairs.append([x, y])\n",
        "\n",
        "        pairs = np.array(pairs)\n",
        "    \n",
        "        # put pairs into training data by groups\n",
        "        for i in range(0, len(pairs) - group_size + 1, group_size):\n",
        "            segment = pairs[i:i+group_size]\n",
        "            assert len(segment) == group_size\n",
        "            training_data.append(segment)\n",
        "            \n",
        "        training_data = np.array(training_data)\n",
        "        \n",
        "        return training_data        \n",
        "        \n",
        "        \n",
        "        \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4FM_r6YAPON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_valid_ins(ins):\n",
        "  count = 0\n",
        "  ls = list(set(val for val in ins.values()))\n",
        "  for i in ls:\n",
        "    if i == 'Piano':\n",
        "      count+= 1\n",
        "    elif i == 'Acoustic Bass' or i == 'Electric Bass':\n",
        "      count += 1\n",
        "    elif i == 'Acoustic Guitar' or i == 'Electric Guitar':\n",
        "      count += 1\n",
        "    elif i == 'Violin':\n",
        "      count += 1\n",
        "    elif i == 'Saxophone':\n",
        "      count += 1\n",
        "  if(count>=3):\n",
        "    return True\n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YeMYirn-Jxw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61974b41-d000-49f9-e89a-8535190be1de"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QclsGGEy2dqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#required listsx\n",
        "chordarr_list = []\n",
        "npenc_list = []\n",
        "ins_list = []\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK81q6cEIbYK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7d1db2d7-c9cd-4e3d-e1b0-99427708503a"
      },
      "source": [
        "#read multiple files \n",
        "i = 0\n",
        "overall = 0\n",
        "for file_name in get_all_midi_dir('/content/drive/My Drive/datasets/hope/3+'):\n",
        "        if overall>90:\n",
        "          break\n",
        "        print('Now loading, ',i,' : ',overall,': \\n',file_name)\n",
        "        try:\n",
        "           mf = file2mf(file_name)\n",
        "        except:\n",
        "           continue\n",
        "           pass\n",
        "        try:\n",
        "           stream =mf2stream(mf)\n",
        "        except:\n",
        "           continue\n",
        "           pass\n",
        "        i += 1\n",
        "        overall += 1\n",
        "        chordarr,ins = stream2chordarr(stream)\n",
        "\n",
        "        if(not(check_valid_ins(ins))):\n",
        "          print('Discarding File :\\n', file_name)\n",
        "          try:\n",
        "              shutil.move(file_name, '/content/drive/My Drive/datasets/hope/discarded')\n",
        "          except:\n",
        "              shutil.move(file_name+str(overall), '/content/drive/My Drive/datasets/hope/discarded')\n",
        "              pass\n",
        "          i -= 1\n",
        "          continue\n",
        "\n",
        "        ins_list.append(ins)\n",
        "        chordarr_list.append(chordarr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now loading,  0 : \n",
            " /content/drive/My Drive/datasets/Canon/Canon In D Major - Johann Pachelbel - Chorus.mid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9iuC8MA2OwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#with open('/content/drive/My Drive/datasets/saved/Partial-multi/chord_list', 'wb') as filepath:\n",
        "#     pickle.dump(chordarr_list, filepath)\n",
        "#with open('/content/drive/My Drive/datasets/saved/Partial-multi/ins_list', 'wb') as filepath:\n",
        "#     pickle.dump(ins_list, filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMxsThkRISuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/datasets/saved/2018_2017/chord_list_2018', 'rb') as filepath:\n",
        "         chordarr_list = pickle.load(filepath)\n",
        "with open('/content/drive/My Drive/datasets/saved/2018_2017/ins_list_2018', 'rb') as filepath:\n",
        "         ins_list = pickle.load(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTAjNIxMvIH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "beb4f615-fe9a-4179-fa1b-63679e3ab923"
      },
      "source": [
        "print('Processing Now')\n",
        "#making uniq list for transformation \n",
        "res = list(set(val for dic in ins_list for val in dic.values())) \n",
        "uniq_ins = dict()\n",
        "for i in range(len(res)):\n",
        "        uniq_ins[res[i]] = i   \n",
        "INS_TOKS = [f'i{i}' for i in range(len(uniq_ins))]\n",
        "rev_uniq_ins = {value : key for (key, value) in uniq_ins.items()} \n",
        "    \n",
        "    \n",
        "for c in range(len(chordarr_list)):\n",
        "        npenc = chordarr2npenc(chordarr_list[c])\n",
        "        for i in npenc:\n",
        "            if(i[2] == -2):\n",
        "                i[2] = -2\n",
        "            else:\n",
        "                i[2] = uniq_ins[ins_list[c][i[2]]]\n",
        "        npenc_list.append(npenc)\n",
        "  \n",
        "    \n",
        "       \n",
        "#the final list or sequence \n",
        "final_list = []\n",
        "    \n",
        "for npenc in npenc_list:\n",
        "        final_list.append(BOS)\n",
        "        final_list.append(PAD)\n",
        "        \n",
        "        for i in range(len(npenc)):\n",
        "            if(npenc[i][0] == -1):\n",
        "                 x = SEP\n",
        "            else:\n",
        "                x = 'n' + str(npenc[i][0])\n",
        "            if npenc[i][1] > 16:\n",
        "              npenc[i][1] = 8\n",
        "            y = 'd' + str(npenc[i][1])\n",
        "            if(npenc[i][2] == -2):\n",
        "                z = IN\n",
        "            else:\n",
        "                z = 'i' + str(npenc[i][2])\n",
        "            final_list.append(x)\n",
        "            final_list.append(y)\n",
        "            final_list.append(z)\n",
        "        \n",
        "        final_list.append(PAD)\n",
        "        final_list.append(EOS)    \n",
        "\n",
        "    \n",
        "unique_notes = list(set(final_list))\n",
        "n_vocab = len(set(unique_notes))\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts(final_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3zyV_XkxNDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chordarr_list = []\n",
        "ins_list = []\n",
        "npenc_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oj_ESalRncp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/datasets/Final/final_list_single', 'wb') as filepath:\n",
        "     pickle.dump(final_list, filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClB0cTTjdfS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#with open('/content/drive/My Drive/datasets/Final/final_list', 'rb') as filepath:\n",
        "#         final_list = pickle.load(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5cGiwHUS9ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#final = []\n",
        "\n",
        "\n",
        "#count = 129\n",
        "#word_index = dict()\n",
        "#index_word = dict()\n",
        "\n",
        "#for item in final_list:\n",
        "#  if item in word_index:\n",
        "#    final.append(word_index[item])\n",
        "#    continue\n",
        "#  if item[0] == 'n':\n",
        "#    final.append(int(item[1:]))\n",
        "#    word_index[item] = final[-1]\n",
        "#    index_word[final[-1]] = item\n",
        "#  else : \n",
        "#    final.append(int(count))\n",
        "#    word_index[item] = final[-1]\n",
        "#    index_word[final[-1]] = item\n",
        "#    count += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XMNZsBU_R1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequence_length = 512\n",
        "network_input,network_output = get_data(final_list,sequence_length)\n",
        "# print(network_input.shape)\n",
        "# print(network_output.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rcc5WP1GAt4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6YxpJN-_RxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    \n",
        "MAX_LENGTH = n_vocab\n",
        "\n",
        "network_in = tokenizer.texts_to_sequences(network_input)\n",
        "network_in = tf.keras.preprocessing.sequence.pad_sequences(network_in,\n",
        "                                                           padding='post')\n",
        "  \n",
        "network_out = tokenizer.texts_to_sequences(network_output)\n",
        "network_out = tf.keras.preprocessing.sequence.pad_sequences(network_out,\n",
        "                                                       padding='post')\n",
        "VOCAB_SIZE =  len(tokenizer.word_index)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9uxAgo7Awfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network_output = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAyesuDK2gHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# decoder inputs use the previous target as input\n",
        "# remove START_TOKEN from targets\n",
        "\n",
        "group_size = 3\n",
        "data = xl_data(network_in, network_out, group_size)\n",
        "\n",
        "network_in = []\n",
        "network_out = []\n",
        "\n",
        "train_len = int(len(data)*0.7)\n",
        "\n",
        "training_data = data[:train_len]\n",
        "val_data = data[train_len:]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoXw5r7QA2sk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tzJwVimhtQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# declare model\n",
        "model = TransformerXL(\n",
        "       vocab_size=VOCAB_SIZE, \n",
        "       checkpoint=None,\n",
        "       is_training=True)\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnoWylvhVkMz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "447c2acf-c478-4af5-a234-24ea8ddc9900"
      },
      "source": [
        "VOCAB_SIZE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "109"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqz31idxMuNn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91251228-7c99-4568-eaad-9bb80e8f9282"
      },
      "source": [
        "training_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(223747, 3, 2, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN0ZMdtDWeA-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97005684-9a88-40a0-e20c-715dcd908ec0"
      },
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'d1': 2,\n",
              " 'd10': 63,\n",
              " 'd11': 59,\n",
              " 'd12': 68,\n",
              " 'd13': 72,\n",
              " 'd14': 83,\n",
              " 'd15': 78,\n",
              " 'd16': 81,\n",
              " 'd2': 5,\n",
              " 'd3': 6,\n",
              " 'd4': 7,\n",
              " 'd5': 8,\n",
              " 'd6': 40,\n",
              " 'd7': 30,\n",
              " 'd8': 12,\n",
              " 'd9': 52,\n",
              " 'i0': 1,\n",
              " 'n100': 90,\n",
              " 'n101': 94,\n",
              " 'n102': 100,\n",
              " 'n103': 102,\n",
              " 'n104': 106,\n",
              " 'n105': 105,\n",
              " 'n106': 107,\n",
              " 'n21': 103,\n",
              " 'n22': 108,\n",
              " 'n23': 104,\n",
              " 'n24': 101,\n",
              " 'n25': 98,\n",
              " 'n26': 97,\n",
              " 'n27': 99,\n",
              " 'n28': 93,\n",
              " 'n29': 91,\n",
              " 'n30': 88,\n",
              " 'n31': 79,\n",
              " 'n32': 87,\n",
              " 'n33': 73,\n",
              " 'n34': 82,\n",
              " 'n35': 74,\n",
              " 'n36': 71,\n",
              " 'n37': 67,\n",
              " 'n38': 66,\n",
              " 'n39': 65,\n",
              " 'n40': 61,\n",
              " 'n41': 62,\n",
              " 'n42': 60,\n",
              " 'n43': 58,\n",
              " 'n44': 50,\n",
              " 'n45': 51,\n",
              " 'n46': 53,\n",
              " 'n47': 45,\n",
              " 'n48': 46,\n",
              " 'n49': 42,\n",
              " 'n50': 41,\n",
              " 'n51': 39,\n",
              " 'n52': 31,\n",
              " 'n53': 38,\n",
              " 'n54': 33,\n",
              " 'n55': 34,\n",
              " 'n56': 13,\n",
              " 'n57': 24,\n",
              " 'n58': 29,\n",
              " 'n59': 22,\n",
              " 'n60': 25,\n",
              " 'n61': 17,\n",
              " 'n62': 27,\n",
              " 'n63': 15,\n",
              " 'n64': 14,\n",
              " 'n65': 21,\n",
              " 'n66': 18,\n",
              " 'n67': 28,\n",
              " 'n68': 9,\n",
              " 'n69': 11,\n",
              " 'n70': 23,\n",
              " 'n71': 16,\n",
              " 'n72': 20,\n",
              " 'n73': 10,\n",
              " 'n74': 32,\n",
              " 'n75': 19,\n",
              " 'n76': 26,\n",
              " 'n77': 36,\n",
              " 'n78': 37,\n",
              " 'n79': 43,\n",
              " 'n80': 35,\n",
              " 'n81': 44,\n",
              " 'n82': 48,\n",
              " 'n83': 49,\n",
              " 'n84': 47,\n",
              " 'n85': 55,\n",
              " 'n86': 56,\n",
              " 'n87': 54,\n",
              " 'n88': 57,\n",
              " 'n89': 64,\n",
              " 'n90': 69,\n",
              " 'n91': 75,\n",
              " 'n92': 70,\n",
              " 'n93': 77,\n",
              " 'n94': 76,\n",
              " 'n95': 85,\n",
              " 'n96': 86,\n",
              " 'n97': 84,\n",
              " 'n98': 89,\n",
              " 'n99': 80,\n",
              " 'xxbos': 95,\n",
              " 'xxeos': 96,\n",
              " 'xxni': 4,\n",
              " 'xxpad': 92,\n",
              " 'xxsep': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75kBf9gT2h6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train\n",
        "model.train(training_data, output_checkpoint_folder='/content/drive/My Drive/datasets/weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la113fZJ1BN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "notes = []\n",
        "num = np.random.randint(len(val_data)-(2*sequence_length))\n",
        "#num = 88993\n",
        "num_seq = 1\n",
        "lenxx = 512\n",
        "for i in range(num_seq):\n",
        "    notes.append(val_data[num+int(i*sequence_length/3), i, 0, :])\n",
        "notes = np.array(notes)[0][-lenxx:]\n",
        "notes = notes.flatten()\n",
        "notes = np.reshape(notes, (1, len(notes)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsabElAKr6js",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d12b09c6-dca3-4284-cec0-7276050bd4bd"
      },
      "source": [
        "notes.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xg91q21AIzI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7af3977b-66bd-470c-b142-8d11cab12ac7"
      },
      "source": [
        "len(notes[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyQZT0VNX_OV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "01a5ffb5-63e0-49cf-dd83-12b2ae776d11"
      },
      "source": [
        "network_input[train_len + (num*3)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'n54 d4 i0 n51 d2 i0 xxsep d3 xxni n53 d9 i0 n50 d16 i0 xxsep d3 xxni n68 d4 i0 xxsep d4 xxni n65 d11 i0 xxsep d2 xxni n54 d3 i0 xxsep d4 xxni n56 d4 i0 xxsep d3 xxni n66 d4 i0 n54 d1 i0 n51 d1 i0 xxsep d3 xxni n68 d10 i0 n53 d3 i0 n50 d3 i0 xxsep d4 xxni n51 d3 i0 n48 d3 i0 xxsep d3 xxni n50 d10 i0 n46 d13 i0 xxsep d3 xxni n65 d4 i0 xxsep d4 xxni n62 d7 i0 xxsep d3 xxni n51 d4 i0 xxsep d3 xxni n53 d4 i0 xxsep d3 xxni n62 d4 i0 xxsep d1 xxni n44 d4 i0 xxsep d2 xxni n63 d3 i0 n54 d1 i0 n42 d1 i0 xxsep d4 xxni n65 d1 i0 n56 d1 i0 n41 d1 i0 xxsep d3 xxni n66 d4 i0 n58 d1 i0 n39 d3 i0 xxsep d4 xxni n67 d1 i0 n58 d1 i0 n49 d3 i0 xxsep d3 xxni n68 d4 i0 n59 d2 i0 n47 d3 i0 xxsep d3 xxni n70 d1 i0 n61 d1 i0 n46 d3 i0 xxsep d3 xxni n71 d4 i0 n63 d1 i0 n44 d4 i0 xxsep d3 xxni n72 d1 i0 n63 d1 i0 n54 d2 i0 xxsep d4 xxni n74 d1 i0 n65 d1 i0 n53 d4 i0 xxsep d3 xxni n75 d3 i0 n66 d3 i0 n51 d5 i0 xxsep d4 xxni n58 d10 i0 xxsep d1 xxni n75 d9 i0 n65 d9 i0 xxsep d4 xxni n71 d5 i0 xxsep d5 xxni n74 d9 i0 n70 d7 i0 n46 d6 i0 xxsep d6 xxni n68 d13 i0 xxsep d12 xxni n75 d8 i0 n67 d8 i0 n51 d8 i0 n39 d8 i0 xxpad xxeos xxbos xxpad xxsep d8 xxni n61 d1 i0 xxsep d1 xxni n85 d1 i0 n70 d1 i0 n67 d1 i0 xxsep d1 xxni n88 d1 i0 xxsep d5 xxni n62 d1 i0 n56 d1 i0 xxsep d6 xxni n51 d1 i0 xxsep d3 xxni n52 d1 i0 n46 d1 i0 xxsep d5 xxni n69 d1 i0 n60 d1 i0 n51 d1 i0 n44 d1 i0 n37 d1 i0 xxsep d2 xxni n61 d1 i0 xxsep d6 xxni n69 d2 i0 n65 d1 i0 n62 d1 i0 n57 d1 i0 n45 d1 i0 n41 d1 i0 n38 d1 i0 xxsep d16 xxni n93 d1 i0 n88 d1 i0 n85 d1 i0 n81 d1 i0 n79 d1 i0 n61 d1 i0 n57 d1 i0 xxsep d8 xxni n45 d5 i0 n33 d4 i0 xxsep d4 xxni n46 d2 i0 n34 d1 i0 xxsep d1 xxni n49 d3 i0 n37 d2 i0 xxsep d2 xxni n53 d1 i0 n41 d1 i0 xxsep d3 xxni n50 d1 i0 xxsep d1 xxni n49 d1 i0 xxsep d1 xxni n45 d1 i0 n33 d1 i0 xxsep d2 xxni n50 d1 i0 xxsep d3 xxni n57 d1 i0 n55 d1 i0 n45 d1 i0 xxsep d2 xxni n52 d1 i0 n40 d1 i0 xxsep d1 xxni n37 d1 i0 xxsep d3 xxni n61 d1 i0 n58 '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZLKMhRD1KWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flag = False\n",
        "final_output = []\n",
        "#oooh = []\n",
        "lens_in = len(notes[0])\n",
        "shift = 1\n",
        "num_times = 2\n",
        "num_notes = 1572\n",
        "#num_notes = sequence_length-lenxx\n",
        "k = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAeftDTUK5fO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29c6cfe1-4e0e-4d54-f568-dcb741d6836e"
      },
      "source": [
        "print('num_notes : ',num_notes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_notes :  1572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkl4IvZwSxX7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7e375fa-f6a5-4a9f-984f-5a29e4fd7891"
      },
      "source": [
        "# Predict\n",
        "model_p = TransformerXL(\n",
        "       vocab_size=VOCAB_SIZE, \n",
        "       checkpoint='/content/drive/My Drive/datasets/Ckpt_past/Maestro-2-param/model-000-0.098',\n",
        "       is_training=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num parameters: 46133101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig9eDcMHmpGV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f714607f-3d20-4d14-aab8-944f7733791b"
      },
      "source": [
        "final_output = []\n",
        "for i in range(1):\n",
        "    print(\"########################################################################## : \",i)\n",
        "    print('lens_in : ',lens_in)\n",
        "    output = model_p.predict(notes, num_notes, k,\n",
        "                             strategies=['temperature', 'nucleus'],\n",
        "                             use_structure=True)\n",
        "    lens = len(output)\n",
        "    notes_temp = []\n",
        "    count = 0\n",
        "    for index, j in enumerate(output):\n",
        "        \n",
        "        if index >= (lens_in)-shift:\n",
        "           #print(' j : ',j,' index : ',index)\n",
        "           if i == 0 and flag == False:\n",
        "              final_output.append(j)\n",
        "              flag = True\n",
        "              count = 0\n",
        "              notes_temp.append(tokenizer.word_index[j])\n",
        "           elif (final_output[-1])[0] == 'n' and (j)[0] == 'd':\n",
        "              final_output.append(j)\n",
        "              count = 0\n",
        "              notes_temp.append(tokenizer.word_index[j])\n",
        "           elif (final_output[-1])[0] == 'd' and ((j)[0] == 'i' or (j) == 'xxni'):\n",
        "              final_output.append(j)\n",
        "              count = 0\n",
        "              notes_temp.append(tokenizer.word_index[j])\n",
        "           elif ((final_output[-1])[0] == 'i' or (final_output[-1]) == 'xxni') and ((j)[0] == 'n' or (j) == 'xxsep'):\n",
        "              final_output.append(j)\n",
        "              count = 0\n",
        "              notes_temp.append(tokenizer.word_index[j])\n",
        "           elif (final_output[-1]) == 'xxsep' and ((j)[0] == 'd' or (j)[0] == 'n') and count <= 1:\n",
        "              final_output.append(j)\n",
        "              count += 1\n",
        "              notes_temp.append(tokenizer.word_index[j])\n",
        "\n",
        "    notes = np.array(notes_temp)[-lenxx:]\n",
        "    lens_in = len(notes)\n",
        "    notes = np.reshape(notes, (1, len(notes)))\n",
        "    #print('\\nlast : ', final_output[-1])\n",
        "    #print('\\nlen : ', lens_in)\n",
        "    print(\"The iteration output : \",final_output,'\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "########################################################################## :  0\n",
            "lens_in :  512\n",
            "new memmmmm\n",
            "n77  :  n84\n",
            "i0  :  xxeos\n",
            "xxni  :  d1\n",
            "xxni  :  d1\n",
            "xxni  :  d1\n",
            "xxni  :  d1\n",
            "xxni  :  d1\n",
            "xxni  :  d1\n",
            "n83  :  n85\n",
            "n83  :  n85\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  n103\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d4\n",
            "d6  :  d15\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d6\n",
            "d6  :  d3\n",
            "d6  :  d2\n",
            "d6  :  n27\n",
            "d6  :  d15\n",
            "d6  :  n51\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  n28\n",
            "d6  :  d6\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d6\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d6\n",
            "d6  :  d10\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d6\n",
            "d6  :  d1\n",
            "d6  :  d6\n",
            "d6  :  d3\n",
            "d6  :  d2\n",
            "d6  :  d15\n",
            "d6  :  d1\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d6\n",
            "d6  :  n28\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  n28\n",
            "d6  :  n28\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  d6\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d6\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  n51\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d5\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  n51\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d6\n",
            "d6  :  d2\n",
            "d6  :  n28\n",
            "d6  :  d6\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d4\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  d3\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d6\n",
            "d6  :  d6\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  n51\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  n51\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d6\n",
            "d6  :  d11\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d6\n",
            "d6  :  d6\n",
            "d6  :  n51\n",
            "d6  :  d6\n",
            "d6  :  d1\n",
            "d6  :  d6\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  n28\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  n28\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  n58\n",
            "d6  :  d6\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d6\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  n28\n",
            "d6  :  d6\n",
            "d6  :  d6\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d6\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d6\n",
            "d6  :  n28\n",
            "d6  :  d2\n",
            "d6  :  d3\n",
            "d6  :  n28\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d6\n",
            "d6  :  d4\n",
            "d6  :  d6\n",
            "d6  :  d2\n",
            "d6  :  d6\n",
            "d6  :  d1\n",
            "d6  :  n51\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  n28\n",
            "Fail :  201\n",
            "d6  :  d2\n",
            "Fail :  202\n",
            "d6  :  d2\n",
            "Fail :  203\n",
            "d6  :  n28\n",
            "Fail :  204\n",
            "d6  :  d2\n",
            "Fail :  205\n",
            "d6  :  d2\n",
            "Fail :  206\n",
            "d6  :  d2\n",
            "Fail :  207\n",
            "d6  :  d2\n",
            "Fail :  208\n",
            "d6  :  d2\n",
            "Fail :  209\n",
            "d6  :  d2\n",
            "Fail :  210\n",
            "d6  :  d2\n",
            "Fail :  211\n",
            "Fail :  211\n",
            "d6  :  d2\n",
            "Fail :  212\n",
            "d6  :  d6\n",
            "Fail :  213\n",
            "d6  :  d2\n",
            "Fail :  214\n",
            "d6  :  d6\n",
            "Fail :  215\n",
            "d6  :  d2\n",
            "Fail :  216\n",
            "d6  :  d6\n",
            "Fail :  217\n",
            "d6  :  d2\n",
            "Fail :  218\n",
            "d6  :  d2\n",
            "Fail :  219\n",
            "d6  :  d4\n",
            "Fail :  220\n",
            "d6  :  d2\n",
            "Fail :  221\n",
            "d6  :  n51\n",
            "Fail :  222\n",
            "d6  :  d2\n",
            "Fail :  223\n",
            "d6  :  d2\n",
            "Fail :  224\n",
            "d6  :  d2\n",
            "Fail :  225\n",
            "d6  :  d2\n",
            "Fail :  226\n",
            "d6  :  d6\n",
            "Fail :  227\n",
            "d6  :  n28\n",
            "Fail :  228\n",
            "d6  :  n101\n",
            "Fail :  229\n",
            "Fail :  229\n",
            "d6  :  d2\n",
            "Fail :  230\n",
            "d6  :  d2\n",
            "Fail :  231\n",
            "d6  :  n51\n",
            "Fail :  232\n",
            "d6  :  d2\n",
            "Fail :  233\n",
            "d6  :  d10\n",
            "Fail :  234\n",
            "d6  :  d2\n",
            "Fail :  235\n",
            "d6  :  d15\n",
            "Fail :  236\n",
            "d6  :  d2\n",
            "Fail :  237\n",
            "d6  :  d2\n",
            "Fail :  238\n",
            "d6  :  d2\n",
            "Fail :  239\n",
            "d6  :  d1\n",
            "Fail :  240\n",
            "d6  :  d2\n",
            "Fail :  241\n",
            "d6  :  d2\n",
            "Fail :  242\n",
            "d6  :  n28\n",
            "Fail :  243\n",
            "d6  :  d2\n",
            "Fail :  244\n",
            "d6  :  d2\n",
            "Fail :  245\n",
            "Fail :  245\n",
            "d6  :  d2\n",
            "Fail :  246\n",
            "d6  :  d2\n",
            "Fail :  247\n",
            "d6  :  d2\n",
            "Fail :  248\n",
            "d6  :  d6\n",
            "Fail :  249\n",
            "d6  :  d2\n",
            "Fail :  250\n",
            "d6  :  d2\n",
            "Fail :  251\n",
            "d6  :  d2\n",
            "Fail :  252\n",
            "d6  :  d2\n",
            "Fail :  253\n",
            "d6  :  d2\n",
            "Fail :  254\n",
            "d6  :  d6\n",
            "Fail :  255\n",
            "d6  :  d2\n",
            "Fail :  256\n",
            "d6  :  d6\n",
            "Fail :  257\n",
            "d6  :  d2\n",
            "Fail :  258\n",
            "d6  :  d15\n",
            "Fail :  259\n",
            "d6  :  d2\n",
            "Fail :  260\n",
            "d6  :  d2\n",
            "Fail :  261\n",
            "d6  :  d6\n",
            "Fail :  262\n",
            "d6  :  d4\n",
            "Fail :  263\n",
            "d6  :  d1\n",
            "Fail :  264\n",
            "d6  :  d2\n",
            "Fail :  265\n",
            "Fail :  265\n",
            "d6  :  d2\n",
            "Fail :  266\n",
            "d6  :  d2\n",
            "Fail :  267\n",
            "d6  :  d2\n",
            "Fail :  268\n",
            "d6  :  n28\n",
            "Fail :  269\n",
            "Fail :  269\n",
            "d6  :  n28\n",
            "Fail :  270\n",
            "d6  :  d2\n",
            "Fail :  271\n",
            "d6  :  d2\n",
            "Fail :  272\n",
            "d6  :  d6\n",
            "Fail :  273\n",
            "d6  :  d2\n",
            "Fail :  274\n",
            "d6  :  d10\n",
            "Fail :  275\n",
            "d6  :  d6\n",
            "Fail :  276\n",
            "Fail :  276\n",
            "d6  :  d2\n",
            "Fail :  277\n",
            "d6  :  d2\n",
            "Fail :  278\n",
            "d6  :  d2\n",
            "Fail :  279\n",
            "d6  :  d6\n",
            "Fail :  280\n",
            "d6  :  d4\n",
            "Fail :  281\n",
            "d6  :  d2\n",
            "Fail :  282\n",
            "Fail :  282\n",
            "d6  :  d2\n",
            "Fail :  283\n",
            "Fail :  283\n",
            "d6  :  d2\n",
            "Fail :  284\n",
            "d6  :  d4\n",
            "Fail :  285\n",
            "d6  :  d6\n",
            "Fail :  286\n",
            "d6  :  n51\n",
            "Fail :  287\n",
            "d6  :  d2\n",
            "Fail :  288\n",
            "d6  :  d1\n",
            "Fail :  289\n",
            "d6  :  d2\n",
            "Fail :  290\n",
            "d6  :  d6\n",
            "Fail :  291\n",
            "d6  :  d6\n",
            "Fail :  292\n",
            "Fail :  292\n",
            "Fail :  292\n",
            "d6  :  d2\n",
            "Fail :  293\n",
            "d6  :  d2\n",
            "Fail :  294\n",
            "d6  :  d6\n",
            "Fail :  295\n",
            "d6  :  d2\n",
            "Fail :  296\n",
            "d6  :  d2\n",
            "Fail :  297\n",
            "d6  :  d2\n",
            "Fail :  298\n",
            "d6  :  d2\n",
            "Fail :  299\n",
            "d6  :  d2\n",
            "Fail :  300\n",
            "d6  :  d1\n",
            "Fail :  301\n",
            "d6  :  d2\n",
            "Fail :  302\n",
            "d6  :  d2\n",
            "Fail :  303\n",
            "d6  :  d2\n",
            "Fail :  304\n",
            "Fail :  304\n",
            "d6  :  d2\n",
            "Fail :  305\n",
            "d6  :  d2\n",
            "Fail :  306\n",
            "d6  :  d2\n",
            "Fail :  307\n",
            "d6  :  d2\n",
            "Fail :  308\n",
            "d6  :  d2\n",
            "Fail :  309\n",
            "d6  :  d2\n",
            "Fail :  310\n",
            "d6  :  d6\n",
            "Fail :  311\n",
            "d6  :  d6\n",
            "Fail :  312\n",
            "d6  :  d2\n",
            "Fail :  313\n",
            "d6  :  d9\n",
            "Fail :  314\n",
            "d6  :  d2\n",
            "Fail :  315\n",
            "d6  :  d2\n",
            "Fail :  316\n",
            "d6  :  xxbos\n",
            "Fail :  317\n",
            "d6  :  d2\n",
            "Fail :  318\n",
            "d6  :  d2\n",
            "Fail :  319\n",
            "d6  :  d2\n",
            "Fail :  320\n",
            "d6  :  d2\n",
            "Fail :  321\n",
            "d6  :  n102\n",
            "Fail :  322\n",
            "d6  :  d9\n",
            "Fail :  323\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  n28\n",
            "d6  :  d1\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  n51\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  n58\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  n51\n",
            "d6  :  d2\n",
            "d6  :  n28\n",
            "d6  :  d3\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d6\n",
            "d6  :  n55\n",
            "d6  :  d6\n",
            "d6  :  d2\n",
            "d6  :  n51\n",
            "d6  :  d1\n",
            "d6  :  d4\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d9\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  n64\n",
            "d6  :  d1\n",
            "d6  :  d1\n",
            "d6  :  d9\n",
            "d6  :  d1\n",
            "d6  :  d1\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  n28\n",
            "d6  :  d1\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d2\n",
            "d6  :  d1\n",
            "d6  :  d1\n",
            "d6  :  d1\n",
            "d6  :  d1\n",
            "d6  :  n51\n",
            "d6  :  d1\n",
            "xxsep  :  i0\n",
            "The iteration output :  ['i0', 'n77', 'd1', 'i0', 'n63', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n91', 'd1', 'i0', 'n90', 'd1', 'i0', 'n83', 'd1', 'i0', 'n54', 'd1', 'i0', 'xxsep', 'n52', 'd1', 'i0', 'n49', 'd1', 'i0', 'n46', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n88', 'd1', 'i0', 'n87', 'd1', 'i0', 'n49', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd3', 'xxni', 'n52', 'd1', 'i0', 'n47', 'd1', 'i0', 'xxsep', 'd3', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n59', 'd1', 'i0', 'n56', 'd1', 'i0', 'xxsep', 'd6', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n59', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd4', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n84', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n84', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n84', 'd1', 'i0', 'n81', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n84', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n71', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n85', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n85', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n85', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n77', 'd1', 'i0', 'n63', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n91', 'd1', 'i0', 'n90', 'd1', 'i0', 'n83', 'd1', 'i0', 'n54', 'd1', 'i0', 'xxsep', 'n52', 'd1', 'i0', 'n49', 'd1', 'i0', 'n46', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n88', 'd1', 'i0', 'n87', 'd1', 'i0', 'n49', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd3', 'xxni', 'n52', 'd1', 'i0', 'n47', 'd1', 'i0', 'xxsep', 'd3', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n59', 'd1', 'i0', 'n56', 'd1', 'i0', 'xxsep', 'd6', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n59', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd4', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n84', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n84', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n84', 'd1', 'i0', 'n81', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n84', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n71', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n85', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n85', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n85', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n77', 'd1', 'i0', 'n63', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n91', 'd1', 'i0', 'n90', 'd1', 'i0', 'n83', 'd1', 'i0', 'n54', 'd1', 'i0', 'xxsep', 'n33', 'd1', 'i0', 'n49', 'd1', 'i0', 'n46', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n88', 'd1', 'i0', 'n87', 'd1', 'i0', 'n49', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd3', 'xxni', 'n52', 'd1', 'i0', 'n47', 'd1', 'i0', 'xxsep', 'd3', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n59', 'd1', 'i0', 'n56', 'd1', 'i0', 'xxsep', 'd6', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n59', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd4', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n84', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n84', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n84', 'd1', 'i0', 'n81', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n84', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n71', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n85', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n85', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n85', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n77', 'd1', 'i0', 'n63', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n91', 'd1', 'i0', 'n90', 'd1', 'i0', 'n83', 'd1', 'i0', 'n54', 'd1', 'i0', 'xxsep', 'n33', 'd1', 'i0', 'n49', 'd1', 'i0', 'n46', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n88', 'd1', 'i0', 'n87', 'd1'] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw5edsirEZUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "store_ = final_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US_vZodwaFQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temps = network_input[train_len + (num*3)] + network_input[train_len + (num*3) + 640] + network_input[train_len + (num*3) + 1280]\n",
        "#temps = network_input[train_len + (num*3)]\n",
        "notes_in = []\n",
        "temp = '' \n",
        "for sentence in temps:\n",
        "  for i in sentence:\n",
        "      if i != ' ':\n",
        "        temp += i \n",
        "      else:\n",
        "        notes_in.append(temp)\n",
        "        temp = ''\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5zOBsLg72uv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c12c782-7c6a-4317-a4e4-898bbaf3e1b8"
      },
      "source": [
        "len(notes_in)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Url-QCqjnvyT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f25d04db-3e3d-4ebe-cc2d-3034fcb94c10"
      },
      "source": [
        "#final_output = notes_in + store_\n",
        "final_output = store_\n",
        "#final_output = notes_in\n",
        "print(len(final_output))\n",
        "\n",
        "npenc_out = []    \n",
        "\n",
        "pred = []\n",
        "\n",
        "for i in final_output:\n",
        "  if i != 'xxeos' and i != 'xxpad' and i != 'xxbos':\n",
        "    pred.append(i)\n",
        "\n",
        "\n",
        "while(pred[0][0] == 'd' or pred[0][0] == 'i' or pred[0] == 'xxni'):\n",
        "    pred = pred[1:]\n",
        "\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1573\n",
            "['n77', 'd1', 'i0', 'n63', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n91', 'd1', 'i0', 'n90', 'd1', 'i0', 'n83', 'd1', 'i0', 'n54', 'd1', 'i0', 'xxsep', 'n52', 'd1', 'i0', 'n49', 'd1', 'i0', 'n46', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n88', 'd1', 'i0', 'n87', 'd1', 'i0', 'n49', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd3', 'xxni', 'n52', 'd1', 'i0', 'n47', 'd1', 'i0', 'xxsep', 'd3', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n59', 'd1', 'i0', 'n56', 'd1', 'i0', 'xxsep', 'd6', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n59', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd4', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n84', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n84', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n84', 'd1', 'i0', 'n81', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n84', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n71', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n85', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n85', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n85', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n77', 'd1', 'i0', 'n63', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n91', 'd1', 'i0', 'n90', 'd1', 'i0', 'n83', 'd1', 'i0', 'n54', 'd1', 'i0', 'xxsep', 'n52', 'd1', 'i0', 'n49', 'd1', 'i0', 'n46', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n88', 'd1', 'i0', 'n87', 'd1', 'i0', 'n49', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd3', 'xxni', 'n52', 'd1', 'i0', 'n47', 'd1', 'i0', 'xxsep', 'd3', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n59', 'd1', 'i0', 'n56', 'd1', 'i0', 'xxsep', 'd6', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n59', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd4', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n84', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n84', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n84', 'd1', 'i0', 'n81', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n84', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n71', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n85', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n85', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n85', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n77', 'd1', 'i0', 'n63', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n91', 'd1', 'i0', 'n90', 'd1', 'i0', 'n83', 'd1', 'i0', 'n54', 'd1', 'i0', 'xxsep', 'n33', 'd1', 'i0', 'n49', 'd1', 'i0', 'n46', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n88', 'd1', 'i0', 'n87', 'd1', 'i0', 'n49', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd3', 'xxni', 'n52', 'd1', 'i0', 'n47', 'd1', 'i0', 'xxsep', 'd3', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n47', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n52', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n59', 'd1', 'i0', 'n56', 'd1', 'i0', 'xxsep', 'd6', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n59', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd4', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n80', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n84', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n84', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n84', 'd1', 'i0', 'n81', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n84', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n71', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n85', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n85', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n85', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'n83', 'd1', 'i0', 'n81', 'd1', 'i0', 'n76', 'd1', 'i0', 'n75', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n90', 'd1', 'i0', 'n76', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n83', 'd1', 'i0', 'n77', 'd1', 'i0', 'n63', 'd1', 'i0', 'xxsep', 'd1', 'xxni', 'n91', 'd1', 'i0', 'n90', 'd1', 'i0', 'n83', 'd1', 'i0', 'n54', 'd1', 'i0', 'xxsep', 'n33', 'd1', 'i0', 'n49', 'd1', 'i0', 'n46', 'd1', 'i0', 'xxsep', 'd2', 'xxni', 'n88', 'd1', 'i0', 'n87', 'd1']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXP21IS4n3wG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ced2559f-954d-4709-e26c-6f441f29b3a2"
      },
      "source": [
        "npenc_out = []    \n",
        "x = 0\n",
        "y = len(pred)\n",
        "\n",
        "for i in range(x,y,3):\n",
        "    x = i\n",
        "    if not( x+1<y and x+2<y):\n",
        "      continue\n",
        "\n",
        "    temp = []\n",
        "    if(pred[x] == 'xxsep'):\n",
        "        temp.append(-1)\n",
        "    elif(pred[x] == 'xxni'):\n",
        "        temp.append(-2)\n",
        "    else:\n",
        "        temp.append(int(pred[x][1:]))\n",
        "\n",
        "          \n",
        "    if(pred[x+1] == 'xxsep'):\n",
        "        temp.append(-1)\n",
        "    elif(pred[x+1] == 'xxni'):\n",
        "        temp.append(-2)\n",
        "    else:\n",
        "        temp.append(int(pred[x+1][1:]))\n",
        "           \n",
        "    if(pred[x+2] == 'xxsep'):\n",
        "        temp.append(-1)\n",
        "    elif(pred[x+2] == 'xxni'):\n",
        "        temp.append(-2)\n",
        "    else:\n",
        "        temp.append(int(pred[x+2][1:]))\n",
        "    npenc_out.append(temp)\n",
        "        \n",
        "print(npenc_out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[77, 1, 0], [63, 1, 0], [-1, 1, -2], [91, 1, 0], [90, 1, 0], [83, 1, 0], [54, 1, 0], [-1, 52, 1], [0, 49, 1], [0, 46, 1], [0, -1, 2], [-2, 88, 1], [0, 87, 1], [0, 49, 1], [0, -1, 2], [-2, 47, 1], [0, -1, 1], [-2, 52, 1], [0, -1, 3], [-2, 52, 1], [0, 47, 1], [0, -1, 3], [-2, 52, 1], [0, -1, 2], [-2, 47, 1], [0, -1, 1], [-2, 52, 1], [0, -1, 2], [-2, 47, 1], [0, -1, 1], [-2, 52, 1], [0, -1, 2], [-2, 47, 1], [0, -1, 1], [-2, 52, 1], [0, -1, 1], [-2, 59, 1], [0, 56, 1], [0, -1, 6], [-2, 80, 1], [0, -1, 1], [-2, 59, 1], [0, -1, 1], [-2, 80, 1], [0, -1, 1], [-2, 83, 1], [0, -1, 1], [-2, 80, 1], [0, -1, 2], [-2, 80, 1], [0, -1, 1], [-2, 80, 1], [0, -1, 1], [-2, 80, 1], [0, -1, 2], [-2, 80, 1], [0, -1, 1], [-2, 80, 1], [0, -1, 2], [-2, 80, 1], [0, -1, 4], [-2, 80, 1], [0, -1, 2], [-2, 80, 1], [0, -1, 1], [-2, 80, 1], [0, -1, 2], [-2, 80, 1], [0, -1, 1], [-2, 80, 1], [0, -1, 1], [-2, 80, 1], [0, -1, 1], [-2, 75, 1], [0, -1, 1], [-2, 81, 1], [0, -1, 1], [-2, 81, 1], [0, -1, 2], [-2, 84, 1], [0, 81, 1], [0, -1, 2], [-2, 84, 1], [0, 81, 1], [0, -1, 1], [-2, 81, 1], [0, -1, 2], [-2, 84, 1], [0, 81, 1], [0, 75, 1], [0, -1, 1], [-2, 81, 1], [0, -1, 2], [-2, 81, 1], [0, -1, 1], [-2, 81, 1], [0, -1, 1], [-2, 81, 1], [0, -1, 2], [-2, 81, 1], [0, -1, 1], [-2, 81, 1], [0, -1, 1], [-2, 84, 1], [0, 75, 1], [0, -1, 1], [-2, 81, 1], [0, -1, 1], [-2, 81, 1], [0, -1, 2], [-2, 83, 1], [0, 81, 1], [0, 71, 1], [0, -1, 1], [-2, 81, 1], [0, -1, 1], [-2, 85, 1], [0, 75, 1], [0, -1, 1], [-2, 81, 1], [0, -1, 1], [-2, 83, 1], [0, 81, 1], [0, -1, 1], [-2, 85, 1], [0, -1, 1], [-2, 81, 1], [0, -1, 1], [-2, 83, 1], [0, 81, 1], [0, -1, 1], [-2, 85, 1], [0, 81, 1], [0, 76, 1], [0, -1, 2], [-2, 81, 1], [0, 76, 1], [0, -1, 2], [-2, 83, 1], [0, 81, 1], [0, 76, 1], [0, -1, 1], [-2, 81, 1], [0, 76, 1], [0, -1, 1], [-2, 90, 1], [0, 76, 1], [0, -1, 1], [-2, 83, 1], [0, 81, 1], [0, 76, 1], [0, -1, 1], [-2, 90, 1], [0, -1, 1], [-2, 90, 1], [0, -1, 1], [-2, 83, 1], [0, 81, 1], [0, 76, 1], [0, -1, 1], [-2, 90, 1], [0, 83, 1], [0, 81, 1], [0, 76, 1], [0, 75, 1], [0, -1, 1], [-2, 90, 1], [0, 76, 1], [0, -1, 1], [-2, 83, 1], [0, 77, 1], [0, 63, 1], [0, -1, 1], [-2, 91, 1], [0, 90, 1], [0, 83, 1], [0, 54, 1], [0, -1, 52], [1, 0, 49], [1, 0, 46], [1, 0, -1], [2, -2, 88], [1, 0, 87], [1, 0, 49], [1, 0, -1], [2, -2, 47], [1, 0, -1], [1, -2, 52], [1, 0, -1], [3, -2, 52], [1, 0, 47], [1, 0, -1], [3, -2, 52], [1, 0, -1], [2, -2, 47], [1, 0, -1], [1, -2, 52], [1, 0, -1], [2, -2, 47], [1, 0, -1], [1, -2, 52], [1, 0, -1], [2, -2, 47], [1, 0, -1], [1, -2, 52], [1, 0, -1], [1, -2, 59], [1, 0, 56], [1, 0, -1], [6, -2, 80], [1, 0, -1], [1, -2, 59], [1, 0, -1], [1, -2, 80], [1, 0, -1], [1, -2, 83], [1, 0, -1], [1, -2, 80], [1, 0, -1], [2, -2, 80], [1, 0, -1], [1, -2, 80], [1, 0, -1], [1, -2, 80], [1, 0, -1], [2, -2, 80], [1, 0, -1], [1, -2, 80], [1, 0, -1], [2, -2, 80], [1, 0, -1], [4, -2, 80], [1, 0, -1], [2, -2, 80], [1, 0, -1], [1, -2, 80], [1, 0, -1], [2, -2, 80], [1, 0, -1], [1, -2, 80], [1, 0, -1], [1, -2, 80], [1, 0, -1], [1, -2, 75], [1, 0, -1], [1, -2, 81], [1, 0, -1], [1, -2, 81], [1, 0, -1], [2, -2, 84], [1, 0, 81], [1, 0, -1], [2, -2, 84], [1, 0, 81], [1, 0, -1], [1, -2, 81], [1, 0, -1], [2, -2, 84], [1, 0, 81], [1, 0, 75], [1, 0, -1], [1, -2, 81], [1, 0, -1], [2, -2, 81], [1, 0, -1], [1, -2, 81], [1, 0, -1], [1, -2, 81], [1, 0, -1], [2, -2, 81], [1, 0, -1], [1, -2, 81], [1, 0, -1], [1, -2, 84], [1, 0, 75], [1, 0, -1], [1, -2, 81], [1, 0, -1], [1, -2, 81], [1, 0, -1], [2, -2, 83], [1, 0, 81], [1, 0, 71], [1, 0, -1], [1, -2, 81], [1, 0, -1], [1, -2, 85], [1, 0, 75], [1, 0, -1], [1, -2, 81], [1, 0, -1], [1, -2, 83], [1, 0, 81], [1, 0, -1], [1, -2, 85], [1, 0, -1], [1, -2, 81], [1, 0, -1], [1, -2, 83], [1, 0, 81], [1, 0, -1], [1, -2, 85], [1, 0, 81], [1, 0, 76], [1, 0, -1], [2, -2, 81], [1, 0, 76], [1, 0, -1], [2, -2, 83], [1, 0, 81], [1, 0, 76], [1, 0, -1], [1, -2, 81], [1, 0, 76], [1, 0, -1], [1, -2, 90], [1, 0, 76], [1, 0, -1], [1, -2, 83], [1, 0, 81], [1, 0, 76], [1, 0, -1], [1, -2, 90], [1, 0, -1], [1, -2, 90], [1, 0, -1], [1, -2, 83], [1, 0, 81], [1, 0, 76], [1, 0, -1], [1, -2, 90], [1, 0, 83], [1, 0, 81], [1, 0, 76], [1, 0, 75], [1, 0, -1], [1, -2, 90], [1, 0, 76], [1, 0, -1], [1, -2, 83], [1, 0, 77], [1, 0, 63], [1, 0, -1], [1, -2, 91], [1, 0, 90], [1, 0, 83], [1, 0, 54], [1, 0, -1], [33, 1, 0], [49, 1, 0], [46, 1, 0], [-1, 2, -2], [88, 1, 0], [87, 1, 0], [49, 1, 0], [-1, 2, -2], [47, 1, 0], [-1, 1, -2], [52, 1, 0], [-1, 3, -2], [52, 1, 0], [47, 1, 0], [-1, 3, -2], [52, 1, 0], [-1, 2, -2], [47, 1, 0], [-1, 1, -2], [52, 1, 0], [-1, 2, -2], [47, 1, 0], [-1, 1, -2], [52, 1, 0], [-1, 2, -2], [47, 1, 0], [-1, 1, -2], [52, 1, 0], [-1, 1, -2], [59, 1, 0], [56, 1, 0], [-1, 6, -2], [80, 1, 0], [-1, 1, -2], [59, 1, 0], [-1, 1, -2], [80, 1, 0], [-1, 1, -2], [83, 1, 0], [-1, 1, -2], [80, 1, 0], [-1, 2, -2], [80, 1, 0], [-1, 1, -2], [80, 1, 0], [-1, 1, -2], [80, 1, 0], [-1, 2, -2], [80, 1, 0], [-1, 1, -2], [80, 1, 0], [-1, 2, -2], [80, 1, 0], [-1, 4, -2], [80, 1, 0], [-1, 2, -2], [80, 1, 0], [-1, 1, -2], [80, 1, 0], [-1, 2, -2], [80, 1, 0], [-1, 1, -2], [80, 1, 0], [-1, 1, -2], [80, 1, 0], [-1, 1, -2], [75, 1, 0], [-1, 1, -2], [81, 1, 0], [-1, 1, -2], [81, 1, 0], [-1, 2, -2], [84, 1, 0], [81, 1, 0], [-1, 2, -2], [84, 1, 0], [81, 1, 0], [-1, 1, -2], [81, 1, 0], [-1, 2, -2], [84, 1, 0], [81, 1, 0], [75, 1, 0], [-1, 1, -2], [81, 1, 0], [-1, 2, -2], [81, 1, 0], [-1, 1, -2], [81, 1, 0], [-1, 1, -2], [81, 1, 0], [-1, 2, -2], [81, 1, 0], [-1, 1, -2], [81, 1, 0], [-1, 1, -2], [84, 1, 0], [75, 1, 0], [-1, 1, -2], [81, 1, 0], [-1, 1, -2], [81, 1, 0], [-1, 2, -2], [83, 1, 0], [81, 1, 0], [71, 1, 0], [-1, 1, -2], [81, 1, 0], [-1, 1, -2], [85, 1, 0], [75, 1, 0], [-1, 1, -2], [81, 1, 0], [-1, 1, -2], [83, 1, 0], [81, 1, 0], [-1, 1, -2], [85, 1, 0], [-1, 1, -2], [81, 1, 0], [-1, 1, -2], [83, 1, 0], [81, 1, 0], [-1, 1, -2], [85, 1, 0], [81, 1, 0], [76, 1, 0], [-1, 2, -2], [81, 1, 0], [76, 1, 0], [-1, 2, -2], [83, 1, 0], [81, 1, 0], [76, 1, 0], [-1, 1, -2], [81, 1, 0], [76, 1, 0], [-1, 1, -2], [90, 1, 0], [76, 1, 0], [-1, 1, -2], [83, 1, 0], [81, 1, 0], [76, 1, 0], [-1, 1, -2], [90, 1, 0], [-1, 1, -2], [90, 1, 0], [-1, 1, -2], [83, 1, 0], [81, 1, 0], [76, 1, 0], [-1, 1, -2], [90, 1, 0], [83, 1, 0], [81, 1, 0], [76, 1, 0], [75, 1, 0], [-1, 1, -2], [90, 1, 0], [76, 1, 0], [-1, 1, -2], [83, 1, 0], [77, 1, 0], [63, 1, 0], [-1, 1, -2], [91, 1, 0], [90, 1, 0], [83, 1, 0], [54, 1, 0], [-1, 33, 1], [0, 49, 1], [0, 46, 1], [0, -1, 2], [-2, 88, 1], [0, 87, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTxrSqSFvNr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "npenc_out_2 = []\n",
        "for i in npenc_out:\n",
        "  if not(i[0] != -1 and i[2] == -2):\n",
        "    npenc_out_2.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32D7TugDC7b1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e4b7fb54-8517-4ed1-d8cc-e47840dace5f"
      },
      "source": [
        "len(npenc_out_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "524"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c9VI2DbvdeK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "51cffeb1-f206-4ee5-9e12-f04de30991d9"
      },
      "source": [
        "s = npenc2stream(npenc_out_2,rev_uniq_ins,120)\n",
        "s.write('midi', fp='/content/drive/My Drive/datasets/output/multi_reincarnated_real-1.mid')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/datasets/output/multi_reincarnated_real-1.mid'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXsXQuJH9zms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "723cd5f3-8b53-4adc-cf69-71d2b99ca934"
      },
      "source": [
        "rev_uniq_ins"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Saxophone',\n",
              " 1: 'Piano',\n",
              " 2: 'Violin',\n",
              " 3: 'Electric Guitar',\n",
              " 4: 'Acoustic Bass',\n",
              " 5: 'Electric Bass'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caLu6uvHFiqX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9babac95-e68d-47fe-de1c-e003ca2c5e0f"
      },
      "source": [
        "npenc_out_1 = []\n",
        "for i in npenc_out:\n",
        "  if i[0] == -1 and i[1]>=3:\n",
        "    print(i)\n",
        "    i[1] = 2\n",
        "  if not(i[0] != -1 and i[2] == -2):\n",
        "    npenc_out_1.append(i)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1, 52, 1]\n",
            "[-1, 3, -2]\n",
            "[-1, 3, -2]\n",
            "[-1, 6, -2]\n",
            "[-1, 4, -2]\n",
            "[-1, 33, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTv_2UMan8q7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dea322d1-496f-4dac-8ad2-f2f21df7ddd0"
      },
      "source": [
        "s = npenc2stream(npenc_out_1,rev_uniq_ins,120)\n",
        "s.write('midi', fp='/content/drive/My Drive/datasets/output/multi_reincarnated_maestro_real-1.mid')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/datasets/output/multi_reincarnated_maestro_real-1.mid'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpPHOtfBrs0q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "249dcd78-55c2-4ae2-88bf-8fe2fddd7755"
      },
      "source": [
        "npenc_out[:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[-2, 61, 2],\n",
              " [2, 57, 2],\n",
              " [2, -1, 2],\n",
              " [-2, 64, 4],\n",
              " [2, 61, 4],\n",
              " [2, 57, 4],\n",
              " [2, 47, 6],\n",
              " [2, -1, 4],\n",
              " [-2, 62, 2],\n",
              " [2, 61, 2],\n",
              " [2, -1, 2],\n",
              " [-2, 61, 2],\n",
              " [2, 57, 2],\n",
              " [2, -1, 2],\n",
              " [-2, 64, 4],\n",
              " [2, 61, 4],\n",
              " [2, 57, 4],\n",
              " [2, 52, 4],\n",
              " [2, -1, 4],\n",
              " [-2, 66, 4],\n",
              " [2, 66, 4],\n",
              " [2, 69, 2],\n",
              " [2, 66, 3],\n",
              " [2, 62, 4],\n",
              " [2, 45, 4],\n",
              " [2, -1, 2],\n",
              " [-2, 64, 2],\n",
              " [2, 66, 2],\n",
              " [2, -1, 2],\n",
              " [-2, 69, 20],\n",
              " [2, 69, 4],\n",
              " [2, 66, 4],\n",
              " [2, 62, 4],\n",
              " [2, 54, 4],\n",
              " [2, -1, 4],\n",
              " [-2, 62, 4],\n",
              " [2, 69, 4],\n",
              " [2, 54, 4],\n",
              " [2, -1, 4],\n",
              " [-2, 71, 4],\n",
              " [2, 71, 4],\n",
              " [2, 64, 12],\n",
              " [2, 52, 4],\n",
              " [2, 52, 4],\n",
              " [2, 40, 4],\n",
              " [2, -1, 4],\n",
              " [-2, 68, 2],\n",
              " [2, 64, 2],\n",
              " [2, 55, 4],\n",
              " [2, -1, 2],\n",
              " [-2, 71, 4],\n",
              " [2, 71, 4],\n",
              " [2, -1, 4],\n",
              " [-2, 69, 2],\n",
              " [2, 69, 2],\n",
              " [2, 64, 4],\n",
              " [2, 57, 4],\n",
              " [2, -1, 2],\n",
              " [-2, 69, 2],\n",
              " [2, 69, 2],\n",
              " [2, -1, 2],\n",
              " [-2, 71, 4],\n",
              " [2, 71, 4],\n",
              " [2, 47, 4],\n",
              " [2, -1, 4],\n",
              " [-2, 62, 4],\n",
              " [2, 62, 4],\n",
              " [2, 59, 4],\n",
              " [2, 57, 4],\n",
              " [2, 54, 4],\n",
              " [2, -1, 4],\n",
              " [-2, 69, 8],\n",
              " [2, 66, 8],\n",
              " [2, 52, 8],\n",
              " [2, 57, 2],\n",
              " [2, 61, 2],\n",
              " [-2, 76, 1],\n",
              " [0, -1, 1],\n",
              " [-2, 76, 1],\n",
              " [0, -1, 2],\n",
              " [-2, 76, 1],\n",
              " [0, -1, 5],\n",
              " [-2, 76, 1],\n",
              " [-2, 40, 1],\n",
              " [0, -1, 1],\n",
              " [-2, 43, 1],\n",
              " [0, -1, 1],\n",
              " [-2, 47, 1],\n",
              " [0, -1, 1],\n",
              " [-2, 43, 1],\n",
              " [0, 40, 1],\n",
              " [0, 35, 1],\n",
              " [0, -1, 2],\n",
              " [-2, 43, 1],\n",
              " [0, 43, 1],\n",
              " [0, -1, 1],\n",
              " [-2, 43, 1],\n",
              " [0, -1, 1],\n",
              " [-2, 43, 1],\n",
              " [0, -1, 2],\n",
              " [-2, 38, 1],\n",
              " [0, -1, 1],\n",
              " [-2, 47, 1],\n",
              " [0, -1, 1],\n",
              " [-2, 47, 1],\n",
              " [0, -1, 1],\n",
              " [-2, 47, 1],\n",
              " [0, -1, 1],\n",
              " [-2, 47, 1],\n",
              " [0, -1, 1],\n",
              " [-2, 47, 1],\n",
              " [-2, 48, 4],\n",
              " [0, -1, 4],\n",
              " [-2, 45, 4],\n",
              " [0, -1, 4],\n",
              " [-2, 47, 4],\n",
              " [0, -1, 47],\n",
              " [1, 0, -1],\n",
              " [1, -2, 43],\n",
              " [1, 0, 40],\n",
              " [1, 0, 35],\n",
              " [1, 0, -1],\n",
              " [2, -2, 43],\n",
              " [1, 0, 43],\n",
              " [1, 0, -1],\n",
              " [1, -2, 43],\n",
              " [1, 0, -1],\n",
              " [1, -2, 43],\n",
              " [1, 0, 38],\n",
              " [1, 0, -1],\n",
              " [1, -2, 43],\n",
              " [1, 0, 36],\n",
              " [1, 0, -1],\n",
              " [1, -2, 47],\n",
              " [1, 0, -1],\n",
              " [1, -2, 47],\n",
              " [1, 0, -1],\n",
              " [1, -2, 47],\n",
              " [1, 0, -1],\n",
              " [1, -2, 47],\n",
              " [1, 0, -1],\n",
              " [1, -2, 47],\n",
              " [1, 0, 40],\n",
              " [1, 0, 43],\n",
              " [1, 0, 31],\n",
              " [3, 0, -1],\n",
              " [3, -2, 43],\n",
              " [1, 0, 43],\n",
              " [1, 0, -1],\n",
              " [1, -2, 47],\n",
              " [3, 0, -1],\n",
              " [4, -2, 47],\n",
              " [3, 0, 42],\n",
              " [3, 0, 41],\n",
              " [3, 0, -1],\n",
              " [4, -2, 45],\n",
              " [3, 0, 42],\n",
              " [3, 0, 40],\n",
              " [3, 0, -1],\n",
              " [3, -2, 42],\n",
              " [4, 0, -1],\n",
              " [4, -2, 47],\n",
              " [3, 0, 42],\n",
              " [3, 0, 40],\n",
              " [3, 0, 38],\n",
              " [3, 0, -1],\n",
              " [3, -2, 42],\n",
              " [1, 0, -1],\n",
              " [1, -2, 48],\n",
              " [4, 0, 63],\n",
              " [4, 2, -1],\n",
              " [4, -2, 47],\n",
              " [3, 0, 42],\n",
              " [3, 0, 40],\n",
              " [3, 0, -1],\n",
              " [4, -2, 42],\n",
              " [4, 0, -1],\n",
              " [4, -2, 40],\n",
              " [4, 0, -1],\n",
              " [4, -2, 42],\n",
              " [3, 0, 43],\n",
              " [3, 0, -1],\n",
              " [4, -2, 38],\n",
              " [3, 0, -1],\n",
              " [4, -2, 50],\n",
              " [3, 0, 40],\n",
              " [3, 0, -1],\n",
              " [4, -2, 48],\n",
              " [3, 0, 42],\n",
              " [3, 0, -1],\n",
              " [4, -2, 45],\n",
              " [3, 0, 45],\n",
              " [3, 0, -1],\n",
              " [4, -2, 47],\n",
              " [3, 0, -1],\n",
              " [4, -2, 50],\n",
              " [4, 0, -1],\n",
              " [4, -2, 40],\n",
              " [2, 0, -1],\n",
              " [4, -2, 47],\n",
              " [3, 0, 40],\n",
              " [3, 0, 40],\n",
              " [2, 0, -1],\n",
              " [2, -2, 40],\n",
              " [1, 0, -1],\n",
              " [1, -2, 49],\n",
              " [1, 0, 40],\n",
              " [1, 0, -1],\n",
              " [1, -2, 47],\n",
              " [3, 0, -1],\n",
              " [4, -2, 50],\n",
              " [2, 0, 47],\n",
              " [3, 0, -1],\n",
              " [3, -2, 47],\n",
              " [1, 0, -1],\n",
              " [1, -2, 45],\n",
              " [2, 0, 40],\n",
              " [4, 0, -1],\n",
              " [2, -2, 40],\n",
              " [4, 0, -1],\n",
              " [2, -2, 40],\n",
              " [2, 0, -1],\n",
              " [2]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW5hlEhxI4HT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}