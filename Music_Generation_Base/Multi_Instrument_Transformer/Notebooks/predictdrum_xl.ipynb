{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predictdrum_xl.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SzPlH8YsVRC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "fbbe3b46-3c4c-424d-abf6-f36c859a6490"
      },
      "source": [
        "!pip install mido"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mido\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/0a/81beb587b1ae832ea6a1901dc7c6faa380e8dd154e0a862f0a9f3d2afab9/mido-1.2.9-py2.py3-none-any.whl (52kB)\n",
            "\r\u001b[K     |██████▎                         | 10kB 19.9MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 20kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 30kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 40kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 51kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.8MB/s \n",
            "\u001b[?25hInstalling collected packages: mido\n",
            "Successfully installed mido-1.2.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izVrDklTrNbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import music21\n",
        "import os\n",
        "#import midifile \n",
        "# pre_process\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from enum import Enum\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import *\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "#import modules\n",
        "#import XL_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F6ShFtxrSqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mido import Message, MidiFile, MidiTrack, MetaMessage, bpm2tempo\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7-8Ft5bsEkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from model import build_CNN\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from keras.utils import np_utils\n",
        "from keras import models\n",
        "import os\n",
        "from mido import MidiFile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdzGGE0eE9zn",
        "colab_type": "text"
      },
      "source": [
        "### **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSKVT67er9fh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MidiProcessor:\n",
        "\n",
        "    def __init__(self, midi_dir):\n",
        "        self.midi_dir = midi_dir\n",
        "        self.midi = MidiFile(self.midi_dir)\n",
        "        l = []\n",
        "        for i in range((len(self.midi.tracks))):\n",
        "            if(self.midi.tracks[i][0].channel == 9):\n",
        "                l.append(i)\n",
        "                break\n",
        "        if(len(l) != 0):\n",
        "            self.drum_track = self.midi.tracks[l[0]]\n",
        "        else:\n",
        "             self.drum_track = -1\n",
        "        self.ticks_per_beat = self.midi.ticks_per_beat\n",
        "        self.ticks_per_32nt = self.ticks_per_beat/8\n",
        "\n",
        "    def midi_to_df(self):\n",
        "\n",
        "        df = pd.DataFrame([m.dict() for m in self.drum_track])\n",
        "\n",
        "        # get time passed since the first message and quantize\n",
        "        df.time = [round(sum(df.time[0:i])/self.ticks_per_32nt)\n",
        "                   for i in range(1, len(df)+1)]\n",
        "        df = df[df.type == 'note_on']\n",
        "        df = df.pivot_table(index='time', columns='note',\n",
        "                            values='velocity', fill_value=0)\n",
        "        # Fill empty notes\n",
        "        df = df.reindex(pd.RangeIndex(df.index.max()+1)).fillna(0).sort_index()\n",
        "        \n",
        "        # if velocity > 0, change it to 1\n",
        "        df = (df > 0).astype(float)\n",
        "        df.columns = df.columns.astype(int)\n",
        "        return df\n",
        "\n",
        "\n",
        "def prepare_data(df, input_window_len=32, pred_steps=1, overlaps=0, train_test_split=None,\n",
        "                 tracks_len_list=None, max_instruments=None):\n",
        "    '''\n",
        "    tracks_len_list: if the provided df is a concatenation of several midis, \n",
        "        a list of tracks length should be provided to segment encoding results\n",
        "    max_instruments: Some percussion instruments are not that frequently appear, \n",
        "        one can set the maximum instruments to lower the complexity.\n",
        "    '''\n",
        "\n",
        "    # choose top max_instruments\n",
        "    if max_instruments != None:\n",
        "        most_frequent_inst = sorted(\n",
        "            df.sum().to_dict().items(), key=lambda kv: kv[1], reverse=True)\n",
        "        most_frequent_inst = [instrument[0]\n",
        "                              for instrument in most_frequent_inst][0:max_instruments]\n",
        "        df = df[most_frequent_inst]\n",
        "    df = df.reset_index(drop=True)\n",
        "    # remember the encoding scheme\n",
        "    instruments = df.columns.tolist()\n",
        "    \n",
        "    #understand this step plsss tomorrow\n",
        "    def split_tracks(df_values, tracks_len_list=tracks_len_list):\n",
        "        \n",
        "        segment_indices = [sum(tracks_len_list[:i])\n",
        "                           for i in range(len(tracks_len_list) + 1)]\n",
        "        \n",
        "        encoded_tracks_list = [df_values[segment_indices[i]:segment_indices[i+1], :]\n",
        "                               for i in range(len(segment_indices)-1)]\n",
        "\n",
        "        return encoded_tracks_list\n",
        "\n",
        "    encoded_tracks_list = split_tracks(\n",
        "            df.values, tracks_len_list=tracks_len_list)\n",
        "    return  encoded_tracks_list,instruments\n",
        "\n",
        "\n",
        "def array_to_midi(encoding_array, instruments_list, bpm=180):\n",
        "    new_song = MidiFile()\n",
        "    new_song.ticks_per_beat = 960\n",
        "    meta_track = MidiTrack()\n",
        "    new_song.tracks.append(meta_track)\n",
        "\n",
        "    # Create meta_track, add neccessary settings.\n",
        "    meta_track.append(MetaMessage(\n",
        "        type='track_name', name='meta_track', time=0))\n",
        "    meta_track.append(MetaMessage(type='time_signature', numerator=4, denominator=4,\n",
        "                                  clocks_per_click=24, notated_32nd_notes_per_beat=8, time=0))\n",
        "    meta_track.append(MetaMessage(type='set_tempo',\n",
        "                                  tempo=bpm2tempo(bpm), time=0))\n",
        "\n",
        "    # drum_track\n",
        "    drum_track = MidiTrack()\n",
        "    new_song.tracks.append(drum_track)\n",
        "\n",
        "    ticks_per_32note = 120\n",
        "\n",
        "    time_indices = []\n",
        "    for i, note in enumerate(encoding_array*instruments_list):\n",
        "        if sum(note) == 0:\n",
        "            pass\n",
        "        else:\n",
        "            time_indices.append(i)\n",
        "\n",
        "            if len(time_indices) <= 1:\n",
        "                notes_from_last_message = 0\n",
        "            else:\n",
        "                notes_from_last_message = time_indices[-1] - time_indices[-2]\n",
        "\n",
        "            same_note_count = 0\n",
        "            for inst in note:\n",
        "\n",
        "                if inst == 0:\n",
        "                    pass\n",
        "                elif same_note_count == 0:\n",
        "                    drum_track.append(Message('note_on', channel=9, note=inst, velocity=80,\n",
        "                                              time=notes_from_last_message*ticks_per_32note))\n",
        "                    same_note_count += 1\n",
        "                else:\n",
        "                    drum_track.append(Message('note_on', channel=9, note=inst, velocity=80,\n",
        "                                              time=0))\n",
        "                    same_note_count += 1\n",
        "    return new_song\n",
        "\n",
        "\n",
        "def concat_all_midi_to_df(root_dir, return_tracks_len_list=True):\n",
        "\n",
        "    def get_all_midi_dir(root_dir=root_dir):\n",
        "        all_midi = []\n",
        "        for dirName, _, fileList in os.walk(root_dir):\n",
        "            for fname in fileList:\n",
        "                if '.mid' in fname:\n",
        "                    all_midi.append(dirName + '/' + fname)\n",
        "\n",
        "        return all_midi\n",
        "\n",
        "    # loop through all the midis in provided root_dir and create df\n",
        "    df_lists = []\n",
        "    for file_name in get_all_midi_dir(root_dir=root_dir):\n",
        "         midiprocessor = MidiProcessor(file_name)\n",
        "         print('\\n',file_name)\n",
        "         if(midiprocessor.drum_track == -1):\n",
        "            continue\n",
        "         df = midiprocessor.midi_to_df()\n",
        "         df_lists.append(df)\n",
        "    df = pd.concat(df_lists).fillna(0).astype(float)\n",
        "\n",
        "    tracks_len_list = [len(df) for df in df_lists]\n",
        "    print(\"{} drum loops\".format(len(df_lists)))\n",
        "    print(\"{} percussion instruments\".format(len(df.columns)))\n",
        "    print(\"{} 32-notes\".format(len(df)))\n",
        "\n",
        "    if return_tracks_len_list:\n",
        "        return df, tracks_len_list\n",
        "    else:\n",
        "        return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5dzCyWxFCON",
        "colab_type": "text"
      },
      "source": [
        "### **Model : Transformer-XL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sqDzJ5RwVgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embedding_lookup(lookup_table, x):\n",
        "    return tf.compat.v1.nn.embedding_lookup(lookup_table, x)\n",
        "\n",
        "\n",
        "def normal_embedding_lookup(x, n_token, d_embed, d_proj, initializer,\n",
        "                            proj_initializer, scope='normal_embed', **kwargs):\n",
        "    emb_scale = d_proj ** 0.5\n",
        "    with tf.compat.v1.variable_scope(scope):\n",
        "        lookup_table = tf.compat.v1.get_variable('lookup_table', [n_token, d_embed], initializer=initializer)\n",
        "        y = embedding_lookup(lookup_table, x)\n",
        "        if d_proj != d_embed:\n",
        "            proj_W = tf.compat.v1.get_variable('proj_W', [d_embed, d_proj], initializer=proj_initializer)\n",
        "            y = tf.einsum('ibe,ed->ibd', y, proj_W)\n",
        "        else:\n",
        "            proj_W = None\n",
        "        ret_params = [lookup_table, proj_W]\n",
        "    y *= emb_scale\n",
        "    return y, ret_params\n",
        "\n",
        "\n",
        "def normal_softmax(hidden, target, n_token, params, scope='normal_softmax', **kwargs):\n",
        "    def _logit(x, W, b, proj):\n",
        "        y = x\n",
        "        if proj is not None:\n",
        "            y = tf.einsum('ibd,ed->ibe', y, proj)\n",
        "        return tf.einsum('ibd,nd->ibn', y, W) + b\n",
        "\n",
        "    params_W, params_projs = params[0], params[1]\n",
        "\n",
        "    with tf.compat.v1.variable_scope(scope):\n",
        "        softmax_b = tf.compat.v1.get_variable('bias', [n_token], initializer=tf.zeros_initializer())\n",
        "        output = _logit(hidden, params_W, softmax_b, params_projs)\n",
        "        nll = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target, logits=output)\n",
        "    return nll, output\n",
        "\n",
        "\n",
        "def positional_embedding(pos_seq, inv_freq, bsz=None):\n",
        "    sinusoid_inp = tf.einsum('i,j->ij', pos_seq, inv_freq)\n",
        "    pos_emb = tf.concat([tf.sin(sinusoid_inp), tf.cos(sinusoid_inp)], -1)\n",
        "    if bsz is not None:\n",
        "        return tf.tile(pos_emb[:, None, :], [1, bsz, 1])\n",
        "    else:\n",
        "        return pos_emb[:, None, :]\n",
        "\n",
        "\n",
        "def positionwise_FF(inp, d_model, d_inner, dropout, kernel_initializer,\n",
        "                    scope='ff', is_training=True):\n",
        "    output = inp\n",
        "    with tf.compat.v1.variable_scope(scope):\n",
        "        output = tf.keras.layers.Dense(d_inner, activation=tf.nn.relu, \n",
        "                                       kernel_initializer=kernel_initializer, name='layer_1')(inp)\n",
        "        output = tf.keras.layers.Dropout(dropout, name='drop_1')(output, training=is_training)\n",
        "        output = tf.keras.layers.Dense(d_model, activation=tf.nn.relu, \n",
        "                                       kernel_initializer=kernel_initializer, name='layer_2')(output)\n",
        "        output = tf.keras.layers.Dropout(dropout, name='drop_2')(output, training=is_training)\n",
        "        output = tf.keras.layers.LayerNormalization(axis=-1)(output + inp)\n",
        "    return output\n",
        "\n",
        "\n",
        "def _create_mask(qlen, mlen, same_length=False):\n",
        "    attn_mask = tf.ones([qlen, qlen])\n",
        "    mask_u = tf.linalg.band_part(attn_mask, 0, -1)\n",
        "    mask_dia = tf.linalg.band_part(attn_mask, 0, 0)\n",
        "    attn_mask_pad = tf.zeros([qlen, mlen])\n",
        "    ret = tf.concat([attn_mask_pad, mask_u - mask_dia], 1)\n",
        "    if same_length:\n",
        "        mask_l = tf.matrix_band_part(attn_mask, -1, 0)\n",
        "        ret = tf.concat([ret[:, :qlen] + mask_l - mask_dia, ret[:, qlen:]], 1)\n",
        "    return ret\n",
        "\n",
        "\n",
        "def _cache_mem(curr_out, prev_mem, mem_len=None):\n",
        "    if mem_len is None or prev_mem is None:\n",
        "        new_mem = curr_out\n",
        "    elif mem_len == 0:\n",
        "        return prev_mem\n",
        "    else:\n",
        "        new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:]\n",
        "    return tf.stop_gradient(new_mem)\n",
        "\n",
        "\n",
        "def rel_shift(x):\n",
        "    x_size = tf.shape(x)\n",
        "    x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])\n",
        "    x = tf.reshape(x, [x_size[1] + 1, x_size[0], x_size[2], x_size[3]])\n",
        "    x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n",
        "    x = tf.reshape(x, x_size)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rel_multihead_attn(w, r, r_w_bias, r_r_bias, attn_mask, mems, d_model,\n",
        "                       n_head, d_head, dropout, dropatt, is_training,\n",
        "                       kernel_initializer, scope='rel_attn'):\n",
        "    scale = 1 / (d_head ** 0.5)\n",
        "    with tf.compat.v1.variable_scope(scope):\n",
        "        qlen = tf.shape(w)[0]\n",
        "        rlen = tf.shape(r)[0]\n",
        "        bsz = tf.shape(w)[1]\n",
        "\n",
        "        cat = tf.concat([mems, w], 0) if mems is not None and mems.shape.ndims > 1 else w\n",
        "\n",
        "        w_heads = tf.keras.layers.Dense(3 * n_head * d_head, use_bias=False, \n",
        "                                        kernel_initializer=kernel_initializer, name='qkv')(cat)\n",
        "        r_head_k = tf.keras.layers.Dense(n_head * d_head, use_bias=False,\n",
        "                                         kernel_initializer=kernel_initializer, name='r')(r)\n",
        "        \n",
        "        w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, -1)\n",
        "        w_head_q = w_head_q[-qlen:]\n",
        "\n",
        "        klen = tf.shape(w_head_k)[0]\n",
        "\n",
        "        w_head_q = tf.reshape(w_head_q, [qlen, bsz, n_head, d_head])\n",
        "        w_head_k = tf.reshape(w_head_k, [klen, bsz, n_head, d_head])\n",
        "        w_head_v = tf.reshape(w_head_v, [klen, bsz, n_head, d_head])\n",
        "\n",
        "        r_head_k = tf.reshape(r_head_k, [rlen, n_head, d_head])\n",
        "\n",
        "        rw_head_q = w_head_q + r_w_bias\n",
        "        rr_head_q = w_head_q + r_r_bias\n",
        "\n",
        "        AC = tf.einsum('ibnd,jbnd->ijbn', rw_head_q, w_head_k)\n",
        "        BD = tf.einsum('ibnd,jnd->ijbn', rr_head_q, r_head_k)\n",
        "        BD = rel_shift(BD)\n",
        "\n",
        "        attn_score = (AC + BD) * scale\n",
        "        attn_mask_t = attn_mask[:, :, None, None]\n",
        "        attn_score = attn_score * (1 - attn_mask_t) - 1e30 * attn_mask_t\n",
        "\n",
        "        attn_prob = tf.nn.softmax(attn_score, 1)\n",
        "        attn_prob = tf.keras.layers.Dropout(dropatt)(attn_prob, training=is_training)\n",
        "\n",
        "        attn_vec = tf.einsum('ijbn,jbnd->ibnd', attn_prob, w_head_v)\n",
        "        size_t = tf.shape(attn_vec)\n",
        "        attn_vec = tf.reshape(attn_vec, [size_t[0], size_t[1], n_head * d_head])\n",
        "\n",
        "        attn_out = tf.keras.layers.Dense(d_model, use_bias=False, \n",
        "                                         kernel_initializer=kernel_initializer, name='o')(attn_vec)\n",
        "        attn_out = tf.keras.layers.Dropout(dropout)(attn_out, training=is_training)\n",
        "        output = tf.keras.layers.LayerNormalization(axis=-1)(attn_out + w)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "def transformer(dec_inp, target, mems, n_token, n_layer, d_model, d_embed,\n",
        "                n_head, d_head, d_inner, dropout, dropatt,\n",
        "                initializer, is_training, proj_initializer=None,\n",
        "                mem_len=None, cutoffs=[], div_val=1, tie_projs=[],\n",
        "                same_length=False, clamp_len=-1,\n",
        "                input_perms=None, target_perms=None, head_target=None,\n",
        "                untie_r=False, proj_same_dim=True,\n",
        "                scope='transformer'):\n",
        "    \"\"\"\n",
        "    cutoffs: a list of python int. Cutoffs for adaptive softmax.\n",
        "    tie_projs: a list of python bools. Whether to tie the projections.\n",
        "    perms: a list of tensors. Each tensor should of size [len, bsz, bin_size].\n",
        "        Only used in the adaptive setting.\n",
        "    \"\"\"\n",
        "\n",
        "    new_mems = []\n",
        "    with tf.compat.v1.variable_scope(scope, reuse= tf.compat.v1.AUTO_REUSE):\n",
        "        if untie_r:\n",
        "            r_w_bias = tf.compat.v1.get_variable('r_w_bias', [n_layer, n_head, d_head], initializer=initializer)\n",
        "            r_r_bias = tf.compat.v1.get_variable('r_r_bias', [n_layer, n_head, d_head], initializer=initializer)\n",
        "        else:\n",
        "            r_w_bias = tf.compat.v1.get_variable('r_w_bias', [n_head, d_head], initializer=initializer)\n",
        "            r_r_bias = tf.compat.v1.get_variable('r_r_bias', [n_head, d_head], initializer=initializer)\n",
        "\n",
        "        qlen = tf.shape(dec_inp)[0]\n",
        "        mlen = tf.shape(mems[0])[0] if mems is not None else 0\n",
        "        klen = qlen + mlen\n",
        "\n",
        "        if proj_initializer is None:\n",
        "            proj_initializer = initializer\n",
        "\n",
        "        embeddings, shared_params = normal_embedding_lookup(\n",
        "            x=dec_inp,\n",
        "            n_token=n_token,\n",
        "            d_embed=d_embed,\n",
        "            d_proj=d_model,\n",
        "            initializer=initializer,\n",
        "            proj_initializer=proj_initializer)\n",
        "        \n",
        "        attn_mask = _create_mask(qlen, mlen, same_length)\n",
        "        \n",
        "        pos_seq = tf.range(klen - 1, -1, -1.0)\n",
        "        if clamp_len > 0:\n",
        "            pos_seq = tf.minimum(pos_seq, clamp_len)\n",
        "        inv_freq = 1 / (10000 ** (tf.range(0, d_model, 2.0) / d_model))\n",
        "        pos_emb = positional_embedding(pos_seq, inv_freq)\n",
        "\n",
        "        output = tf.keras.layers.Dropout(rate=dropout)(embeddings, training=is_training)\n",
        "        pos_emb = tf.keras.layers.Dropout(rate=dropout)(pos_emb, training=is_training)\n",
        "\n",
        "        if mems is None:\n",
        "            mems = [None] * n_layer\n",
        "\n",
        "        for i in range(n_layer):\n",
        "            # cache new mems\n",
        "            new_mems.append(_cache_mem(output, mems[i], mem_len))\n",
        "\n",
        "            with tf.compat.v1.variable_scope('layer_{}'.format(i)):\n",
        "                output = rel_multihead_attn(\n",
        "                    w=output,\n",
        "                    r=pos_emb,\n",
        "                    r_w_bias=r_w_bias if not untie_r else r_w_bias[i],\n",
        "                    r_r_bias=r_r_bias if not untie_r else r_r_bias[i],\n",
        "                    attn_mask=attn_mask,\n",
        "                    mems=mems[i],\n",
        "                    d_model=d_model,\n",
        "                    n_head=n_head,\n",
        "                    d_head=d_head,\n",
        "                    dropout=dropout,\n",
        "                    dropatt=dropatt,\n",
        "                    is_training=is_training,\n",
        "                    kernel_initializer=initializer)\n",
        "\n",
        "                output = positionwise_FF(\n",
        "                    inp=output,\n",
        "                    d_model=d_model,\n",
        "                    d_inner=d_inner,\n",
        "                    dropout=dropout,\n",
        "                    kernel_initializer=initializer,\n",
        "                    is_training=is_training)\n",
        "\n",
        "        # apply Dropout\n",
        "        output = tf.keras.layers.Dropout(dropout)(output, training=is_training)\n",
        "\n",
        "        loss, logits = normal_softmax(\n",
        "            hidden=output,\n",
        "            target=target,\n",
        "            n_token=n_token,\n",
        "            params=shared_params)\n",
        "\n",
        "        return loss, logits, new_mems"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrY1FFdhwcBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerXL(object):\n",
        "    ########################################\n",
        "    # initialize\n",
        "    ########################################\n",
        "    def __init__(self, vocab_size, memlen, checkpoint=None, is_training=False, training_seqs=None):\n",
        "        # load dictionary\n",
        "        self.event2word = vocab_size\n",
        "        # model settings\n",
        "        self.x_len = 256     #input sequence length\n",
        "        self.mem_len = memlen    #\n",
        "        self.n_layer = 6\n",
        "        self.d_embed = 768\n",
        "        self.d_model = 768\n",
        "        self.dropout = 0.1    ##\n",
        "        self.n_head = 12\n",
        "        self.d_head = self.d_model // self.n_head\n",
        "        self.d_ff = 3072\n",
        "        self.n_token = (self.event2word)\n",
        "        self.learning_rate = 1e-4      ##\n",
        "        self.group_size = 3\n",
        "        self.entry_len = self.group_size * self.x_len\n",
        "        # mode\n",
        "        self.is_training = is_training\n",
        "        self.training_seqs = training_seqs\n",
        "        self.checkpoint = checkpoint\n",
        "        if self.is_training: # train from scratch or finetune\n",
        "            self.batch_size = 8        \n",
        "        else: # inference\n",
        "            self.batch_size = 1\n",
        "        # load model\n",
        "        self.load_model()\n",
        "\n",
        "    ########################################\n",
        "    # load model\n",
        "    ########################################\n",
        "    \n",
        "    def load_model(self):\n",
        "        tf.compat.v1.disable_eager_execution()\n",
        "        # placeholders ---> train\n",
        "        self.x = tf.compat.v1.placeholder(tf.int32, shape=[self.batch_size, None])\n",
        "        self.y = tf.compat.v1.placeholder(tf.int32, shape=[self.batch_size, None])\n",
        "        self.mems_i = [tf.compat.v1.placeholder(tf.float32, [self.mem_len, self.batch_size, self.d_model]) for _ in range(self.n_layer)]\n",
        "        # placeholders ---> test\n",
        "        self.x_t = tf.compat.v1.placeholder(tf.int32, shape=[1, None])\n",
        "        self.y_t = tf.compat.v1.placeholder(tf.int32, shape=[1, None])\n",
        "        self.mems_it = [tf.compat.v1.placeholder(tf.float32, [self.mem_len, 1, self.d_model]) for _ in range(self.n_layer)]\n",
        "        # model\n",
        "        self.global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "\n",
        "        # initialize parameters\n",
        "        initializer = tf.compat.v1.initializers.random_normal(stddev=0.02, seed=None)\n",
        "        proj_initializer = tf.compat.v1.initializers.random_normal(stddev=0.01, seed=None)\n",
        "        \n",
        "        with tf.compat.v1.variable_scope(tf.compat.v1.get_variable_scope()):\n",
        "            xx = tf.transpose(self.x, [1, 0])\n",
        "            yy = tf.transpose(self.y, [1, 0])\n",
        "            loss, self.logits, self.new_mem = transformer(\n",
        "                dec_inp=xx,\n",
        "                target=yy,\n",
        "                mems=self.mems_i,\n",
        "                n_token=self.n_token,\n",
        "                n_layer=self.n_layer,\n",
        "                d_model=self.d_model,\n",
        "                d_embed=self.d_embed,\n",
        "                n_head=self.n_head,\n",
        "                d_head=self.d_head,\n",
        "                d_inner=self.d_ff,\n",
        "                dropout=self.dropout,\n",
        "                dropatt=self.dropout,\n",
        "                initializer=initializer,\n",
        "                proj_initializer=proj_initializer,\n",
        "                is_training=self.is_training,\n",
        "                mem_len=self.mem_len,\n",
        "                cutoffs=[],\n",
        "                div_val=-1,\n",
        "                tie_projs=[],\n",
        "                same_length=False,\n",
        "                clamp_len=-1,\n",
        "                input_perms=None,\n",
        "                target_perms=None,\n",
        "                head_target=None,\n",
        "                untie_r=False,\n",
        "                proj_same_dim=True)\n",
        "        self.avg_loss = tf.reduce_mean(loss)\n",
        "        # vars\n",
        "        all_vars = tf.compat.v1.trainable_variables()\n",
        "        print ('num parameters:', np.sum([np.prod(v.get_shape().as_list()) for v in all_vars]))\n",
        "        grads = tf.gradients(self.avg_loss, all_vars)\n",
        "        grads_and_vars = list(zip(grads, all_vars))\n",
        "        # gradient clipping\n",
        "        def ClipIfNotNone(grad):\n",
        "            if grad is None:\n",
        "                return grad\n",
        "            return tf.clip_by_norm(grad, 100.)\n",
        "        \n",
        "        grads_and_vars = [(ClipIfNotNone(grad), var) for grad, var in grads_and_vars]\n",
        "        all_trainable_vars = tf.reduce_sum([tf.reduce_prod(v.shape) for v in tf.compat.v1.trainable_variables()])\n",
        "        # optimizer\n",
        "        #warmup_steps = 0\n",
        "        # increase the learning rate linearly\n",
        "        #if warmup_steps > 0:\n",
        "        #    warmup_lr = tf.compat.v1.to_float(self.global_step) / tf.compat.v1.to_float(warmup_steps) \\\n",
        "        #          * self.learning_rate\n",
        "        #else:\n",
        "        #    warmup_lr = 0.0\n",
        "\n",
        "        decay_lr = tf.compat.v1.train.cosine_decay(\n",
        "            self.learning_rate,\n",
        "            global_step=self.global_step,\n",
        "            decay_steps=200000,\n",
        "            alpha=0.004)\n",
        "        \n",
        "        #lr_decay_warmup = tf.where(self.global_step < warmup_steps,\n",
        "        #                    warmup_lr, decay_lr)\n",
        "        #decay_lr = tf.compat.v1.train.cosine_decay_warmup(     ##\n",
        "        #     self.learning_rate,\n",
        "        #     global_step=self.global_step,\n",
        "        #     decay_steps=200000,\n",
        "        #     warmup_steps=16000,\n",
        "        #     alpha=0.004\n",
        "        #)\n",
        "        \n",
        "        #try:\n",
        "            #self.optimizer = tfa.optimizers.LAMB(learning_rate=decay_lr)\n",
        "            #print('LAMBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB')\n",
        "        #except:\n",
        "            #self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=decay_lr)\n",
        "            #print('ADAMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM')\n",
        "            #pass\n",
        "        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=decay_lr)\n",
        "        self.train_op = self.optimizer.apply_gradients(grads_and_vars, self.global_step)\n",
        "        # saver\n",
        "        self.saver = tf.compat.v1.train.Saver(max_to_keep=100)\n",
        "        config = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n",
        "        config.gpu_options.allow_growth = True\n",
        "        self.sess = tf.compat.v1.Session(config=config)\n",
        "        # load pre-trained checkpoint or note\n",
        "        if self.checkpoint:\n",
        "            self.saver.restore(self.sess, self.checkpoint)\n",
        "        else:\n",
        "            self.sess.run(tf.compat.v1.global_variables_initializer())\n",
        "            \n",
        "    \n",
        "            \n",
        "    ########################################\n",
        "    # train\n",
        "    ########################################\n",
        "    def train(self, training_data, output_checkpoint_folder):\n",
        "        # check output folder\n",
        "        if not os.path.exists(output_checkpoint_folder):\n",
        "            os.mkdir(output_checkpoint_folder)\n",
        "        # shuffle\n",
        "        index = np.arange(len(training_data))\n",
        "        np.random.shuffle(index)\n",
        "        training_data = training_data[index]\n",
        "        num_batches = len(training_data) // self.batch_size\n",
        "        st = time.time()\n",
        "        for e in range(1000):\n",
        "            total_loss = []\n",
        "            for i in range(num_batches):\n",
        "                segments = training_data[self.batch_size*i:self.batch_size*(i+1)]\n",
        "                batch_m = [np.zeros((self.mem_len, self.batch_size, self.d_model), dtype=np.float32) for _ in range(self.n_layer)]\n",
        "                for j in range(self.group_size):\n",
        "                    batch_x = segments[:, j, 0, :]\n",
        "                    batch_y = segments[:, j, 1, :]\n",
        "                    # prepare feed dict\n",
        "                    feed_dict = {self.x: batch_x, self.y: batch_y}\n",
        "                    for m, m_np in zip(self.mems_i, batch_m):\n",
        "                        feed_dict[m] = m_np\n",
        "                    # run\n",
        "                    _, gs_, loss_, new_mem_ = self.sess.run([self.train_op, self.global_step, self.avg_loss, self.new_mem], feed_dict=feed_dict)\n",
        "                    batch_m = new_mem_\n",
        "                    total_loss.append(loss_)\n",
        "                    # print ('Current lr: {}'.format(self.sess.run(self.optimizer._lr)))\n",
        "                    print('>>> Epoch: {}, Step: {}, Loss: {:.5f}, Time: {:.2f}'.format(e, gs_, loss_, time.time()-st))\n",
        "                    print('i : ',i,' j : ',j)\n",
        "                    if not i % 500:\n",
        "                        self.saver.save(self.sess, '{}/model-{:03d}-{:.3f}'.format(output_checkpoint_folder, e, np.mean(total_loss)))\n",
        "                    \n",
        "\n",
        "            print ('[epoch {} avg loss] {:.5f}'.format(e, np.mean(total_loss)))\n",
        "            if not e % 6:\n",
        "                self.saver.save(self.sess, '{}/model-{:03d}-{:.3f}'.format(output_checkpoint_folder, e, np.mean(total_loss)))\n",
        "            # stop\n",
        "            if np.mean(total_loss) <= 0.0001:\n",
        "                break\n",
        "\n",
        "    ########################################\n",
        "    # search strategy: temperature (re-shape)\n",
        "    ########################################\n",
        "    def temperature(self, logits, temperature):\n",
        "        probs = np.exp(logits / temperature) / np.sum(np.exp(logits / temperature))\n",
        "        return probs\n",
        "\n",
        "\n",
        "    ########################################\n",
        "    # search strategy: nucleus (truncate)\n",
        "    ########################################\n",
        "    def nucleus(self, probs, p):\n",
        "        probs /= sum(probs)\n",
        "        sorted_probs = np.sort(probs)[::-1]\n",
        "        sorted_index = np.argsort(probs)[::-1]\n",
        "        cusum_sorted_probs = np.cumsum(sorted_probs)\n",
        "        after_threshold = cusum_sorted_probs > p\n",
        "        if sum(after_threshold) > 0:\n",
        "            last_index = np.where(after_threshold)[0][-1]\n",
        "            candi_index = sorted_index[:last_index]\n",
        "        else:\n",
        "            candi_index = sorted_index[:3] # just assign a value\n",
        "        candi_probs = [probs[i] for i in candi_index]\n",
        "        candi_probs /= sum(candi_probs)\n",
        "        word = np.random.choice(candi_index, size=1, p=candi_probs)[0]\n",
        "        return word\n",
        "\n",
        "    ########################################\n",
        "    # evaluate (for batch size = 1)\n",
        "    ########################################\n",
        "    def evaluate(self, notes, num_notes, k, strategies, use_structure=False, init_mem = None):\n",
        "\n",
        "      batch_size = 1\n",
        "      # initialize mem\n",
        "      if init_mem is None:\n",
        "          batch_m = [np.zeros((self.mem_len, batch_size, self.d_model), dtype=np.float32) for _ in range(self.n_layer)]\n",
        "      else:\n",
        "          batch_m = init_mem \n",
        "\n",
        "      initial_flag = True\n",
        "      fail = 0\n",
        "      i = 0\n",
        "\n",
        "      while i < num_notes:\n",
        "            if fail>200:\n",
        "              print('Fail : ',fail)\n",
        "              #continue\n",
        "\n",
        "            # prepare input\n",
        "            if initial_flag:\n",
        "                temp_x = np.zeros((batch_size, len(notes[0])))\n",
        "                for b in range(batch_size):\n",
        "                    for z, t in enumerate(notes[b]):\n",
        "                        temp_x[b][z] = t\n",
        "                initial_flag = False\n",
        "            else:\n",
        "                temp_x = np.zeros((batch_size, 1))\n",
        "                for b in range(batch_size):\n",
        "                    temp_x[b][0] = notes[b][-1]\n",
        "\n",
        "            # prepare feed dict\n",
        "            # inside a feed dict\n",
        "            # placeholder : data\n",
        "            # put input into feed_dict\n",
        "            feed_dict = {self.x: temp_x}\n",
        "\n",
        "            # put memeory into feed_dict\n",
        "            for m, m_np in zip(self.mems_i, batch_m):\n",
        "                feed_dict[m] = m_np\n",
        "            \n",
        "            # model (prediction)\n",
        "            _logits, _new_mem = self.sess.run([self.logits, self.new_mem], feed_dict=feed_dict)\n",
        "            \n",
        "            #print('shape : ',_logits.shape)\n",
        "            logits = _logits[-1, 0]\n",
        "\n",
        "            # temperature or not\n",
        "            if k == 0:\n",
        "              ran = float((np.random.randint(14,16))/10)\n",
        "            else:\n",
        "              ran = float((np.random.randint(7,10))/10)\n",
        "            \n",
        "            probs = self.temperature(logits=logits, temperature=ran)\n",
        "\n",
        "            # sampling\n",
        "            # note : the generated tokenized event\n",
        "            #ran_n = float((np.random.randint(90,98))/100)\n",
        "            note = self.nucleus(probs=probs, p=0.90)\n",
        "            \n",
        "\n",
        "            if note not in tokenizer.index_word:\n",
        "              continue\n",
        "            \n",
        "            \n",
        "\n",
        "            # add new event to record sequence\n",
        "            notes = np.append(notes[0], note)\n",
        "            notes = np.reshape(notes, (1, len(notes)))\n",
        "            #print('notes : ',notes.shape)\n",
        "            \n",
        "            # re-new mem\n",
        "            batch_m = _new_mem\n",
        "            fail = 0\n",
        "            i += 1\n",
        "\n",
        "      return notes[0]\n",
        "\n",
        "    ########################################\n",
        "        # predict (for batch size = 1)\n",
        "    ########################################\n",
        "    def predict(self, notes, num_notes, k, strategies, use_structure=False):\n",
        "      prediction = self.evaluate(notes, num_notes, k, strategies, use_structure)\n",
        "\n",
        "      predicted_sentence = []\n",
        "  \n",
        "      for i in prediction:\n",
        "          # print('helllllo',int(i))\n",
        "          i = int(i)\n",
        "          if i < len(tokenizer.word_index) and i>0:\n",
        "              predicted_sentence.append(tokenizer.index_word[i])\n",
        "      return predicted_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yxPMwYDFKgZ",
        "colab_type": "text"
      },
      "source": [
        "### **Training and Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-s7lij4xpua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_data(notes_chords, sequence_length):\n",
        "    \n",
        "    # sequence_length = 100\n",
        "    notes_input = []\n",
        "    notes_output = []\n",
        "    shift = 1\n",
        "    \n",
        "    for i in range(0, len(notes_chords) - sequence_length, 1):\n",
        "        temp_input = ''\n",
        "        temp_output = ''\n",
        "        for j in range(i,i + sequence_length):\n",
        "            temp_input += notes_chords[j] + ' '\n",
        "        notes_input.append(temp_input)\n",
        "        for j in range(i+shift,i + sequence_length+shift):\n",
        "            temp_output += notes_chords[j] + ' '\n",
        "        notes_output.append(temp_output)\n",
        "\n",
        "\n",
        "    n_patterns = len(notes_input)\n",
        "    # notes_normalized_input = np.reshape(notes_input, (n_patterns, sequence_length))\n",
        "    # notes_normalized_input =  notes_normalized_input / float(n_vocab)\n",
        "    #notes_output = np.array(notes_output)\n",
        "\n",
        "\n",
        "    return (notes_input, notes_output)\n",
        "\n",
        "\n",
        "########################################\n",
        "    # Prepare data\n",
        "########################################\n",
        "        \n",
        "def xl_data(input_, output, group_size):\n",
        "        training_data = []\n",
        "    \n",
        "        pairs = []\n",
        "        for i in range(0, len(input_)):\n",
        "            x, y = input_[i], output[i]\n",
        "            \n",
        "            pairs.append([x, y])\n",
        "\n",
        "        pairs = np.array(pairs)\n",
        "    \n",
        "        # put pairs into training data by groups\n",
        "        for i in range(0, len(pairs) - group_size + 1, group_size):\n",
        "            segment = pairs[i:i+group_size]\n",
        "            assert len(segment) == group_size\n",
        "            training_data.append(segment)\n",
        "            \n",
        "        training_data = np.array(training_data)\n",
        "        \n",
        "        return training_data        \n",
        "        \n",
        "        \n",
        "        \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctu8J2vmwCSd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88566ebe-5e69-4bcb-bd58-d864c3c7d835"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAx_Jb8ir-ak",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "53c9f153-2821-4cff-ad48-beeaa2ab6174"
      },
      "source": [
        "input_window_len = 128\n",
        "pred_steps = 1\n",
        "\n",
        "df, tracks_len_list = concat_all_midi_to_df(\n",
        "    root_dir='/content/drive/My Drive/datasets/drum data/test/3', return_tracks_len_list=True)\n",
        "\n",
        "track_list,instruments = prepare_data(df, input_window_len=input_window_len, pred_steps=1, overlaps=0, train_test_split=0.1,\n",
        "                                                                  tracks_len_list=tracks_len_list, max_instruments=6)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " /content/drive/My Drive/datasets/drum data/test/3/JoeyJordisonDrumSoloFromSurfacing.mid\n",
            "1 drum loops\n",
            "10 percussion instruments\n",
            "809 32-notes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru1OoexF4ta5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_list = []\n",
        "\n",
        "for i in track_list:\n",
        "    i = i.astype(\"int32\")\n",
        "    for j in i:\n",
        "      temp = ''\n",
        "      for k in j:\n",
        "          if str(k) != ' ' and str(k) != ']' and str(k) != '[':\n",
        "              temp += (str(k))\n",
        "      final_list.append(temp)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM_biU6Ds4Bf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with open('/content/drive/My Drive/datasets/saved/drums/final_list', 'wb') as filepath:\n",
        "#      pickle.dump(final_list, filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhxoGiAVw2gG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/datasets/saved/drums/final_list', 'rb') as filepath:\n",
        "         final_list_orig = pickle.load(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeBN7BE46ADs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_notes = list(set(final_list))\n",
        "n_vocab = len(set(unique_notes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VwW4w9s7boR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67fcb67c-6119-4db3-b758-d895fbea26ae"
      },
      "source": [
        "len(final_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "809"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrQH7bpzw4pP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequence_length = 256\n",
        "network_input,network_output = get_data(final_list,sequence_length)\n",
        "# print(network_input.shape)\n",
        "# print(network_output.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZXPykL-w-mN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "148f1944-d172-4b4d-a1b8-072075d922bb"
      },
      "source": [
        "print(n_vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwRN7M2cy5Tc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=' ')\n",
        "tokenizer.fit_on_texts(final_list_orig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzTiTYk_2anm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "outputId": "e2e8f289-1a9d-4f38-d967-854ccdd82fb7"
      },
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'000000': 1,\n",
              " '000001': 18,\n",
              " '000010': 11,\n",
              " '000100': 13,\n",
              " '000101': 25,\n",
              " '001000': 7,\n",
              " '001001': 17,\n",
              " '001010': 20,\n",
              " '001011': 28,\n",
              " '001100': 16,\n",
              " '001101': 32,\n",
              " '010000': 8,\n",
              " '010001': 31,\n",
              " '011000': 6,\n",
              " '011001': 23,\n",
              " '011010': 27,\n",
              " '100000': 2,\n",
              " '100001': 9,\n",
              " '100010': 4,\n",
              " '100011': 22,\n",
              " '100100': 5,\n",
              " '100101': 21,\n",
              " '100110': 24,\n",
              " '101000': 10,\n",
              " '101001': 14,\n",
              " '101010': 19,\n",
              " '101100': 15,\n",
              " '101101': 26,\n",
              " '110000': 3,\n",
              " '110001': 29,\n",
              " '111000': 12,\n",
              " '111100': 30}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCDbPQ1Fw4yL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA3PQpnhxAlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    \n",
        "MAX_LENGTH = n_vocab\n",
        "\n",
        "network_in = tokenizer.texts_to_sequences(network_input)\n",
        "network_in = tf.keras.preprocessing.sequence.pad_sequences(network_in,\n",
        "                                                           padding='post')\n",
        "  \n",
        "network_out = tokenizer.texts_to_sequences(network_output)\n",
        "network_out = tf.keras.preprocessing.sequence.pad_sequences(network_out,\n",
        "                                                       padding='post')\n",
        "VOCAB_SIZE =  len(tokenizer.word_index)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaQYUFdw6tgt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "31a77a02-4be7-46a9-8140-1bdc602eb1ab"
      },
      "source": [
        "network_in"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5,  1,  1, ...,  0,  0,  0],\n",
              "       [ 1,  1,  1, ...,  0,  0,  0],\n",
              "       [ 1,  1,  8, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [ 1,  1, 10, ...,  1,  1,  1],\n",
              "       [ 1, 10,  1, ...,  1,  1,  1],\n",
              "       [10,  1,  1, ...,  1,  1,  1]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa9fZsqPxCMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network_output = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY1Z5mRvxEB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# decoder inputs use the previous target as input\n",
        "# remove START_TOKEN from targets\n",
        "\n",
        "group_size = 3\n",
        "data = xl_data(network_in, network_out, group_size)\n",
        "\n",
        "network_in = []\n",
        "network_out = []\n",
        "\n",
        "train_len = int(len(data)*0.7)\n",
        "\n",
        "training_data = data[:train_len]\n",
        "val_data = data[train_len:]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sASYmpgxF6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbMKi4nJxHqP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7e87524e-fb62-4252-80a5-b633f4a8ad6f"
      },
      "source": [
        "# declare model\n",
        "\n",
        "model = TransformerXL(\n",
        "       vocab_size=VOCAB_SIZE, \n",
        "       checkpoint=None,\n",
        "       is_training=True)\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num parameters: 46074657\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs4Ev-yrxJal",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9579638-cb3b-4172-ec5d-d61e3d8e67f6"
      },
      "source": [
        "VOCAB_SIZE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOQxXCWBxLHf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1ced2d6-6e3d-4140-ae8b-87c0f349389b"
      },
      "source": [
        "training_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 3, 2, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17bniTezxOse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train\n",
        "model.train(training_data, output_checkpoint_folder='/content/drive/My Drive/datasets/weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTVJzEig6Zdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#notes = []\n",
        "#num = np.random.randint(len(val_data)-(2*sequence_length))\n",
        "#num = 88993\n",
        "num_seq = 1\n",
        "#for i in range(num_seq):\n",
        "#    notes.append(val_data[num+int(i*sequence_length/3), i, 0, :])\n",
        "notes = training_data[100, 0, 0, :]\n",
        "notes = np.array(notes)\n",
        "notes = notes.flatten()\n",
        "notes = np.reshape(notes, (1, len(notes)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PRuk9Nc1SpT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4123c18e-9f58-43d3-f459-da9f62d9d310"
      },
      "source": [
        "num_notes = 1024\n",
        "memlen = 3072\n",
        "print('num_notes : ',num_notes)\n",
        "print('memlen : ',memlen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_notes :  1024\n",
            "memlen :  3072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWaaCnmj1VTF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7940ce41-3e78-44e7-c8ee-7624e2b1ce82"
      },
      "source": [
        "# Predict\n",
        "model_p = TransformerXL(\n",
        "       vocab_size=VOCAB_SIZE, \n",
        "       memlen = memlen,\n",
        "       checkpoint='/content/drive/My Drive/datasets/ckpt/model-001-0.027',\n",
        "       is_training=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num parameters: 46074657\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/datasets/ckpt/model-001-0.027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E84sX9L1bI0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "1c81245c-87cc-4364-dbb4-661055f39862"
      },
      "source": [
        "k = 0\n",
        "lens_in = len(notes[0])\n",
        "final_output = []\n",
        "for i in range(1):\n",
        "    print(\"########################################################################## : \",i)\n",
        "    print('lens_in : ',lens_in)\n",
        "    output = model_p.predict(notes, num_notes, k,\n",
        "                             strategies=['temperature', 'nucleus'],\n",
        "                             use_structure=True)\n",
        "    lens = len(output)\n",
        "    notes_temp = []\n",
        "    count = 0\n",
        "    for index, j in enumerate(output):\n",
        "        notes_temp.append(tokenizer.word_index[j])\n",
        "        if index >= sequence_length-1:\n",
        "            final_output.append(j)\n",
        "        \n",
        "\n",
        "    notes = np.array(notes_temp)\n",
        "    lens_in = len(notes)\n",
        "    notes = np.reshape(notes, (1, len(notes)))\n",
        "    #print('\\nlast : ', final_output[-1])\n",
        "    #print('\\nlen : ', lens_in)\n",
        "    print(\"The iteration output : \",final_output,'\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "########################################################################## :  0\n",
            "lens_in :  256\n",
            "The iteration output :  ['000000', '100010', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101001', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100010', '000000', '100010', '000000', '100010', '000000', '100000', '000000', '100000', '000000', '100010', '000000', '100010', '000000', '110000', '000000', '110000', '000000', '110000', '000000', '110000', '000000', '101000', '000000', '000000', '000000', '000000', '000000', '000000', '000000', '101000', '000000', '000000', '000000', '100010', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101001', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100010', '000000', '100010', '000000', '100010', '000000', '100000', '000000', '100000', '000000', '100010', '000000', '100010', '000000', '110000', '000000', '110000', '000000', '110000', '000000', '110000', '000000', '101000', '000000', '000000', '000000', '000000', '000000', '000000', '000000', '101000', '000000', '000000', '000000', '100010', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101001', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100010', '000000', '100010', '000000', '100010', '000000', '100000', '000000', '100000', '000000', '100010', '000000', '100010', '000000', '101000', '000000', '110000', '000000', '110000', '000000', '110000', '000000', '101000', '000000', '000000', '100000', '000000', '100000', '000000', '000000', '101000', '000000', '000000', '000000', '100010', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101001', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '111000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100000', '000000', '100000', '000000', '100000', '000000', '101000', '000000', '100010', '000000', '100010', '000000', '100010', '000000', '100000', '000000', '100000', '000000', '100010', '000000', '100010', '000000', '101000', '000000', '110000', '000000', '110000', '000000', '100000', '000000', '101000', '000000', '000000', '100000', '000000', '100000', '000000', '000000', '101000', '000000', '000000', '000000', '100010', '100000', '000000', '111000'] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlrx27d_AROq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = final_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h0W7MvZB5U8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "befa8e0c-c30d-401d-a2a4-6c415d37615a"
      },
      "source": [
        "pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['000000',\n",
              " '100010',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101001',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100010',\n",
              " '000000',\n",
              " '100010',\n",
              " '000000',\n",
              " '100010',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100010',\n",
              " '000000',\n",
              " '100010',\n",
              " '000000',\n",
              " '110000',\n",
              " '000000',\n",
              " '110000',\n",
              " '000000',\n",
              " '110000',\n",
              " '000000',\n",
              " '110000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '000000',\n",
              " '000000',\n",
              " '000000',\n",
              " '000000',\n",
              " '000000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '000000',\n",
              " '000000',\n",
              " '100010',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101001',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100010',\n",
              " '000000',\n",
              " '100010',\n",
              " '000000',\n",
              " '100010',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100010',\n",
              " '000000',\n",
              " '100010',\n",
              " '000000',\n",
              " '110000',\n",
              " '000000',\n",
              " '110000',\n",
              " '000000',\n",
              " '110000',\n",
              " '000000',\n",
              " '110000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '000000',\n",
              " '000000',\n",
              " '000000',\n",
              " '000000',\n",
              " '000000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '000000',\n",
              " '000000',\n",
              " '100010',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101001',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100010',\n",
              " '000000',\n",
              " '100010',\n",
              " '000000',\n",
              " '100010',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100010',\n",
              " '000000',\n",
              " '100010',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '110000',\n",
              " '000000',\n",
              " '110000',\n",
              " '000000',\n",
              " '110000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '000000',\n",
              " '000000',\n",
              " '100010',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101001',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '111000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '101000',\n",
              " '000000',\n",
              " '100010',\n",
              " '000000',\n",
              " '100010',\n",
              " '000000',\n",
              " '100010',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100000',\n",
              " '000000',\n",
              " '100010',\n",
              " '000000',\n",
              " '100010',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSa_LWMbAWIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_pred = []\n",
        "\n",
        "for i in pred:\n",
        "    temp = []\n",
        "    for j in i:\n",
        "        temp.append(int(j))\n",
        "    final_pred.append(temp)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOvUa9BSAZYM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6649ffc6-9ca7-44d6-e08a-70ce11b4c858"
      },
      "source": [
        "final_pred = np.array(final_pred)\n",
        "print(final_pred.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1025, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WvOiUHAAbYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_song = array_to_midi(final_pred,instruments,160)\n",
        "new_song.save('/content/drive/My Drive/datasets/output/transformer_pred_doublebass2.mid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtJkmfAwCKjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}