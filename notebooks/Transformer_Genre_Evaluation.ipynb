{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OesYYo1qDPX9",
        "outputId": "6142cc9a-1f0d-4cad-a0cf-73c7e8e5b74e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct 22 08:27:12 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   28C    P0    42W / 400W |      3MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  \n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRpRWOGgDajP",
        "outputId": "b4b163ef-1553-4194-8880-2f745c8eb6ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cellView": "code",
        "id": "qB0RpETOOm6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf4e4c20-45c1-4062-8dbf-2b248627a1ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fastai==1.0.61 in /usr/local/lib/python3.7/dist-packages (1.0.61)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.61) (0.13.1+cu113)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.61) (1.7.3)\n",
            "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.61) (3.4.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.61) (4.6.3)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.61) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.61) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.61) (2.23.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.61) (1.12.1+cu113)\n",
            "Requirement already satisfied: fastprogress>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.61) (1.0.3)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.61) (7.352.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.61) (21.3)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.61) (1.3.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.61) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.61) (3.2.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.61) (2.8.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.61) (6.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.61) (2.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.61) (1.9.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.61) (57.4.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.61) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.61) (2.4.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.61) (4.64.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.61) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.61) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.61) (8.1.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.61) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.61) (3.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.61) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.61) (1.0.9)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.61) (0.6.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.61) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.61) (4.1.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai==1.0.61) (1.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=2.0.18->fastai==1.0.61) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->fastai==1.0.61) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=2.0.18->fastai==1.0.61) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==1.0.61) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==1.0.61) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==1.0.61) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==1.0.61) (2022.9.24)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.18->fastai==1.0.61) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.18->fastai==1.0.61) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=2.0.18->fastai==1.0.61) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=2.0.18->fastai==1.0.61) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==1.0.61) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==1.0.61) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==1.0.61) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->fastai==1.0.61) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai==1.0.61) (2022.4)\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# !git clone https://github.com/fastai/fastai\n",
        "!pip install fastai==1.0.61"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "cellView": "code",
        "id": "wq_q8RchfuIz"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/bearpelican/musicautobot.git\n",
        "\n",
        "# %cd musicautobot\n",
        "\n",
        "# # !wget https://repo.anaconda.com/archive/Anaconda3-5.2.0-Linux-x86_64.sh && bash Anaconda3-5.2.0-Linux-x86_64.sh -bfp /usr/local\n",
        "\n",
        "# import sys\n",
        "# sys.path.append('/usr/local/lib/python3.6/site-packages')\n",
        "\n",
        "# # !conda env update -f environment.yml\n",
        "\n",
        "# # !sudo apt-get install musescore\n",
        "\n",
        "# !source activate musicautobot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/\n",
        "# !wget https://lilypond.org/download/binaries/linux-64/lilypond-2.22.2-1.linux-64.sh\n",
        "# !sh /content/lilypond-2.22.2-1.linux-64.sh\n",
        "\n",
        "# !pip install musescore\n",
        "# !pip install timidity"
      ],
      "metadata": {
        "id": "1ys1QH8y9T6C"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import music21"
      ],
      "metadata": {
        "id": "_56OCCbg8Y-T"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !whereis musescore"
      ],
      "metadata": {
        "id": "y8IO23J325qc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# music21.environment.UserSettings().keys()"
      ],
      "metadata": {
        "id": "0ocDHWMr-C35"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# music21.environment.set(\"musescoreDirectPNGPath\", \"/usr/bin/musescore\")\n",
        "# music21.environment.set(\"musicxmlPath\", \"/usr/bin/musescore\")\n",
        "# music21.environment.set(\"lilypondPath\", \"/usr/local/lilypond\")"
      ],
      "metadata": {
        "id": "ivHX88-y24aw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import Image\n",
        "# c = music21.chord.Chord(\"C4 E4 G4\")\n",
        "# c.show('lily.png')\n",
        "# Image(filename=c.write('lily.png'))"
      ],
      "metadata": {
        "id": "4nIAEez47G7E"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !whereis lilypond"
      ],
      "metadata": {
        "id": "mX1PQ51G7jxF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3NdueiGTOkj",
        "outputId": "0c2a5bb1-2d4f-4b97-c3ae-79a333af5aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pebble in /usr/local/lib/python3.7/dist-packages (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pebble"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "Kxz3Bzql7q0E"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-9fmdfiI6Ry"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "cellView": "code",
        "id": "pgErROWE11qg"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import music21\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from enum import Enum\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import *\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "NzmD_jhZxIsS"
      },
      "outputs": [],
      "source": [
        "from fastai.callbacks import EarlyStoppingCallback, SaveModelCallback, ReduceLROnPlateauCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AS45dGeLAxT"
      },
      "source": [
        "## **Utils**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1iIeO4SIY_T"
      },
      "source": [
        "### **Declaration of Helper Variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "cellView": "code",
        "id": "slksmnTo2HiN"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from enum import Enum\n",
        "import music21\n",
        "\n",
        "PIANO_TYPES = list(range(24)) + list(range(80, 96)) # Piano, Synths\n",
        "PLUCK_TYPES = list(range(24, 40)) + list(range(104, 112)) # Guitar, Bass, Ethnic\n",
        "BRIGHT_TYPES = list(range(40, 56)) + list(range(56, 80))\n",
        "\n",
        "PIANO_RANGE = (21, 109) # https://en.wikipedia.org/wiki/Scientific_pitch_notation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "cellView": "code",
        "id": "MYnhon50m2uz"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#Using enums in python\n",
        "class Track(Enum):\n",
        "    PIANO = 0 # discrete instruments - keyboard, woodwinds\n",
        "    PLUCK = 1 # continuous instruments with pitch bend: violin, trombone, synths\n",
        "    BRIGHT = 2\n",
        "    PERC = 3\n",
        "    UNDEF = 4\n",
        "    \n",
        "ype2inst = {\n",
        "    # use print_music21_instruments() to see supported types\n",
        "    Track.PIANO: 0, # Piano\n",
        "    Track.PLUCK: 24, # Guitar\n",
        "    Track.BRIGHT: 40, # Violin\n",
        "    Track.PERC: 114, # Steel Drum\n",
        "}\n",
        "\n",
        "# INFO_TYPES = set(['TIME_SIGNATURE', 'KEY_SIGNATURE'])\n",
        "INFO_TYPES = set(['TIME_SIGNATURE', 'KEY_SIGNATURE', 'SET_TEMPO'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "cellView": "code",
        "id": "dSTIG5UDm48H"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def file2mf(fp):\n",
        "    mf = music21.midi.MidiFile()\n",
        "    if isinstance(fp, bytes):\n",
        "        mf.readstr(fp)\n",
        "    else:\n",
        "        mf.open(fp)\n",
        "        mf.read()\n",
        "        mf.close()\n",
        "    return mf\n",
        "\n",
        "def mf2stream(mf): return music21.midi.translate.midiFileToStream(mf)\n",
        "\n",
        "def is_empty_midi(fp):\n",
        "    if fp is None: return False\n",
        "    mf = file2mf(fp)\n",
        "    return not any([t.hasNotes() for t in mf.tracks])\n",
        "\n",
        "def num_piano_tracks(fp):\n",
        "    music_file = file2mf(fp)\n",
        "    note_tracks = [t for t in music_file.tracks if t.hasNotes() and get_track_type(t) == Track.PIANO]\n",
        "    return len(note_tracks)\n",
        "\n",
        "def is_channel(t, c_val):\n",
        "    return any([c == c_val for c in t.getChannels()])\n",
        "\n",
        "def track_sort(t): # sort by 1. variation of pitch, 2. number of notes\n",
        "    return len(unique_track_notes(t)), len(t.events)\n",
        "\n",
        "def is_piano_note(pitch):\n",
        "    return (pitch >= PIANO_RANGE[0]) and (pitch < PIANO_RANGE[1])\n",
        "\n",
        "def unique_track_notes(t):\n",
        "    return { e.pitch for e in t.events if e.pitch is not None }\n",
        "\n",
        "def compress_midi_file(fp, cutoff=6, min_variation=3, supported_types=set([Track.PIANO, Track.PLUCK, Track.BRIGHT])):\n",
        "    music_file = file2mf(fp)\n",
        "    \n",
        "    info_tracks = [t for t in music_file.tracks if not t.hasNotes()]\n",
        "    note_tracks = [t for t in music_file.tracks if t.hasNotes()]\n",
        "    \n",
        "    if len(note_tracks) > cutoff:\n",
        "        note_tracks = sorted(note_tracks, key=track_sort, reverse=True)\n",
        "        \n",
        "    supported_tracks = []\n",
        "    for idx,t in enumerate(note_tracks):\n",
        "        if len(supported_tracks) >= cutoff: break\n",
        "        track_type = get_track_type(t)\n",
        "        if track_type not in supported_types: continue\n",
        "        pitch_set = unique_track_notes(t)\n",
        "        if (len(pitch_set) < min_variation): continue # must have more than x unique notes\n",
        "        if not all(map(is_piano_note, pitch_set)): continue # must not contain midi notes outside of piano range\n",
        "#         if track_type == Track.UNDEF: print('Could not designate track:', fp, t)\n",
        "        change_track_instrument(t, type2inst[track_type])\n",
        "        supported_tracks.append(t)\n",
        "    if not supported_tracks: return None\n",
        "    music_file.tracks = info_tracks + supported_tracks\n",
        "    return music_file\n",
        "\n",
        "def get_track_type(t):\n",
        "    if is_channel(t, 10): return Track.PERC\n",
        "    i = get_track_instrument(t)\n",
        "    if i in PIANO_TYPES: return Track.PIANO\n",
        "    if i in PLUCK_TYPES: return Track.PLUCK\n",
        "    if i in BRIGHT_TYPES: return Track.BRIGHT\n",
        "    return Track.UNDEF\n",
        "\n",
        "def get_track_instrument(t):\n",
        "    for idx,e in enumerate(t.events):\n",
        "        if e.type == 'PROGRAM_CHANGE': return e.data\n",
        "    return None\n",
        "\n",
        "def change_track_instrument(t, value):\n",
        "    for idx,e in enumerate(t.events):\n",
        "        if e.type == 'PROGRAM_CHANGE': e.data = value\n",
        "\n",
        "def print_music21_instruments():\n",
        "    for i in range(200):\n",
        "        try: print(i, music21.instrument.instrumentFromMidiProgram(i))\n",
        "        except: pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVWjSJChEjsR"
      },
      "source": [
        "### Vocab variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "cellView": "code",
        "id": "EucntAHhEjlK"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#specifying data paths \n",
        "path = 'debussy'\n",
        "\n",
        "BPB = 4 # beats per bar\n",
        "TIMESIG = f'{BPB}/4' # default time signature\n",
        "PIANO_RANGE = (21, 108)\n",
        "NOTE_RANGE = (1,127)\n",
        "VALTSEP = -1 # separator value for numpy encoding\n",
        "VALTCONT = -2 # numpy value for TCONT - needed for compressing chord array\n",
        "\n",
        "SAMPLE_FREQ = 4\n",
        "NOTE_SIZE = 128\n",
        "DUR_SIZE = (10*BPB*SAMPLE_FREQ)+1 # Max length - 8 bars. Or 16 beats/quarternotes\n",
        "MAX_NOTE_DUR = (8*BPB*SAMPLE_FREQ)\n",
        "\n",
        "#tokenizing\n",
        "BOS = 'xxbos'\n",
        "PAD = 'xxpad'\n",
        "EOS = 'xxeos'\n",
        "MASK = 'xxmask' # Used for BERT masked language modeling. \n",
        "#CSEQ = 'xxcseq' # Used for Seq2Seq translation - denotes start of chord sequence\n",
        "#MSEQ = 'xxmseq' # Used for Seq2Seq translation - denotes start of melody sequence\n",
        "#S2SCLS = 'xxs2scls' # deprecated\n",
        "#NSCLS = 'xxnscls' # deprecated\n",
        "SEP = 'xxsep'\n",
        "IN = 'xxni'     #null instrument\n",
        "\n",
        "# Genre Tokens \n",
        "ELECTRONIC = 'xxelec'\n",
        "FOLK = 'xxfolk'\n",
        "FUNK = 'xxfunk'\n",
        "JAZZ = 'xxjazz'\n",
        "POP = 'xxpop'\n",
        "ROCK = 'xxrock'\n",
        "\n",
        "# Instrument to be accepted \n",
        "ACCEP_INS = dict()\n",
        "ACCEP_INS['Piano'] = 0 \n",
        "ACCEP_INS['Guitar'] = 1\n",
        "ACCEP_INS['Bass'] = 2 \n",
        "ACCEP_INS['WoodwindInstrument'] = 3 \n",
        "ACCEP_INS['BrassInstrument'] = 4 \n",
        "ACCEP_INS['StringInstrument'] = 5 \n",
        "ACCEP_INS['Misc'] = 6 \n",
        "\n",
        "ACCEP_INS_REV = {v:k for k,v in zip(ACCEP_INS.keys(), ACCEP_INS.values())}\n",
        "\n",
        "NOTE_TOKS = [f'n{i}' for i in range(NOTE_SIZE)] \n",
        "DUR_TOKS = [f'd{i}' for i in range(DUR_SIZE)]\n",
        "#DONE\n",
        "INS_TOKS = [f'i{i}' for i in range(len(ACCEP_INS.keys()))]\n",
        "\n",
        "NOTE_START, NOTE_END = NOTE_TOKS[0], NOTE_TOKS[-1]\n",
        "DUR_START, DUR_END = DUR_TOKS[0], DUR_TOKS[-1]\n",
        "INS_START, INS_END = INS_TOKS[0], INS_TOKS[-1]\n",
        "\n",
        "MTEMPO_SIZE = 10\n",
        "MTEMPO_OFF = 'mt0'\n",
        "MTEMPO_TOKS = [f'mt{i}' for i in range(MTEMPO_SIZE)]\n",
        "\n",
        "SEQType = Enum('SEQType', 'Mask, Sentence, Melody, Chords, Empty')\n",
        "\n",
        "# Important: SEP token must be last\n",
        "#DONE\n",
        "# Important: IN token must be second last\n",
        "\n",
        "#SPECIAL_TOKS = [BOS, PAD, EOS, S2SCLS, MASK, CSEQ, MSEQ, NSCLS, SEP]\n",
        "SPECIAL_TOKS = [BOS, PAD, EOS, MASK, ELECTRONIC, FOLK, FUNK, JAZZ, POP, ROCK, IN, SEP] # Important: SEP token must be last"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "cellView": "code",
        "id": "1bHYma2jiZ44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fce4ff79-a393-46ab-e391-a41ac50dd283"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Piano': 0,\n",
              " 'Guitar': 1,\n",
              " 'Bass': 2,\n",
              " 'WoodwindInstrument': 3,\n",
              " 'BrassInstrument': 4,\n",
              " 'StringInstrument': 5,\n",
              " 'Misc': 6}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "#@title\n",
        "ACCEP_INS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "sO6ekCxQ_QlF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaf2a6c0-4b5b-4f1f-af2d-dc6b7235959c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Bass',\n",
              " 'BrassInstrument',\n",
              " 'Guitar',\n",
              " 'Misc',\n",
              " 'StringInstrument',\n",
              " 'WoodwindInstrument'}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "ACCEP_INS.keys() - {'Piano'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "cellView": "code",
        "id": "ZvBUQu_8iaMS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f19543f-334f-4400-b49e-91d008099dca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Piano',\n",
              " 1: 'Guitar',\n",
              " 2: 'Bass',\n",
              " 3: 'WoodwindInstrument',\n",
              " 4: 'BrassInstrument',\n",
              " 5: 'StringInstrument',\n",
              " 6: 'Misc'}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "#@title\n",
        "ACCEP_INS_REV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTJ1FHQULEZA"
      },
      "source": [
        "### **Encoding Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a58PehrlA4I"
      },
      "source": [
        "Adapted from https://github.com/bearpelican/musicautobot/blob/master/notebooks/data_encoding/Midi2Tensor.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "cellView": "code",
        "id": "Ew5brC192Jc-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "951ddc50-c992-4b62-cf35-9603a128ed02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Note: not worrying about overlaps - as notes will still play. just look tied\\n# http://web.mit.edu/music21/doc/moduleReference/moduleStream.html#music21.stream.Stream.getOverlaps\\ndef timestep2npenc(timestep, note_range=PIANO_RANGE, enc_type=None):\\n    # inst x pitch\\n    notes = []\\n    a, b = zip(*timestep.nonzero())\\n    for i,n in zip(*timestep.nonzero()):\\n        d = timestep[i,n]\\n        if d < 0: continue # only supporting short duration encoding for now\\n        if n < note_range[0] or n >= note_range[1]: continue # must be within midi range\\n        notes.append([n,d,i])\\n        \\n    notes = sorted(notes, key=lambda x: x[0], reverse=True) # sort by note (highest to lowest)\\n    \\n    if enc_type is None: \\n        # note, duration\\n        return [n[:2] for n in notes] \\n    if enc_type == 'parts':\\n        # note, duration, part\\n        return [n for n in notes]\\n    if enc_type == 'full':\\n        # note_class, duration, octave, instrument\\n        return [[n%12, d, n//12, i] for n,d,i in notes] \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "#@title\n",
        "from fastai.torch_core import ParameterModule\n",
        "def file2stream(fp):\n",
        "    if isinstance(fp, music21.midi.MidiFile): return music21.midi.translate.midiFileToStream(fp)\n",
        "    return music21.converter.parse(fp)\n",
        "\n",
        "def npenc2stream(arr,bpm=120, instr_list = None):\n",
        "    \"Converts numpy encoding to music21 stream\"\n",
        "    chordarr = npenc2chordarr(np.array(arr)) # 1.\n",
        "    return chordarr2stream(chordarr,bpm=bpm, instr_list = instr_list) # 2.\n",
        "\n",
        "# 2.\n",
        "def stream2chordarr(s, note_size=NOTE_SIZE, sample_freq=SAMPLE_FREQ, max_note_dur=MAX_NOTE_DUR):\n",
        "    \"Converts music21.Stream to 1-  numpy array\"\n",
        "    # assuming 4/4 time\n",
        "    # note x instrument x pitch\n",
        "    # FYI: midi middle C value=60\n",
        "    \n",
        "    # (AS) TODO: need to order by instruments most played and filter out percussion or include the channel\n",
        "    highest_time = max(s.flat.getElementsByClass('Note').highestTime, s.flat.getElementsByClass('Chord').highestTime)\n",
        "    maxTimeStep = round(highest_time * sample_freq)+1\n",
        "    score_arr = np.zeros((maxTimeStep, len(s.parts), NOTE_SIZE))\n",
        "\n",
        "    def note_data(pitch, note):\n",
        "        return (pitch.midi, int(round(note.offset*sample_freq)), int(round(note.duration.quarterLength*sample_freq)))\n",
        "    \n",
        "    ins=dict()\n",
        "\n",
        "    # print('---------------------------------------------------------------')\n",
        "\n",
        "    for idx,part in enumerate(s.parts):\n",
        "        \n",
        "        notes=[]\n",
        "        iterate = False\n",
        "        \n",
        "        for elem in part.flat:\n",
        "\n",
        "            # Verbose \n",
        "            # if isinstance(elem,music21.instrument.Instrument):\n",
        "            #   print(elem)\n",
        "            # elif (not isinstance(elem, music21.note.Note)) and (not isinstance(elem, music21.chord.Chord)) and (not isinstance(elem, music21.note.Rest)):\n",
        "            #   print(elem)\n",
        "\n",
        "            if isinstance(elem,music21.instrument.Instrument) and (elem.instrumentName is not None):\n",
        "                # Get the classes for each instrument \n",
        "                #Flawed logic\n",
        "                if elem.instrumentName.replace(\" \", \"\") not in ACCEP_INS.keys():\n",
        "                  classes = set(elem.classes) - {'Instrument', 'Music21Object', 'object', f'{elem.instrumentName.replace(\" \", \"\")}'}\n",
        "                else:\n",
        "                  classes = set(elem.classes) - {'Instrument', 'Music21Object', 'object'}\n",
        "\n",
        "                # Check for piano \n",
        "                if(\"KeyboardInstrument\" in classes):\n",
        "                  ins[idx] = 'Piano'\n",
        "                  iterate = True \n",
        "                # Handle for guitar and bass\n",
        "                elif(elem.instrumentName == 'Guitar' or elem.instrumentName == 'Acoustic Guitar' or elem.instrumentName == 'Electric Guitar'):\n",
        "                  ins[idx] = 'Guitar'\n",
        "                  iterate = True\n",
        "                elif('Guitar' in classes and ('Bass' in elem.instrumentName)):\n",
        "                  ins[idx] = 'Bass'\n",
        "                  iterate = True\n",
        "                # Handle for remaining instruments \n",
        "                elif(len(classes.intersection(ACCEP_INS.keys())) != 0):\n",
        "                  inter = list(classes.intersection(ACCEP_INS.keys()))\n",
        "                  try:\n",
        "                    assert len(inter) <= 1\n",
        "                  except AssertionError:\n",
        "                    print('Intersection with ACCEP_INS have multiple values: ',inter)\n",
        "                  ins[idx] = inter[0]\n",
        "                  iterate = True\n",
        "                else:\n",
        "                  # print(f'instrument rejected : {elem.instrumentName}')\n",
        "                  break\n",
        "\n",
        "                # if elem.instrumentName in ACCEP_INS.keys():\n",
        "                #     ins[idx] = elem.instrumentName \n",
        "                #     iterate = True\n",
        "                # else :\n",
        "                #     print(f'instrument rejected : {elem.instrumentName}')\n",
        "                #     break\n",
        "            elif isinstance(elem,music21.instrument.Instrument) and (elem.instrumentName is  None):\n",
        "                ins[idx] = 'Misc'\n",
        "                iterate = True \n",
        "            \n",
        "            if isinstance(elem, music21.note.Note):\n",
        "                notes.append(note_data(elem.pitch, elem))\n",
        "            if isinstance(elem, music21.chord.Chord):\n",
        "                for p in elem.pitches:\n",
        "                    notes.append(note_data(p, elem)) \n",
        "        \n",
        "        # print('---------------------------------------------------------------')\n",
        "\n",
        "        # sort notes by offset (1), duration (2) so that hits are not overwritten and longer notes have priority\n",
        "        notes_sorted = sorted(notes, key=lambda x: (x[1], x[2])) \n",
        "        \n",
        "        if(iterate == True):\n",
        "            for n in notes_sorted:\n",
        "                if n is None: continue\n",
        "                pitch,offset,duration = n\n",
        "                if max_note_dur is not None and duration > max_note_dur: duration = max_note_dur\n",
        "                score_arr[offset,idx, pitch] = duration\n",
        "                score_arr[offset+1:offset+duration, idx, pitch] = VALTCONT      # Continue holding not\n",
        "    \n",
        "    # def key_function(elem, x):\n",
        "    #   print(x)\n",
        "    #   key = list(x).index(elem)\n",
        "    #   print(key)\n",
        "    #   instrument = ins[key]\n",
        "    #   pos = ACCEP_INS[instrument]\n",
        "    #   return pos\n",
        "\n",
        "    # score_arr_sorted = sorted(score_arr, key=lambda x: key_function(x[1], x))\n",
        "\n",
        "    return score_arr, ins\n",
        "\n",
        "def chordarr2npenc(chordarr, skip_last_rest=True):\n",
        "    # combine instruments\n",
        "    # print(chordarr)\n",
        "\n",
        "    result = []\n",
        "    wait_count = 0\n",
        "    for idx,timestep in enumerate(chordarr):\n",
        "        flat_time = timestep2npenc(timestep)\n",
        "        #DONE\n",
        "        #print(idx, flat_time)\n",
        "        if len(flat_time) == 0:\n",
        "            wait_count += 1\n",
        "        else:\n",
        "            # pitch, octave, duration, instrument\n",
        "            # DONE: Replaced -2 with (-2 - len(NOTE_TOKS) - len(DUR_TOKS)) so that in `npenc2idxenc`,\n",
        "            # `t[:, 2] = t[:, 2] + vocab.ins_range[0]` gives right mapping in vocal.itos\n",
        "            if wait_count > 0: result.append([VALTSEP, wait_count, -2 - len(NOTE_TOKS) - len(DUR_TOKS)])\n",
        "            result.extend(flat_time)\n",
        "            wait_count = 1\n",
        "    if wait_count > 0 and not skip_last_rest: result.append([VALTSEP, wait_count, -2 - len(NOTE_TOKS) - len(DUR_TOKS)])\n",
        "    return np.array(result,dtype = int)\n",
        "    #return np.array(result, dtype=int).reshape(-1, 2) # reshaping. Just in case result is empty\n",
        "\n",
        "'''\n",
        "def chordarr2npenc(chordarr, skip_last_rest=True):\n",
        "    # combine instruments\n",
        "    result = []\n",
        "    wait_count = 0\n",
        "    for idx,timestep in enumerate(chordarr):\n",
        "        flat_time = timestep2npenc(timestep)\n",
        "        if len(flat_time) == 0:\n",
        "            wait_count += 1\n",
        "        else:\n",
        "            # pitch, octave, duration, instrument\n",
        "            if wait_count > 0: result.append([VALTSEP, wait_count])\n",
        "            result.extend(flat_time)\n",
        "            wait_count = 1\n",
        "    if wait_count > 0 and not skip_last_rest: result.append([VALTSEP, wait_count])\n",
        "    return np.array(result, dtype=int).reshape(-1, 2) # reshaping. Just in case result is empty\n",
        "'''\n",
        "\n",
        "# Note: not worrying about overlaps - as notes will still play. just look tied\n",
        "# http://web.mit.edu/music21/doc/moduleReference/moduleStream.html#music21.stream.Stream.getOverlaps\n",
        "def timestep2npenc(timestep, note_range=NOTE_RANGE, enc_type='full'):\n",
        "    \n",
        "    # inst x pitch\n",
        "    notes = []\n",
        "    for i,n in zip(*timestep.nonzero()):\n",
        "        d = timestep[i,n]\n",
        "        if d < 0: continue # only supporting short duration encoding for now\n",
        "        if n < note_range[0] or n >= note_range[1]: continue # must be within midi range\n",
        "        notes.append([n,d,i])\n",
        "        \n",
        "    notes = sorted(notes, key=lambda x: x[0], reverse=True) # sort by note (highest to lowest)\n",
        "    \n",
        "    if enc_type is None: \n",
        "        # note, duration\n",
        "        return [n[:2] for n in notes] \n",
        "    if enc_type == 'parts':\n",
        "        # note, duration, part\n",
        "        return [n for n in notes]\n",
        "    if enc_type == 'full':\n",
        "        # note_class, duration , instrument\n",
        "        return [[n, d, i] for n,d,i in notes] \n",
        "\n",
        "'''\n",
        "# Note: not worrying about overlaps - as notes will still play. just look tied\n",
        "# http://web.mit.edu/music21/doc/moduleReference/moduleStream.html#music21.stream.Stream.getOverlaps\n",
        "def timestep2npenc(timestep, note_range=PIANO_RANGE, enc_type=None):\n",
        "    # inst x pitch\n",
        "    notes = []\n",
        "    a, b = zip(*timestep.nonzero())\n",
        "    for i,n in zip(*timestep.nonzero()):\n",
        "        d = timestep[i,n]\n",
        "        if d < 0: continue # only supporting short duration encoding for now\n",
        "        if n < note_range[0] or n >= note_range[1]: continue # must be within midi range\n",
        "        notes.append([n,d,i])\n",
        "        \n",
        "    notes = sorted(notes, key=lambda x: x[0], reverse=True) # sort by note (highest to lowest)\n",
        "    \n",
        "    if enc_type is None: \n",
        "        # note, duration\n",
        "        return [n[:2] for n in notes] \n",
        "    if enc_type == 'parts':\n",
        "        # note, duration, part\n",
        "        return [n for n in notes]\n",
        "    if enc_type == 'full':\n",
        "        # note_class, duration, octave, instrument\n",
        "        return [[n%12, d, n//12, i] for n,d,i in notes] \n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BFrRG78Irgq"
      },
      "source": [
        "### **Decoding Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "IVYc3-28cN27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da476b60-ddd7-49aa-cd14-dd5a827ed51f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['Piano', 'Guitar', 'Bass', 'WoodwindInstrument', 'BrassInstrument', 'StringInstrument', 'Misc'])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "ACCEP_INS.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "cellView": "code",
        "id": "9i715if1IzvV"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# 1.\n",
        "def npenc2chordarr(npenc,note_size=NOTE_SIZE):\n",
        "    num_instruments = 1 if npenc.shape[1] <= 2 else npenc.max(axis=0)[-1]\n",
        "    max_len = npenc_len(npenc)\n",
        "    # score_arr = (steps, inst, note)\n",
        "    score_arr = np.zeros((max_len, num_instruments + 1, note_size))\n",
        "    \n",
        "    idx = 0\n",
        "    for step in npenc:\n",
        "        n,d,i = (step.tolist()+[0])[:3] # or n,d,i\n",
        "        if n < VALTSEP: continue # special token\n",
        "        if n == VALTSEP:\n",
        "            idx += d\n",
        "            continue\n",
        "        score_arr[idx,i,n] = d\n",
        "    return score_arr\n",
        "\n",
        "def npenc_len(npenc):\n",
        "    duration = 0\n",
        "    for t in npenc:\n",
        "        if t[0] == VALTSEP: duration += t[1]\n",
        "    return duration + 1\n",
        "\n",
        "\n",
        "# 2.\n",
        "def chordarr2stream(arr,sample_freq=SAMPLE_FREQ, bpm=120, instr_list = None):\n",
        "    duration = music21.duration.Duration(1. / sample_freq)\n",
        "    stream = music21.stream.Score()\n",
        "    stream.append(music21.meter.TimeSignature(TIMESIG))\n",
        "    stream.append(music21.tempo.MetronomeMark(number=bpm))\n",
        "    stream.append(music21.key.KeySignature(0))\n",
        "    for inst in range(arr.shape[1]):\n",
        "        p = partarr2stream(arr[:,inst,:],inst,duration)\n",
        "        #print(p.getInstrument())\n",
        "        if instr_list is not None and str(p.getInstrument()) not in instr_list:\n",
        "          #print('+', instr_list)\n",
        "          continue\n",
        "        stream.append(p)\n",
        "    stream = stream.transpose(0)\n",
        "    return stream\n",
        "\n",
        "# 2b.\n",
        "def partarr2stream(partarr,inst,duration):\n",
        "    \"convert instrument part to music21 chords\"\n",
        "#    part = music21.stream.Part()\n",
        "#    part.append(music21.instrument.Piano())\n",
        "#    part_append_duration_notes(partarr, duration, part) # notes already have duration calculated\n",
        "    l = len(ACCEP_INS_REV) \n",
        "    inst = inst%l\n",
        "    part = music21.stream.Part()\n",
        "    if(ACCEP_INS_REV[inst] == 'Piano'):\n",
        "        part.append(music21.instrument.Piano())\n",
        "    #DONE\n",
        "    elif(ACCEP_INS_REV[inst] == 'Bass'):\n",
        "        part.append(music21.instrument.AcousticBass())\n",
        "    elif(ACCEP_INS_REV[inst] == 'Guitar'):\n",
        "        part.append(music21.instrument.AcousticGuitar())  \n",
        "    elif(ACCEP_INS_REV[inst] == 'WoodwindInstrument'):\n",
        "        part.append(music21.instrument.TenorSaxophone())\n",
        "    elif(ACCEP_INS_REV[inst] == 'BrassInstrument'):\n",
        "        part.append(music21.instrument.Trumpet())   \n",
        "    \n",
        "    elif(ACCEP_INS_REV[inst] == 'Trumpet'):\n",
        "        part.append(music21.instrument.Trumpet())\n",
        "    elif(ACCEP_INS_REV[inst] == 'Tenor Saxophone'):\n",
        "        part.append(music21.instrument.TenorSaxophone())\n",
        "    elif(ACCEP_INS_REV[inst] == 'Vibraphone'):\n",
        "        part.append(music21.instrument.Vibraphone())\n",
        "    elif(ACCEP_INS_REV[inst] == 'Baritone Saxophone'):\n",
        "        part.append(music21.instrument.BaritoneSaxophone())\n",
        "    elif(ACCEP_INS_REV[inst] == 'Acoustic Bass'):\n",
        "        part.append(music21.instrument.AcousticBass())\n",
        "    elif(ACCEP_INS_REV[inst] == 'Trombone'):\n",
        "        part.append(music21.instrument.Trombone())\n",
        "    elif(ACCEP_INS_REV[inst] == 'Flute'):\n",
        "        part.append(music21.instrument.Flute())\n",
        "    elif(ACCEP_INS_REV[inst] == 'Saxophone'):\n",
        "        part.append(music21.instrument.Saxophone())\n",
        "    elif(ACCEP_INS_REV[inst] == 'Electric Bass'):\n",
        "        part.append(music21.instrument.ElectricBass())\n",
        "    elif(ACCEP_INS_REV[inst] == 'Electric Guitar'):\n",
        "        part.append(music21.instrument.ElectricGuitar())\n",
        "    elif(ACCEP_INS_REV[inst] == 'Acoustic Guitar'):\n",
        "        part.append(music21.instrument.AcousticGuitar())\n",
        "    elif(ACCEP_INS_REV[inst] == 'Glockenspiel'):\n",
        "        part.append(music21.instrument.Glockenspiel())\n",
        "    elif(ACCEP_INS_REV[inst] == 'Vibraphone'):\n",
        "        part.append(music21.instrument.Vibraphone())\n",
        "    elif(ACCEP_INS_REV[inst] == 'Violin'):\n",
        "        part.append(music21.instrument.Violin())\n",
        "    else:\n",
        "        part.append(music21.instrument.Piano())\n",
        "    part_append_duration_notes(partarr, duration, part)\n",
        "    \n",
        "\n",
        "    return part\n",
        "\n",
        "def part_append_duration_notes(partarr, duration, stream):\n",
        "    \"convert instrument part to music21 chords\"\n",
        "    for tidx,t in enumerate(partarr):\n",
        "        note_idxs = np.where(t > 0)[0] # filter out any negative values (continuous mode)\n",
        "        if len(note_idxs) == 0: continue\n",
        "        notes = []\n",
        "        for nidx in note_idxs:\n",
        "            note = music21.note.Note(nidx)\n",
        "            note.duration = music21.duration.Duration(partarr[tidx,nidx]*duration.quarterLength)\n",
        "            notes.append(note)\n",
        "        for g in group_notes_by_duration(notes):\n",
        "            if len(g) == 1:\n",
        "                stream.insert(tidx*duration.quarterLength, g[0])\n",
        "            else:\n",
        "                chord = music21.chord.Chord(g)\n",
        "                stream.insert(tidx*duration.quarterLength, chord)\n",
        "    return stream\n",
        "\n",
        "from itertools import groupby\n",
        "#  combining notes with different durations into a single chord may overwrite conflicting durations. Example: aylictal/still-waters-run-deep\n",
        "def group_notes_by_duration(notes):\n",
        "    \"separate notes into chord groups\"\n",
        "    keyfunc = lambda n: n.duration.quarterLength\n",
        "    notes = sorted(notes, key=keyfunc)\n",
        "    return [list(g) for k,g in groupby(notes, keyfunc)]\n",
        "\n",
        "\n",
        "# Midi -> npenc Conversion helpers\n",
        "def is_valid_npenc(npenc, note_range=PIANO_RANGE, max_dur=DUR_SIZE, \n",
        "                   min_notes=32, input_path=None, verbose=True):\n",
        "    if len(npenc) < min_notes:\n",
        "        if verbose: print('Sequence too short:', len(npenc), input_path)\n",
        "        return False\n",
        "    if (npenc[:,1] >= max_dur).any(): \n",
        "        if verbose: print(f'npenc exceeds max {max_dur} duration:', npenc[:,1].max(), input_path)\n",
        "        return False\n",
        "    # https://en.wikipedia.org/wiki/Scientific_pitch_notation - 88 key range - 21 = A0, 108 = C8\n",
        "    if ((npenc[...,0] > VALTSEP) & ((npenc[...,0] < note_range[0]) | (npenc[...,0] >= note_range[1]))).any(): \n",
        "        print(f'npenc out of piano note range {note_range}:', input_path)\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# seperates overlapping notes to different tracks\n",
        "def remove_overlaps(stream, separate_chords=True):\n",
        "    if not separate_chords:\n",
        "        return stream.flat.makeVoices().voicesToParts()\n",
        "    return separate_melody_chord(stream)\n",
        "\n",
        "# seperates notes and chords to different tracks\n",
        "def separate_melody_chord(stream):\n",
        "    new_stream = music21.stream.Score()\n",
        "    if stream.timeSignature: new_stream.append(stream.timeSignature)\n",
        "    new_stream.append(stream.metronomeMarkBoundaries()[0][-1])\n",
        "    if stream.keySignature: new_stream.append(stream.keySignature)\n",
        "    \n",
        "    melody_part = music21.stream.Part(stream.flat.getElementsByClass('Note'))\n",
        "    melody_part.insert(0, stream.getInstrument())\n",
        "    chord_part = music21.stream.Part(stream.flat.getElementsByClass('Chord'))\n",
        "    chord_part.insert(0, stream.getInstrument())\n",
        "    new_stream.append(melody_part)\n",
        "    new_stream.append(chord_part)\n",
        "    return new_stream\n",
        "    \n",
        " # processing functions for sanitizing data\n",
        "\n",
        "def compress_chordarr(chordarr):\n",
        "    return shorten_chordarr_rests(trim_chordarr_rests(chordarr))\n",
        "\n",
        "def trim_chordarr_rests(arr, max_rests=4, sample_freq=SAMPLE_FREQ):\n",
        "    # max rests is in quarter notes\n",
        "    # max 1 bar between song start and end\n",
        "    start_idx = 0\n",
        "    max_sample = max_rests*sample_freq\n",
        "    for idx,t in enumerate(arr):\n",
        "        if (t != 0).any(): break\n",
        "        start_idx = idx+1\n",
        "        \n",
        "    end_idx = 0\n",
        "    for idx,t in enumerate(reversed(arr)):\n",
        "        if (t != 0).any(): break\n",
        "        end_idx = idx+1\n",
        "    start_idx = start_idx - start_idx % max_sample\n",
        "    end_idx = end_idx - end_idx % max_sample\n",
        "#     if start_idx > 0 or end_idx > 0: print('Trimming rests. Start, end:', start_idx, len(arr)-end_idx, end_idx)\n",
        "    return arr[start_idx:(len(arr)-end_idx)]\n",
        "\n",
        "def shorten_chordarr_rests(arr, max_rests=8, sample_freq=SAMPLE_FREQ):\n",
        "    # max rests is in quarter notes\n",
        "    # max 2 bar pause\n",
        "    rest_count = 0\n",
        "    result = []\n",
        "    max_sample = max_rests*sample_freq\n",
        "    for timestep in arr:\n",
        "        if (timestep==0).all(): \n",
        "            rest_count += 1\n",
        "        else:\n",
        "            if rest_count > max_sample:\n",
        "#                 old_count = rest_count\n",
        "                rest_count = (rest_count % sample_freq) + max_sample\n",
        "#                 print(f'Compressing rests: {old_count} -> {rest_count}')\n",
        "            for i in range(rest_count): result.append(np.zeros(timestep.shape))\n",
        "            rest_count = 0\n",
        "            result.append(timestep)\n",
        "    for i in range(rest_count): result.append(np.zeros(timestep.shape))\n",
        "    return np.array(result)\n",
        "\n",
        "# sequence 2 sequence convenience functions\n",
        "\n",
        "def stream2npenc_parts(stream, sort_pitch=True):\n",
        "    chordarr = stream2chordarr(stream)\n",
        "    _,num_parts,_ = chordarr.shape\n",
        "    parts = [part_enc(chordarr, i) for i in range(num_parts)]\n",
        "    return sorted(parts, key=avg_pitch, reverse=True) if sort_pitch else parts\n",
        "\n",
        "def chordarr_combine_parts(parts):\n",
        "    max_ts = max([p.shape[0] for p in parts])\n",
        "    parts_padded = [pad_part_to(p, max_ts) for p in parts]\n",
        "    chordarr_comb = np.concatenate(parts_padded, axis=1)\n",
        "    return chordarr_comb\n",
        "\n",
        "def pad_part_to(p, target_size):\n",
        "    pad_width = ((0,target_size-p.shape[0]),(0,0),(0,0))\n",
        "    return np.pad(p, pad_width, 'constant')\n",
        "\n",
        "def part_enc(chordarr, part):\n",
        "    partarr = chordarr[:,part:part+1,:]\n",
        "    npenc = chordarr2npenc(partarr)\n",
        "    return npenc\n",
        "\n",
        "def avg_tempo(t, sep_idx=VALTSEP):\n",
        "    avg = t[t[:, 0] == sep_idx][:, 1].sum()/t.shape[0]\n",
        "    avg = int(round(avg/SAMPLE_FREQ))\n",
        "    return 'mt'+str(min(avg, MTEMPO_SIZE-1))\n",
        "\n",
        "def avg_pitch(t, sep_idx=VALTSEP):\n",
        "    return t[t[:, 0] > sep_idx][:, 0].mean()   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8smagvrBQuz3"
      },
      "source": [
        "###Extra fastai utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "cellView": "code",
        "id": "LIhQM8qDDGJ5"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def check_valid_ins(ins):\n",
        "  count = 0\n",
        "  ls = list(set(val for val in ins.values()))\n",
        "  for i in ls:\n",
        "    if i == 'Piano':\n",
        "      count+= 1\n",
        "    elif i == 'Acoustic Bass' or i == 'Electric Bass':\n",
        "      count += 1\n",
        "    elif i == 'Acoustic Guitar' or i == 'Electric Guitar':\n",
        "      count += 1\n",
        "    elif i == 'Violin':\n",
        "      count += 1\n",
        "    elif i == 'Saxophone':\n",
        "      count += 1\n",
        "  if(count>=3):\n",
        "    return True\n",
        "  return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "cellView": "code",
        "id": "x8i1aP_adbit"
      },
      "outputs": [],
      "source": [
        "import time \n",
        "\n",
        "#DONE\n",
        "def fastai_num_track_filter (arg, num_ins_thresh = 1):\n",
        "  global time_taken_avg, processed_files\n",
        "\n",
        "  t1 = time.time()\n",
        "  # print('-> Processing file no. ', files_dict[arg])\n",
        "  # Try for inconsistent vocab and file errors  \n",
        "  try:\n",
        "    filename, file_extension = os.path.splitext(arg)\n",
        "    if file_extension == '.mid':\n",
        "      item = MusicItem.from_file(arg, data_vocab)\n",
        "      data_vocab.textify(item.data)\n",
        "    elif file_extension == '.npy':\n",
        "      nparr = np.load(arg, allow_pickle=True)\n",
        "      item = MusicItem.from_npenc(nparr, data_vocab)\n",
        "      data_vocab.textify(item.data)\n",
        "\n",
        "  except:\n",
        "    print('\\t file discarded : ', arg)\n",
        "    os.makedirs('/content/drive/MyDrive/datasets/discarded', exist_ok = True) \n",
        "    shutil.move(arg, os.path.join('/content/drive/MyDrive/datasets/discarded',os.path.basename(arg)))\n",
        "\n",
        "    # t2 = time.time()\n",
        "    # time_taken_avg = ((t2 - t1) + time_taken_avg*processed_files)/(processed_files + 1)\n",
        "    # processed_files += 1\n",
        "\n",
        "    # print(f'\\t estimated_time : {time_taken_avg*len(files_dict)}')\n",
        "    return False \n",
        "  \n",
        "  # t2 = time.time()\n",
        "  # time_taken_avg = ((t2 - t1) + time_taken_avg*processed_files)/(processed_files + 1)\n",
        "  # processed_files += 1\n",
        "  \n",
        "  # Check for no. of instruments \n",
        "  # print(item.ins)\n",
        "\n",
        "\n",
        "\n",
        "  if (item.ins is not None) and (len(item.ins.keys()) >= num_ins_thresh):\n",
        "    print('\\t file accepted : ', arg)\n",
        "    print(f'\\t {item.ins}')\n",
        "    # print(f'\\t estimated_time : {time_taken_avg*len(files_dict)}')\n",
        "    return True\n",
        "  #Else if we did not store the track -> instrument dictionary from MIDI file\n",
        "  elif item.ins is None:\n",
        "    # print(item.data.shape)\n",
        "    lst =  list(item.data)\n",
        "    # print(lst)\n",
        "    cond_lst = [True if ( (x >= data_vocab.ins_range[0] and x < data_vocab.ins_range[1]) or (x == data_vocab.stoi['xxni']) ) else False for x in lst]\n",
        "    ins_idxs = item.data[cond_lst]\n",
        "    # print([data_vocab.itos[x] for index,x in enumerate(lst) if index%3 == 0])\n",
        "    uniq_ins = np.unique(ins_idxs)\n",
        "    num_ins = len(uniq_ins)\n",
        "    if num_ins >= num_ins_thresh:\n",
        "      print('\\t file accepted : ', arg)\n",
        "      print(f'\\t {[data_vocab.itos[x] for x in uniq_ins]}')\n",
        "      return True\n",
        "    else:\n",
        "      print('\\t file discarded due to less instruments : ', arg)\n",
        "      return False\n",
        "\n",
        "  else:\n",
        "    print('\\t file discarded due to less instruments : ', arg)\n",
        "    # print(f'\\t estimated_time : {time_taken_avg*len(files_dict)}')\n",
        "    return False\n",
        "\n",
        "  # ins = dict()\n",
        "\n",
        "  # if not is_empty_midi(arg):\n",
        "  #   s = file2stream(arg)\n",
        "    \n",
        "  #   for idx,part in enumerate(s.parts):\n",
        "  #     for elem in part.flat:\n",
        "  #       if (isinstance(elem,music21.instrument.Instrument)) and (elem.instrumentName is not None): \n",
        "  #         # DONE\n",
        "  #         # Get the classes for each instrument \n",
        "  #         if elem.instrumentName.replace(\" \", \"\") not in ACCEP_INS.keys():\n",
        "  #           classes = set(elem.classes) - {'Instrument', 'Music21Object', 'object', f'{elem.instrumentName.replace(\" \", \"\")}'}\n",
        "  #         else:\n",
        "  #           classes = set(elem.classes) - {'Instrument', 'Music21Object', 'object'}\n",
        "          \n",
        "  #         # Check for piano \n",
        "  #         if(\"KeyboardInstrument\" in classes):\n",
        "  #           if('Piano' not in ins.keys()): ins['Piano'] = 1\n",
        "  #           else: ins['Piano'] += 1\n",
        "  #         # Handle for guitar and bass\n",
        "  #         elif(elem.instrumentName == 'Guitar' or elem.instrumentName == 'Acoustic Guitar' or elem.instrumentName == 'Electric Guitar'):\n",
        "  #           if('Guitar' not in ins.keys()): ins['Guitar'] = 1\n",
        "  #           else: ins['Guitar'] += 1\n",
        "  #         elif('Guitar' in classes and ('Bass' in elem.instrumentName)):\n",
        "  #           if('Bass' not in ins.keys()): ins['Bass'] = 1\n",
        "  #           else: ins['Bass'] += 1\n",
        "  #         # Handle for remaining instruments \n",
        "  #         else:\n",
        "  #           inter = list(classes.intersection(ACCEP_INS.keys()))\n",
        "  #           if(len(inter) != 0):\n",
        "  #             if(inter[0] not in ins.keys()): ins[inter[0]] = 1\n",
        "  #             else: ins[inter[0]] += 1\n",
        "  #       elif isinstance(elem,music21.instrument.Instrument) and (elem.instrumentName is None):\n",
        "  #           if('Misc' not in ins.keys()): ins['Misc'] = 1\n",
        "  #           else: ins['Misc'] += 1\n",
        "  #       else: \n",
        "  #         break \n",
        "  #         # if elem.instrumentName in (ACCEP_INS.keys()):\n",
        "  #         #     ins_count += 1\n",
        "  #         # else :\n",
        "  #         #     break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xui0oBW0LPDp"
      },
      "source": [
        "## **Transformer-XL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aidEA1QWMSvz"
      },
      "source": [
        "####TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "cellView": "code",
        "id": "NDFDRh7RMUaF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "940f5f1e-d18e-4b00-fdd0-f513b0a8e81e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n(i) Correct the filter function : fast_ai_filter_function \\n(ii) filter the dataset / enforce max duration + notes + instruments in : npenc2idxenc \\n(ii.v) Add xxbos, xxeos, xxpad etc\\n\\n-> Balance the data (currently imbalanced towards piano, which also might be the cause for 'dense' outputs for piano in terms of num. notes each timestep)\\n-> One thing that could help with the above is to group multiple instruments together, eg: merging AcousticGuitar, ElectricGuitar, BassGuitar into StringInstrument\\n-> Filter output of model properly (.to_stream() fails many times due to corner cases violating 'partnerless' n or d or i\\n-> Fix the flawed logic of conversion of every song to 4/4 time signature by rounding off, and then add time signature tokens to model vocab \\n  and dataset in order to get consistent output.\\n\\n\\n(iv) train :)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "#@title\n",
        "'''\n",
        "(i) Correct the filter function : fast_ai_filter_function \n",
        "(ii) filter the dataset / enforce max duration + notes + instruments in : npenc2idxenc \n",
        "(ii.v) Add xxbos, xxeos, xxpad etc\n",
        "\n",
        "-> Balance the data (currently imbalanced towards piano, which also might be the cause for 'dense' outputs for piano in terms of num. notes each timestep)\n",
        "-> One thing that could help with the above is to group multiple instruments together, eg: merging AcousticGuitar, ElectricGuitar, BassGuitar into StringInstrument\n",
        "-> Filter output of model properly (.to_stream() fails many times due to corner cases violating 'partnerless' n or d or i\n",
        "-> Fix the flawed logic of conversion of every song to 4/4 time signature by rounding off, and then add time signature tokens to model vocab \n",
        "  and dataset in order to get consistent output.\n",
        "\n",
        "\n",
        "(iv) train :)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1QRzYem_C79"
      },
      "source": [
        "###**vocab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "cellView": "code",
        "id": "7Mbu10XY_Cdw"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#SEE 'Vocab variables'for more details\n",
        "\n",
        "# Vocab - token to index mapping\n",
        "class MusicVocab():\n",
        "    \"Contain the correspondence between numbers and tokens and numericalize.\"\n",
        "    def __init__(self, itos:Collection[str]):\n",
        "        self.itos = itos\n",
        "        self.stoi = {v:k for k,v in enumerate(self.itos)}\n",
        "\n",
        "    def numericalize(self, t:Collection[str]) -> List[int]:\n",
        "        \"Convert a list of tokens `t` to their ids.\"\n",
        "        return [self.stoi[w] for w in t]\n",
        "\n",
        "    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n",
        "        \"Convert a list of `nums` to their tokens.\"\n",
        "        items = [self.itos[i] for i in nums]\n",
        "        return sep.join(items) if sep is not None else items\n",
        "    \n",
        "    #DONE\n",
        "    def to_music_item(self, idxenc, ins = None):\n",
        "        return MusicItem(idxenc, self, ins)\n",
        "    \n",
        "    @property \n",
        "    def mask_idx(self): return self.stoi[MASK]\n",
        "    @property \n",
        "    def pad_idx(self): return self.stoi[PAD]\n",
        "    @property\n",
        "    def bos_idx(self): return self.stoi[BOS]\n",
        "    @property\n",
        "    def sep_idx(self): return self.stoi[SEP]\n",
        "    #DONE\n",
        "    @property\n",
        "    def ni_idx(self): return self.stoi[IN]\n",
        "    @property\n",
        "    #DONE: changed 'DUR_END' to 'INS_END'\n",
        "    def npenc_range(self): return (self.stoi[IN], self.stoi[INS_END]+1)\n",
        "    @property\n",
        "    def note_range(self): return self.stoi[NOTE_START], self.stoi[NOTE_END]+1\n",
        "    @property\n",
        "    def dur_range(self): return self.stoi[DUR_START], self.stoi[DUR_END]+1\n",
        "    #DONE\n",
        "    @property\n",
        "    def ins_range(self): return self.stoi[INS_START], self.stoi[INS_END]+1\n",
        "\n",
        "    def is_duration(self, idx): \n",
        "        return idx >= self.dur_range[0] and idx < self.dur_range[1]\n",
        "    def is_duration_or_pad(self, idx):\n",
        "        return idx == self.pad_idx or self.is_duration(idx)\n",
        "    #DONE\n",
        "    def is_note(self, idx): \n",
        "        return idx == self.sep_idx or (idx >= self.note_range[0] and idx < self.note_range[1])\n",
        "    def is_ins(self, idx):\n",
        "        return idx == self.ni_idx or (idx >= self.ins_range[0] and idx < self.ins_range[1])\n",
        "    def __getstate__(self):\n",
        "        return {'itos':self.itos}\n",
        "\n",
        "    def __setstate__(self, state:dict):\n",
        "        self.itos = state['itos']\n",
        "        self.stoi = {v:k for k,v in enumerate(self.itos)}\n",
        "        \n",
        "    def __len__(self): return len(self.itos)\n",
        "\n",
        "    def save(self, path):\n",
        "        \"Save `self.itos` in `path`\"\n",
        "        pickle.dump(self.itos, open(path, 'wb'))\n",
        "\n",
        "    @classmethod\n",
        "    def create(cls) -> 'Vocab':\n",
        "        \"Create a vocabulary from a set of `tokens`.\"\n",
        "        #DONE\n",
        "        #itos = SPECIAL_TOKS + NOTE_TOKS + DUR_TOKS + MTEMPO_TOKS\n",
        "        itos = SPECIAL_TOKS + NOTE_TOKS + DUR_TOKS + INS_TOKS + MTEMPO_TOKS\n",
        "        \n",
        "        if len(itos)%8 != 0:\n",
        "            itos = itos + [f'dummy{i}' for i in range(len(itos)%8)]\n",
        "        return cls(itos)\n",
        "    \n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        \"Load the `Vocab` contained in `path`\"\n",
        "        itos = pickle.load(open(path, 'rb'))\n",
        "        return cls(itos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmrfdfpI-ydB"
      },
      "source": [
        "###**dataloader.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "cellView": "code",
        "id": "UIJ8DukQPWKi"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import fastai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "cellView": "code",
        "id": "8bMKaXgl98XH"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#https://github.com/bearpelican/musicautobot/blob/master/musicautobot/music_transformer/dataloader.py\n",
        "\n",
        "\"Fastai Language Model Databunch modified to work with music\"\n",
        "from fastai.basics import *\n",
        "# from fastai.basic_data import DataBunch\n",
        "from fastai.text.data import LMLabelList\n",
        "#from .transform import *\n",
        "#from ..vocab import MusicVocab\n",
        "\n",
        "\n",
        "class MusicDataBunch(DataBunch):\n",
        "    \"Create a `TextDataBunch` suitable for training a language model.\"\n",
        "    @classmethod\n",
        "    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', no_check:bool=False, bs=64, val_bs:int=None, \n",
        "               num_workers:int=0, device:torch.device=None, collate_fn:Callable=data_collate, \n",
        "               dl_tfms:Optional[Collection[Callable]]=None, bptt:int=70,\n",
        "               preloader_cls=None, shuffle_dl=False, transpose_range=(0,12), **kwargs) -> DataBunch:\n",
        "        \"Create a `TextDataBunch` in `path` from the `datasets` for language modelling.\"\n",
        "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
        "        preloader_cls = MusicPreloader if preloader_cls is None else preloader_cls\n",
        "        val_bs = ifnone(val_bs, bs)\n",
        "        datasets = [preloader_cls(ds, shuffle=(i==0), bs=(bs if i==0 else val_bs), bptt=bptt, transpose_range=transpose_range, **kwargs) \n",
        "                    for i,ds in enumerate(datasets)]\n",
        "        val_bs = bs\n",
        "        dl_tfms = [partially_apply_vocab(tfm, train_ds.vocab) for tfm in listify(dl_tfms)]\n",
        "        dls = [DataLoader(d, b, shuffle=shuffle_dl) for d,b in zip(datasets, (bs,val_bs,val_bs,val_bs)) if d is not None]\n",
        "        return cls(*dls, path=path, device=device, dl_tfms=dl_tfms, collate_fn=collate_fn, no_check=no_check)\n",
        "    \n",
        "    @classmethod    \n",
        "    def from_folder(cls, path:PathOrStr, extensions='.npy', **kwargs):\n",
        "        files = get_files(path, extensions=extensions, recurse=True);\n",
        "        return cls.from_files(files, path, **kwargs)\n",
        "    \n",
        "    @classmethod\n",
        "    def from_files(cls, files, path, processors=None, split_pct=0.1, \n",
        "                   vocab=None, list_cls=None, **kwargs):\n",
        "        if vocab is None: vocab = MusicVocab.create()\n",
        "        if list_cls is None: list_cls = MusicItemList\n",
        "        src = (list_cls(items=files, path=path, processor=processors, vocab=vocab)\n",
        "                .filter_by_func(fastai_num_track_filter)\n",
        "                .split_by_rand_pct(split_pct, seed=6)\n",
        "                .label_const(label_cls=LMLabelList))\n",
        "        return src.databunch(**kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def empty(cls, path, **kwargs):\n",
        "        vocab = MusicVocab.create()\n",
        "        src = MusicItemList([], path=path, vocab=vocab, ignore_empty=True).split_none()\n",
        "        return src.label_const(label_cls=LMLabelList).databunch()\n",
        "        \n",
        "def partially_apply_vocab(tfm, vocab):\n",
        "    if 'vocab' in inspect.getfullargspec(tfm).args:\n",
        "        return partial(tfm, vocab=vocab)\n",
        "    return tfm\n",
        "    \n",
        "class MusicItemList(ItemList):\n",
        "    _bunch = MusicDataBunch\n",
        "    \n",
        "    def __init__(self, items:Iterator, vocab:MusicVocab=None, **kwargs):\n",
        "        super().__init__(items, **kwargs)\n",
        "        self.vocab = vocab\n",
        "        self.copy_new += ['vocab']\n",
        "    \n",
        "    def get(self, i):\n",
        "        o = super().get(i)\n",
        "        if is_pos_enc(o): \n",
        "            return MusicItem.from_idx(o, self.vocab)\n",
        "        return MusicItem(o, self.vocab)\n",
        "\n",
        "def is_pos_enc(idxenc):\n",
        "    if len(idxenc.shape) == 2 and idxenc.shape[0] == 2: return True\n",
        "    return idxenc.dtype == np.object and idxenc.shape == (2,)\n",
        "\n",
        "class MusicItemProcessor(PreProcessor):\n",
        "    \"`PreProcessor` that transforms numpy files to indexes for training\"\n",
        "    def process_one(self,item):\n",
        "        item, genre = item\n",
        "        item = MusicItem.from_npenc(item, vocab=self.vocab, genre = genre)\n",
        "        return item.to_idx()\n",
        "    \n",
        "    def process(self, ds):\n",
        "        self.vocab = ds.vocab\n",
        "        super().process(ds)\n",
        "        \n",
        "class OpenNPFileProcessor(PreProcessor):\n",
        "    \"`PreProcessor` that opens the filenames and read the texts.\"\n",
        "    def process_one(self,item):\n",
        "        genre = os.path.split(os.path.split(item)[0])[1].lower()\n",
        "        return (np.load(item, allow_pickle=True), genre) if isinstance(item, Path) else (item, genre)\n",
        "\n",
        "class Midi2ItemProcessor(PreProcessor):\n",
        "    \"Skips midi preprocessing step. And encodes midi files to MusicItems\"\n",
        "    def process_one(self,item):\n",
        "        # print('Midi2ItemProcess process_one')\n",
        "        item = MusicItem.from_file(item, vocab=self.vocab)\n",
        "        print('item.to_idx(): ', item.to_idx())\n",
        "        return item.to_idx()\n",
        "    \n",
        "    def process(self, ds):\n",
        "        self.vocab = ds.vocab\n",
        "        super().process(ds)\n",
        "\n",
        "## For npenc dataset\n",
        "class MusicPreloader(Callback):\n",
        "    \"Transforms the tokens in `dataset` to a stream of contiguous batches for language modelling.\"\n",
        "    \n",
        "    class CircularIndex():\n",
        "        \"Handles shuffle, direction of indexing, wraps around to head tail in the ragged array as needed\"\n",
        "        def __init__(self, length:int, forward:bool): self.idx, self.forward = np.arange(length), forward\n",
        "        def __getitem__(self, i): \n",
        "            # print('MusicPreloader __getitem__ ')\n",
        "            \n",
        "            return self.idx[ i%len(self.idx) if self.forward else len(self.idx)-1-i%len(self.idx)]\n",
        "        def __len__(self) -> int: return len(self.idx)\n",
        "        def shuffle(self): np.random.shuffle(self.idx)\n",
        "\n",
        "    def __init__(self, dataset:LabelList, lengths:Collection[int]=None, bs:int=32, bptt:int=70, backwards:bool=False, \n",
        "                 shuffle:bool=False, y_offset:int=1, \n",
        "                 transpose_range=None, transpose_p=0.5,\n",
        "                 encode_position=True,\n",
        "                 **kwargs):\n",
        "        self.dataset,self.bs,self.bptt,self.shuffle,self.backwards,self.lengths = dataset,bs,bptt,shuffle,backwards,lengths\n",
        "        self.vocab = self.dataset.vocab\n",
        "        self.bs *= num_distrib() or 1\n",
        "        self.totalToks,self.ite_len,self.idx = int(0),None,None\n",
        "        self.y_offset = y_offset\n",
        "        \n",
        "        self.transpose_range,self.transpose_p = transpose_range,transpose_p\n",
        "        self.encode_position = encode_position\n",
        "        self.bptt_len = self.bptt\n",
        "        \n",
        "        self.allocate_buffers() # needed for valid_dl on distributed training - otherwise doesn't get initialized on first epoch\n",
        "\n",
        "    def __len__(self): \n",
        "        if self.ite_len is None:\n",
        "            if self.lengths is None: self.lengths = np.array([len(item) for item in self.dataset.x])\n",
        "            self.totalToks = self.lengths.sum()\n",
        "            self.ite_len   = self.bs*int( math.ceil( self.totalToks/(self.bptt*self.bs) )) if self.item is None else 1\n",
        "        return self.ite_len\n",
        "\n",
        "    def __getattr__(self,k:str)->Any: return getattr(self.dataset, k)\n",
        "   \n",
        "    def allocate_buffers(self):\n",
        "        \"Create the ragged array that will be filled when we ask for items.\"\n",
        "        if self.ite_len is None: len(self)\n",
        "        self.idx   = MusicPreloader.CircularIndex(len(self.dataset.x), not self.backwards)\n",
        "        \n",
        "        # batch shape = (bs, bptt, 2 - [index, pos]) if encode_position. Else - (bs, bptt)\n",
        "        buffer_len = (2,) if self.encode_position else ()\n",
        "        self.batch = np.zeros((self.bs, self.bptt+self.y_offset) + buffer_len, dtype=np.int64)\n",
        "        self.batch_x, self.batch_y = self.batch[:,0:self.bptt], self.batch[:,self.y_offset:self.bptt+self.y_offset] \n",
        "        #ro: index of the text we're at inside our datasets for the various batches\n",
        "        self.ro    = np.zeros(self.bs, dtype=np.int64)\n",
        "        #ri: index of the token we're at inside our current text for the various batches\n",
        "        self.ri    = np.zeros(self.bs, dtype=np.int)\n",
        "        \n",
        "        # allocate random transpose values. Need to allocate this before hand.\n",
        "        self.transpose_values = self.get_random_transpose_values()\n",
        "        \n",
        "    def get_random_transpose_values(self):\n",
        "        if self.transpose_range is None: return None\n",
        "        n = len(self.dataset)\n",
        "        rt_arr = torch.randint(*self.transpose_range, (n,))-self.transpose_range[1]//2\n",
        "        mask = torch.rand(rt_arr.shape) > self.transpose_p\n",
        "        rt_arr[mask] = 0\n",
        "        return rt_arr\n",
        "\n",
        "    def on_epoch_begin(self, **kwargs):\n",
        "        if self.idx is None: self.allocate_buffers()\n",
        "        elif self.shuffle:   \n",
        "            self.ite_len = None\n",
        "            self.idx.shuffle()\n",
        "            self.transpose_values = self.get_random_transpose_values()\n",
        "            self.bptt_len = self.bptt\n",
        "        self.idx.forward = not self.backwards \n",
        "\n",
        "        step = self.totalToks / self.bs\n",
        "        ln_rag, countTokens, i_rag = 0, 0, -1\n",
        "        for i in range(0,self.bs):\n",
        "            #Compute the initial values for ro and ri \n",
        "            while ln_rag + countTokens <= int(step * i):\n",
        "                countTokens += ln_rag\n",
        "                i_rag       += 1\n",
        "                ln_rag       = self.lengths[self.idx[i_rag]]\n",
        "            self.ro[i] = i_rag\n",
        "            self.ri[i] = ( ln_rag - int(step * i - countTokens) ) if self.backwards else int(step * i - countTokens)\n",
        "        \n",
        "    #Training dl gets on_epoch_begin called, val_dl, on_epoch_end\n",
        "    def on_epoch_end(self, **kwargs): self.on_epoch_begin()\n",
        "\n",
        "    def __getitem__(self, k:int):\n",
        "        j = k % self.bs\n",
        "        if j==0:\n",
        "            if self.item is not None: return self.dataset[0]\n",
        "            if self.idx is None: self.on_epoch_begin()\n",
        "                \n",
        "        self.ro[j],self.ri[j] = self.fill_row(not self.backwards, self.dataset.x, self.idx, self.batch[j][:self.bptt_len+self.y_offset], \n",
        "                                              self.ro[j], self.ri[j], overlap=1, lengths=self.lengths)\n",
        "        return self.batch_x[j][:self.bptt_len], self.batch_y[j][:self.bptt_len]\n",
        "\n",
        "    def fill_row(self, forward, items, idx, row, ro, ri, overlap, lengths):\n",
        "        \"Fill the row with tokens from the ragged array. --OBS-- overlap != 1 has not been implemented\"\n",
        "        ibuf = n = 0 \n",
        "        ro  -= 1\n",
        "        while ibuf < row.shape[0]:  \n",
        "            ro   += 1 \n",
        "            ix    = idx[ro]\n",
        "            \n",
        "            item = items[ix]\n",
        "            if self.transpose_values is not None: \n",
        "                item = item.transpose(self.transpose_values[ix].item())\n",
        "                \n",
        "            if self.encode_position:\n",
        "                # Positions are colomn stacked with indexes. This makes it easier to keep in sync\n",
        "                rag = np.stack([item.data, item.position], axis=1)\n",
        "            else:\n",
        "                rag = item.data\n",
        "                \n",
        "            if forward:\n",
        "                ri = 0 if ibuf else ri\n",
        "                n  = min(lengths[ix] - ri, row.shape[0] - ibuf)\n",
        "                row[ibuf:ibuf+n] = rag[ri:ri+n]\n",
        "            else:    \n",
        "                ri = lengths[ix] if ibuf else ri\n",
        "                n  = min(ri, row.size - ibuf) \n",
        "                row[ibuf:ibuf+n] = rag[ri-n:ri][::-1]\n",
        "            ibuf += n\n",
        "        return ro, ri + ((n-overlap) if forward else -(n-overlap))\n",
        "\n",
        "\n",
        "\n",
        "def batch_position_tfm(b):\n",
        "    \"Batch transform for training with positional encoding\"\n",
        "    x,y = b\n",
        "    x = {\n",
        "        'x': x[...,0],\n",
        "        'pos': x[...,1]\n",
        "    }\n",
        "    return x, y[...,0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24gPTNLV-tud"
      },
      "source": [
        "###**transform.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "cellView": "code",
        "id": "Iq5TLUrF-enG"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#https://github.com/bearpelican/musicautobot/blob/master/musicautobot/music_transformer/transform.py\n",
        "\n",
        "#from ..numpy_encode import *\n",
        "import numpy as np\n",
        "from enum import Enum\n",
        "import torch\n",
        "#from ..vocab import *\n",
        "from functools import partial\n",
        "\n",
        "SEQType = Enum('SEQType', 'Mask, Sentence, Melody, Chords, Empty, Genre')\n",
        "\n",
        "class MusicItem():\n",
        "    def __init__(self, data, vocab, ins = None, verbose=False, stream=None, position=None):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "        #DONE\n",
        "        self.ins = ins\n",
        "        self._stream = stream\n",
        "        self._position = position\n",
        "    def __repr__(self): return '\\n'.join([\n",
        "        f'\\n{self.__class__.__name__} - {self.data.shape}',\n",
        "        f'npenc: {self.data[:10]}',\n",
        "        f'{self.vocab.textify(self.data[:10])}...'])\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    @classmethod\n",
        "    def from_file(cls, midi_file, vocab): \n",
        "        return cls.from_stream(file2stream(midi_file), vocab)\n",
        "    @classmethod\n",
        "    def from_stream(cls, stream, vocab):\n",
        "        if not isinstance(stream, music21.stream.Score): stream = stream.voicesToParts()\n",
        "        chordarr, ins = stream2chordarr(stream) # 2.\n",
        "        cls.ins = ins\n",
        "        npenc = chordarr2npenc(chordarr) # 3.\n",
        "        \n",
        "        # print(npenc)\n",
        "\n",
        "        #DONE\n",
        "        # print('ins: ', ins)\n",
        "        # print('MusicItem.from_stream: ')\n",
        "        # print(chordarr.shape)\n",
        "        # print(chordarr.sum())\n",
        "        # print(npenc.shape)\n",
        "        \n",
        "        # print('npenc orig: ',npenc[:,0], npenc[:,1], npenc[:,2])\n",
        "        #TODO: use `self.ins` to do correct vocab.stoi for instruments during encoding\n",
        "        return cls.from_npenc(npenc, vocab, stream, ins)\n",
        "    @classmethod\n",
        "    def from_npenc(cls, npenc, vocab, stream=None, ins = None, genre = None): \n",
        "      # Added code for sorting the instruments in a predefined order between separation indices \n",
        "      npenc = sort_instruments(npenc, vocab)\n",
        "      if(genre != None):\n",
        "        return MusicItem(npenc2idxenc(npenc, vocab, ins = ins, genre = genre, seq_type=SEQType.Genre), vocab, ins = ins, stream = stream)\n",
        "      else:\n",
        "        return MusicItem(npenc2idxenc(npenc, vocab, ins = ins, genre = genre), vocab, ins = ins, stream = stream)\n",
        "\n",
        "    @classmethod\n",
        "    def from_idx(cls, item, vocab):\n",
        "        idx,pos = item\n",
        "        return MusicItem(idx, vocab=vocab, position=pos)\n",
        "    def to_idx(self): return self.data, self.position\n",
        "\n",
        "    @classmethod\n",
        "    def empty(cls, vocab, seq_type=SEQType.Sentence):\n",
        "        return MusicItem(seq_prefix(seq_type, vocab), vocab)\n",
        "\n",
        "    # Added utiliy function to sort the instruments between each separation indices \n",
        "    # def sort_idx(self):\n",
        "    #   self.data = sort_instruments(idxenc2npenc(self.data,self.vocab), self.vocab)\n",
        "    #   return self.data\n",
        "\n",
        "    @property\n",
        "    def stream(self):\n",
        "        self._stream = self.to_stream() if self._stream is None else self._stream\n",
        "        return self._stream\n",
        "    \n",
        "    def to_stream(self, bpm=120):\n",
        "        #TODO: This 'idxenc2stream' is called when MusicItem.show() is called, alter it accordingly to use self.ins to convert idx back to npenc/stream\n",
        "        return idxenc2stream(self.data, self.vocab, bpm=bpm)\n",
        "\n",
        "    def to_tensor(self, device=None):\n",
        "        return to_tensor(self.data, device)\n",
        "    \n",
        "    def to_text(self, sep=' '): return self.vocab.textify(self.data, sep)\n",
        "    \n",
        "    @property\n",
        "    def position(self): \n",
        "        self._position = position_enc(self.data, self.vocab) if self._position is None else self._position\n",
        "        return self._position\n",
        "    \n",
        "    def get_pos_tensor(self, device=None): return to_tensor(self.position, device)\n",
        "\n",
        "    def to_npenc(self):\n",
        "        return idxenc2npenc(self.data, self.vocab)\n",
        "\n",
        "    def show(self, format:str=None):\n",
        "        return self.stream.show(format)\n",
        "    def play(self): self.stream.show('midi')\n",
        "        \n",
        "    @property\n",
        "    def new(self):\n",
        "        return partial(type(self), vocab=self.vocab)\n",
        "\n",
        "    def trim_to_beat(self, beat, include_last_sep=False):\n",
        "        return self.new(trim_to_beat(self.data, self.position, self.vocab, beat, include_last_sep))\n",
        "    \n",
        "    def transpose(self, interval):\n",
        "        return self.new(tfm_transpose(self.data, interval, self.vocab), position=self._position)\n",
        "    \n",
        "    def append(self, item):\n",
        "        return self.new(np.concatenate((self.data, item.data), axis=0))\n",
        "    \n",
        "    def mask_pitch(self, section=None):\n",
        "        return self.new(self.mask(self.vocab.note_range, section), position=self.position)\n",
        "    \n",
        "    def mask_duration(self, section=None, keep_position_enc=True):\n",
        "        masked_data = self.mask(self.vocab.dur_range, section)\n",
        "        if keep_position_enc: return self.new(masked_data, position=self.position)\n",
        "        return self.new(masked_data)\n",
        "\n",
        "    def mask(self, token_range, section_range=None):\n",
        "        return mask_section(self.data, self.position, token_range, self.vocab.mask_idx, section_range=section_range)\n",
        "    \n",
        "    def pad_to(self, bptt):\n",
        "        data = pad_seq(self.data, bptt, self.vocab.pad_idx)\n",
        "        pos = pad_seq(self.position, bptt, 0)\n",
        "        return self.new(data, stream=self._stream, position=pos)\n",
        "    \n",
        "    def split_stream_parts(self):\n",
        "        self._stream = separate_melody_chord(self.stream)\n",
        "        return self.stream\n",
        "\n",
        "    def remove_eos(self):\n",
        "        if self.data[-1] == self.vocab.stoi[EOS]: return self.new(self.data, stream=self.stream)\n",
        "        return self\n",
        "\n",
        "    def split_parts(self):\n",
        "        return self.new(self.data, stream=separate_melody_chord(self.stream), position=self.position)\n",
        "        \n",
        "def pad_seq(seq, bptt, value):\n",
        "    pad_len = max(bptt-seq.shape[0], 0)\n",
        "    return np.pad(seq, (0, pad_len), 'constant', constant_values=value)[:bptt]\n",
        "\n",
        "def to_tensor(t, device=None):\n",
        "    t = t if isinstance(t, torch.Tensor) else torch.tensor(t)\n",
        "    if device is None and torch.cuda.is_available(): t = t.cuda()\n",
        "    else: t.to(device)\n",
        "    return t.long()\n",
        "    \n",
        "def midi2idxenc(midi_file, vocab):\n",
        "    \"Converts midi file to index encoding for training\"\n",
        "    npenc = midi2npenc(midi_file) # 3.\n",
        "    return npenc2idxenc(npenc, vocab)\n",
        "\n",
        "def idxenc2stream(arr, vocab, bpm=120):\n",
        "    \"Converts index encoding to music21 stream\"\n",
        "    npenc = idxenc2npenc(arr, vocab)\n",
        "    return npenc2stream(npenc, bpm=bpm)\n",
        "\n",
        "#DONE\n",
        "def npins2vocabins(x, ins:dict):\n",
        "  if x in ins.keys():\n",
        "    \n",
        "    if(ins[x] in ACCEP_INS.keys()):\n",
        "      return ACCEP_INS[ins[x]]\n",
        "    else:\n",
        "      return ACCEP_INS['Piano']\n",
        "\n",
        "  elif x == (-2 - len(NOTE_TOKS) - len(DUR_TOKS)):\n",
        "    return x\n",
        "  else:\n",
        "    raise Exception \n",
        "\n",
        "# single stream instead of note,dur\n",
        "def npenc2idxenc(t, vocab, ins = None, genre = None, seq_type=SEQType.Sentence, add_eos=True):\n",
        "    \"Transforms numpy array from 2 column (note, duration) matrix to a single column\"\n",
        "    \"[[n1, d1], [n2, d2], ...] -> [n1, d1, n2, d2]\"\n",
        "    if isinstance(t, (list, tuple)) and len(t) == 2: \n",
        "        return [npenc2idxenc(x, vocab, start_seq) for x in t]\n",
        "    t = t.copy()\n",
        "    \n",
        "    #print('|npenc2idxenc|', t.shape)\n",
        "    #print('chordarr ndim: ', t.ndim)\n",
        "    try:\n",
        "      #DONE\n",
        "      if t.shape[1] == 2:\n",
        "        t[:, 0] = t[:, 0] + vocab.note_range[0]\n",
        "        t[:, 1] = t[:, 1] + vocab.dur_range[0]\n",
        "        \n",
        "        prefix = seq_prefix(seq_type, vocab)\n",
        "        suffix = np.array([vocab.stoi[EOS]]) if add_eos else np.empty(0, dtype=int)   \n",
        "      elif t.shape[1] == 3:\n",
        "        t[:, 0] = t[:, 0] + vocab.note_range[0]\n",
        "        t[:, 1] = t[:, 1] + vocab.dur_range[0]    # check wheter duration token is less than max duration \n",
        "        #SEE: `chordarr2npenc`\n",
        "        #DONE\n",
        "        # l1 = (t[:,2] == 5).nonzero()[0]\n",
        "        # print('npenc ins: ', t[:,2][:10])\n",
        "\n",
        "        if ins is not None:\n",
        "          f = lambda x, y: npins2vocabins(x,y)\n",
        "          t[:,2] = np.vectorize(f)(t[:,2], ins)\n",
        "          # print('npenc ins mapped: ', t[:,2][:10])\n",
        "        \n",
        "        # l2 = (t[:,2] == 0).nonzero()[0]\n",
        "        # print(len(l1))\n",
        "        # print(len([i for i in l1 if i in l2]))\n",
        "\n",
        "        t[:, 2] = t[:, 2] + vocab.ins_range[0]\n",
        "        # print('npenc ins mapped: ',t[:,0], t[:,1], t[:,2])\n",
        "        \n",
        "\n",
        "        prefix = seq_prefix(seq_type, vocab, genre)\n",
        "        suffix = np.array([vocab.stoi[EOS]]) if add_eos else np.empty(0, dtype=int)\n",
        "        # print('npenc2idxenc result: ',np.concatenate([t.reshape(-1), suffix]))\n",
        "    except IndexError as e:\n",
        "        print('IndexError, t.shape:', t.shape )\n",
        "        raise e\n",
        "    return np.concatenate([prefix, t.reshape(-1), suffix])\n",
        "\n",
        "def seq_prefix(seq_type, vocab, genre = None):\n",
        "    if seq_type == SEQType.Empty: return np.empty(0, dtype=int)\n",
        "    start_token = vocab.bos_idx\n",
        "    if seq_type == SEQType.Chords: start_token = vocab.stoi[CSEQ]\n",
        "    if seq_type == SEQType.Melody: start_token = vocab.stoi[MSEQ]\n",
        "    if seq_type == SEQType.Genre and genre != None:\n",
        "      token = BOS\n",
        "      genre = genre.lower() \n",
        "      if('electronic' in genre): token = ELECTRONIC  \n",
        "      elif('folk' in genre): token = FOLK\n",
        "      elif('funk' in genre): token = FUNK\n",
        "      elif('jazz' in genre): token = JAZZ\n",
        "      elif('pop' in genre): token = POP\n",
        "      elif('rock' in genre): token = ROCK\n",
        "      start_token = vocab.stoi[token]\n",
        "    return np.array([start_token, vocab.pad_idx])\n",
        "\n",
        "#IMP\n",
        "#TODO: Use MusicItem/self.ins to verify proper reconversion into npenc of shape [X, 3]\n",
        "def idxenc2npenc(t, vocab, validate=True):\n",
        "    if validate: \n",
        "      t = to_valid_idxenc(t, vocab.npenc_range)\n",
        "      \n",
        "    #DONE\n",
        "    # t = t.copy().reshape(-1, 2)\n",
        "    # if t.shape[-1]%3 != 0 : \n",
        "    #   t = t[:(t.shape[-1] - t.shape[-1]%3)]\n",
        "\n",
        "    # temp = vocab.textify(t, ' ').split(' ')\n",
        "    # print(temp[:50])\n",
        "    # print([x for index,x in enumerate(temp) if index%3 == 0])\n",
        "\n",
        "    # print('idxenc2npenc : ')\n",
        "    # print('t = ',  t)\n",
        "    ins_toks = [True if vocab.is_ins(x) else False for x in t]\n",
        "    # print(ins_toks)\n",
        "    last_idx_tok_idx_rev = ins_toks[::-1].index(True)\n",
        "    # print(last_idx_tok_idx_rev)\n",
        "    # print('Last elements before proc: ', t[-5:])\n",
        "    t = t[:(len(ins_toks) - last_idx_tok_idx_rev)]\n",
        "    # print('Last elements after proc: ', t[-5:])\n",
        "\n",
        "\n",
        "    t = t.copy().reshape(-1, 3)\n",
        "\n",
        "    # print('idxenc2npenc input: ', t[:,0], t[:,1], t[:,2]) \n",
        "\n",
        "    \n",
        "\n",
        "    if t.shape[0] == 0: return t\n",
        "        \n",
        "    t[:, 0] = t[:, 0] - vocab.note_range[0]\n",
        "    t[:, 1] = t[:, 1] - vocab.dur_range[0]\n",
        "    t[:, 2] = t[:, 2] - vocab.ins_range[0]\n",
        "\n",
        "    # print('idxenc2npenc: ', t[:,0], t[:,1], t[:,2] )\n",
        "\n",
        "    #TODO: generalise to `ndi`, currently for `nd`\n",
        "    if validate:\n",
        "      t = to_valid_npenc(t)\n",
        "    return t\n",
        "\n",
        "def to_valid_idxenc(t, valid_range):\n",
        "    r = valid_range\n",
        "    t = t[np.where((t >= r[0]) & (t < r[1]))]\n",
        "    #DONE\n",
        "    #Removed this line that removes the odd dimension, which is required by our nxdxi array of shape [X, 3]\n",
        "    #if t.shape[-1] % 2 == 1: t = t[..., :-1]\n",
        "    return t\n",
        "\n",
        "def to_valid_npenc(t):\n",
        "    is_note = (t[:, 0] < VALTSEP) | (t[:, 0] >= NOTE_SIZE)\n",
        "    invalid_note_idx = is_note.argmax()\n",
        "    invalid_dur_idx = (t[:, 1] < 0).argmax()\n",
        "\n",
        "    invalid_idx = max(invalid_dur_idx, invalid_note_idx)\n",
        "    if invalid_idx > 0: \n",
        "        if invalid_note_idx > 0 and invalid_dur_idx > 0: invalid_idx = min(invalid_dur_idx, invalid_note_idx)\n",
        "        print('Non midi note detected. Only returning valid portion. Index, seed', invalid_idx, t.shape)\n",
        "        return t[:invalid_idx]\n",
        "    return t\n",
        "\n",
        "def sort_instruments(npenc, vocab):\n",
        "    \"Sorts instrument according to accept instrument list\"\n",
        "    sep_idxs = (npenc[:,0] == -1).nonzero()[0]\n",
        "    \n",
        "    updated_npenc = []\n",
        "\n",
        "    first_sep = sep_idxs[0]\n",
        "    \n",
        "    if(first_sep != 0):\n",
        "      npenc_sub = npenc[0 : first_sep]\n",
        "      npenc_sub = sorted(npenc_sub, key = lambda x : x[2])\n",
        "      final_subset = npenc_sub\n",
        "      updated_npenc.extend(final_subset)\n",
        "\n",
        "    for e in zip(sep_idxs[:-1],sep_idxs[1:]):\n",
        "      npenc_sub = npenc[e[0] + 1 : e[1]]\n",
        "      npenc_sub = sorted(npenc_sub, key = lambda x : x[2])\n",
        "      # npenc_sub = [list(i) for i in npenc_sub]\n",
        "      sep = npenc[e[0]]\n",
        "      final_subset = [sep] + npenc_sub\n",
        "\n",
        "      # print(final_subset)\n",
        "      updated_npenc.extend(final_subset)\n",
        "    \n",
        "    last_sep = sep_idxs[-1]\n",
        "    \n",
        "    if(len(npenc) > last_sep + 1):\n",
        "      npenc_sub = npenc[last_sep + 1 : ]\n",
        "      npenc_sub = sorted(npenc_sub, key = lambda x : x[2])\n",
        "      sep = npenc[e[0]]\n",
        "      final_subset = [sep] + npenc_sub\n",
        "    else:\n",
        "      final_subset = [sep]\n",
        "\n",
        "    updated_npenc.extend(final_subset)\n",
        "\n",
        "    updated_npenc = np.array(updated_npenc)\n",
        "    sep_idxs_updated = (updated_npenc[:,0] == -1).nonzero()[0]\n",
        "\n",
        "    # print(sep_idxs)\n",
        "    # print(sep_idxs_updated)\n",
        "\n",
        "    assert list(sep_idxs) == list(sep_idxs_updated)\n",
        "    # print(updated_npenc)\n",
        "    return updated_npenc\n",
        "\n",
        "def position_enc(idxenc, vocab):\n",
        "    \"Calculates positional beat encoding.\"\n",
        "    \n",
        "    # print('position encoding : \\n')\n",
        "    # print(idxenc)\n",
        "\n",
        "    # gets the separation tokens indices \n",
        "    sep_idxs = (idxenc == vocab.sep_idx).nonzero()[0]\n",
        "\n",
        "    \n",
        "    sep_idxs = sep_idxs[sep_idxs+2 < idxenc.shape[0]] # remove any indexes right before out of bounds (sep_idx+2)\n",
        "    dur_vals = idxenc[sep_idxs+1]\n",
        "\n",
        "    dur_vals[dur_vals == vocab.mask_idx] = vocab.dur_range[0] # make sure masked durations are 0\n",
        "    dur_vals -= vocab.dur_range[0]\n",
        "    \n",
        "    posenc = np.zeros_like(idxenc)\n",
        "\n",
        "    # DONE : changed to account for xxni token  \n",
        "    # posenc[sep_idxs+2] = dur_vals\n",
        "\n",
        "    \n",
        "    # print(sep_idxs)\n",
        "    # print(sep_idxs[:-1])\n",
        "\n",
        "    # last_sep_idx = sep_idxs[-1]\n",
        "    # last_dur_val = dur_vals[-1]\n",
        "    try:\n",
        "      if(len(idxenc) > sep_idxs[-1] + 3):\n",
        "        posenc[sep_idxs+3] = dur_vals\n",
        "      else:\n",
        "        sep_idxs = sep_idxs[:-1]\n",
        "        dur_vals = dur_vals[:-1]\n",
        "        posenc[sep_idxs+3] = dur_vals\n",
        "    except:\n",
        "      print('idx_enc = ', idxenc)\n",
        "      print('sep_idxs = ', sep_idxs )\n",
        "\n",
        "    return posenc.cumsum()\n",
        "\n",
        "def beat2index(idxenc, pos, vocab, beat, include_last_sep=False):\n",
        "    cutoff = find_beat(pos, beat)\n",
        "    if cutoff < 2: return 2 # always leave starter tokens\n",
        "    if len(idxenc) < 2 or include_last_sep: return cutoff\n",
        "    if idxenc[cutoff - 2] == vocab.sep_idx: return cutoff - 2\n",
        "    return cutoff\n",
        "\n",
        "def find_beat(pos, beat, sample_freq=SAMPLE_FREQ, side='left'):\n",
        "    return np.searchsorted(pos, beat * sample_freq, side=side)\n",
        "\n",
        "# TRANSFORMS\n",
        "\n",
        "def tfm_transpose(x, value, vocab):\n",
        "    x = x.copy()\n",
        "    x[(x >= vocab.note_range[0]) & (x < vocab.note_range[1])] += value\n",
        "    return x\n",
        "\n",
        "def trim_to_beat(idxenc, pos, vocab, to_beat=None, include_last_sep=True):\n",
        "    if to_beat is None: return idxenc\n",
        "    cutoff = beat2index(idxenc, pos, vocab, to_beat, include_last_sep=include_last_sep)\n",
        "    return idxenc[:cutoff]\n",
        "\n",
        "def mask_input(xb, mask_range, replacement_idx):\n",
        "    xb = xb.copy()\n",
        "    xb[(xb >= mask_range[0]) & (xb < mask_range[1])] = replacement_idx\n",
        "    return xb\n",
        "\n",
        "def mask_section(xb, pos, token_range, replacement_idx, section_range=None):\n",
        "    xb = xb.copy()\n",
        "    token_mask = (xb >= token_range[0]) & (xb < token_range[1])\n",
        "\n",
        "    if section_range is None: section_range = (None, None)\n",
        "    section_mask = np.zeros_like(xb, dtype=bool)\n",
        "    start_idx = find_beat(pos, section_range[0]) if section_range[0] is not None else 0\n",
        "    end_idx = find_beat(pos, section_range[1]) if section_range[1] is not None else xb.shape[0]\n",
        "    section_mask[start_idx:end_idx] = True\n",
        "    \n",
        "    xb[token_mask & section_mask] = replacement_idx\n",
        "    return xb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_ItlNFi_uQj"
      },
      "source": [
        "###**model.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "cellView": "code",
        "id": "jRl_Dm0pAYzf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def window_mask(x_len, device, m_len=0, size=(1,1)):\n",
        "    win_size,k = size\n",
        "    mem_mask = torch.zeros((x_len,m_len), device=device)\n",
        "    tri_mask = torch.triu(torch.ones((x_len//win_size+1,x_len//win_size+1), device=device),diagonal=k)\n",
        "    window_mask = tri_mask.repeat_interleave(win_size,dim=0).repeat_interleave(win_size,dim=1)[:x_len,:x_len]\n",
        "    if x_len: window_mask[...,0] = 0 # Always allowing first index to see. Otherwise you'll get NaN loss\n",
        "    mask = torch.cat((mem_mask, window_mask), dim=1)[None,None]\n",
        "    return mask.bool() if hasattr(mask, 'bool') else mask.byte()\n",
        "    \n",
        "def rand_window_mask(x_len,m_len,device,max_size:int=None,p:float=0.2,is_eval:bool=False):\n",
        "    if is_eval or np.random.rand() >= p or max_size is None: \n",
        "        win_size,k = (1,1)\n",
        "    else: win_size,k = (np.random.randint(0,max_size)+1,0)\n",
        "    return window_mask(x_len, device, m_len, size=(win_size,k))\n",
        "\n",
        "def lm_mask(x_len, device):\n",
        "    mask = torch.triu(torch.ones((x_len, x_len), device=device), diagonal=1)[None,None]\n",
        "    return mask.bool() if hasattr(mask, 'bool') else mask.byte()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "cellView": "code",
        "id": "3ozBaYA5_tni"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#https://github.com/bearpelican/musicautobot/blob/master/musicautobot/music_transformer/model.py\n",
        "\n",
        "from fastai.basics import *\n",
        "from fastai.text.models.transformer import TransformerXL\n",
        "#from ..utils.attention_mask import rand_window_mask\n",
        "\n",
        "class MusicTransformerXL(TransformerXL):\n",
        "    \"Exactly like fastai's TransformerXL, but with more aggressive attention mask: see `rand_window_mask`\"\n",
        "    def __init__(self, *args, encode_position=True, mask_steps=1, **kwargs):\n",
        "        import inspect\n",
        "        sig = inspect.signature(TransformerXL)\n",
        "        arg_params = { k:kwargs[k] for k in sig.parameters if k in kwargs }\n",
        "        super().__init__(*args, **arg_params)\n",
        "\n",
        "        self.encode_position = encode_position\n",
        "        if self.encode_position: self.beat_enc = BeatPositionEncoder(kwargs['d_model'])\n",
        "            \n",
        "        self.mask_steps=mask_steps\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        #The hidden state has to be initiliazed in the forward pass for nn.DataParallel\n",
        "        if self.mem_len > 0 and not self.init: \n",
        "            self.reset()\n",
        "            self.init = True\n",
        "\n",
        "        benc = 0\n",
        "        if self.encode_position:\n",
        "            # print(x)\n",
        "            x,pos = x['x'], x['pos']\n",
        "            benc = self.beat_enc(pos)\n",
        "\n",
        "        bs,x_len = x.size()\n",
        "        inp = self.drop_emb(self.encoder(x) + benc) #.mul_(self.d_model ** 0.5)\n",
        "        m_len = self.hidden[0].size(1) if hasattr(self, 'hidden') and len(self.hidden[0].size()) > 1 else 0\n",
        "        seq_len = m_len + x_len\n",
        "        \n",
        "        mask = rand_window_mask(x_len, m_len, inp.device, max_size=self.mask_steps, is_eval=not self.training) if self.mask else None\n",
        "        if m_len == 0: mask[...,0,0] = 0\n",
        "        #[None,:,:None] for einsum implementation of attention\n",
        "        hids = []\n",
        "        pos = torch.arange(seq_len-1, -1, -1, device=inp.device, dtype=inp.dtype)\n",
        "        pos_enc = self.pos_enc(pos)\n",
        "        hids.append(inp)\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            mem = self.hidden[i] if self.mem_len > 0 else None\n",
        "            inp = layer(inp, r=pos_enc, u=self.u, v=self.v, mask=mask, mem=mem)\n",
        "            hids.append(inp)\n",
        "        core_out = inp[:,-x_len:]\n",
        "        if self.mem_len > 0 : self._update_mems(hids)\n",
        "        return (self.hidden if self.mem_len > 0 else [core_out]),[core_out]\n",
        "\n",
        "\n",
        " # Beat encoder\n",
        "class BeatPositionEncoder(nn.Module):\n",
        "    \"Embedding + positional encoding + dropout\"\n",
        "    def __init__(self, emb_sz:int, beat_len=32, max_bar_len=1024):\n",
        "        super().__init__()\n",
        "\n",
        "        self.beat_len, self.max_bar_len = beat_len, max_bar_len\n",
        "        self.beat_enc = nn.Embedding(beat_len, emb_sz, padding_idx=0)\n",
        "        self.bar_enc = nn.Embedding(max_bar_len, emb_sz, padding_idx=0)\n",
        "    \n",
        "    def forward(self, pos):\n",
        "        beat_enc = self.beat_enc(pos % self.beat_len)\n",
        "        bar_pos = pos // self.beat_len % self.max_bar_len\n",
        "        bar_pos[bar_pos >= self.max_bar_len] = self.max_bar_len - 1\n",
        "        bar_enc = self.bar_enc((bar_pos))\n",
        "        return beat_enc + bar_enc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMhha4KADa2M"
      },
      "source": [
        "###**utils**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "cellView": "code",
        "id": "83Wgl5AGDapV"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#https://github.com/bearpelican/musicautobot/blob/master/musicautobot/utils/top_k_top_p.py\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "__all__ = ['top_k_top_p']\n",
        "\n",
        "# top_k + nucleus filter - https://twitter.com/thom_wolf/status/1124263861727760384?lang=en\n",
        "# https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "def top_k_top_p(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "    \"\"\"\n",
        "    logits = logits.clone()\n",
        "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "cellView": "code",
        "id": "_HeX8lEdDamu"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#https://github.com/bearpelican/musicautobot/blob/master/musicautobot/utils/midifile.py\n",
        "\n",
        "def is_empty_midi(fp):\n",
        "    if fp is None: return False\n",
        "    mf = file2mf(fp)\n",
        "    return not any([t.hasNotes() for t in mf.tracks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "J1i5OwA-OkSS"
      },
      "outputs": [],
      "source": [
        "\"Parallel processing for midi files\"\n",
        "import csv\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "from pathlib import Path\n",
        "from pebble import ProcessPool\n",
        "from concurrent.futures import TimeoutError\n",
        "import numpy as np\n",
        "\n",
        "# https://stackoverflow.com/questions/20991968/asynchronous-multiprocessing-with-a-worker-pool-in-python-how-to-keep-going-aft\n",
        "def process_all(func, arr, timeout_func=None, total=None, max_workers=None, timeout=None):\n",
        "    with ProcessPool() as pool:\n",
        "        future = pool.map(func, arr, timeout=timeout)\n",
        "\n",
        "        iterator = future.result()\n",
        "        results = []\n",
        "        for i in progress_bar(range(len(arr)), total=len(arr)):\n",
        "            try:\n",
        "                result = next(iterator)\n",
        "                if result: results.append(result)\n",
        "            except StopIteration:\n",
        "                break  \n",
        "            except TimeoutError as error:\n",
        "                if timeout_func: timeout_func(arr[i], error.args[1])\n",
        "    return results\n",
        "\n",
        "def process_file(file_path, tfm_func=None, src_path=None, dest_path=None):\n",
        "    \"Utility function that transforms midi file to numpy array.\"\n",
        "    output_file = Path(str(file_path).replace(str(src_path), str(dest_path))).with_suffix('.npy')\n",
        "    if output_file.exists(): return output_file\n",
        "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Call tfm_func and save file\n",
        "    npenc = tfm_func(file_path)\n",
        "    if npenc is not None: \n",
        "        np.save(output_file, npenc)\n",
        "        return output_file\n",
        "\n",
        "def arr2csv(arr, out_file):\n",
        "    \"Convert metadata array to csv\"\n",
        "    all_keys = {k for d in arr for k in d.keys()}\n",
        "    arr = [format_values(x) for x in arr]\n",
        "    with open(out_file, 'w') as f:\n",
        "        dict_writer = csv.DictWriter(f, list(all_keys))\n",
        "        dict_writer.writeheader()\n",
        "        dict_writer.writerows(arr)\n",
        "        \n",
        "def format_values(d):\n",
        "    \"Format array values for csv encoding\"\n",
        "    def format_value(v):\n",
        "        if isinstance(v, list): return ','.join(v)\n",
        "        return v\n",
        "    return {k:format_value(v) for k,v in d.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsqI0L7zC3BU"
      },
      "source": [
        "###**learner.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "cellView": "code",
        "id": "Azk4iLnmC2y6"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#https://github.com/bearpelican/musicautobot/blob/15bc523548f8ae737a594ee92564538d02e0dc94/musicautobot/music_transformer/learner.py\n",
        "\n",
        "from fastai.basics import *\n",
        "from fastai.text.learner import LanguageLearner, get_language_model, _model_meta\n",
        "#from .model import *\n",
        "#from .transform import MusicItem\n",
        "#from ..numpy_encode import SAMPLE_FREQ\n",
        "#from ..utils.top_k_top_p import top_k_top_p\n",
        "#from ..utils.midifile import is_empty_midi\n",
        "\n",
        "_model_meta[MusicTransformerXL] = _model_meta[TransformerXL] # copy over fastai's model metadata\n",
        "\n",
        "def music_model_learner(data:DataBunch, arch=MusicTransformerXL, config:dict=None, drop_mult:float=1.,\n",
        "                        pretrained_path:PathOrStr=None, encode_position = True, **learn_kwargs) -> 'LanguageLearner':\n",
        "    \"Create a `Learner` with a language model from `data` and `arch`.\"\n",
        "    meta = _model_meta[arch]\n",
        "\n",
        "    if pretrained_path: \n",
        "        state = torch.load(pretrained_path, map_location='cpu')\n",
        "        if config is None: config = state['config']\n",
        "        \n",
        "    model = get_language_model(arch, len(data.vocab.itos), config=config, drop_mult=drop_mult)\n",
        "    #DONE\n",
        "    # if hasattr(model[0], 'encode_position'):\n",
        "      # model[0].encode_position = encode_position\n",
        "    learn = MusicLearner(data, model, split_func=meta['split_lm'], **learn_kwargs)\n",
        "\n",
        "    if pretrained_path: \n",
        "        get_model(model).load_state_dict(state['model'], strict=False)\n",
        "        if not hasattr(learn, 'opt'): learn.create_opt(defaults.lr, learn.wd)\n",
        "        try:    learn.opt.load_state_dict(state['opt'])\n",
        "        except: pass\n",
        "        del state\n",
        "        gc.collect()\n",
        "\n",
        "    return learn\n",
        "\n",
        "# Predictions\n",
        "from fastai import basic_train # for predictions\n",
        "class MusicLearner(LanguageLearner):\n",
        "    def save(self, file:PathLikeOrBinaryStream=None, with_opt:bool=True, config=None):\n",
        "        \"Save model and optimizer state (if `with_opt`) with `file` to `self.model_dir`. `file` can be file-like (file or buffer)\"\n",
        "        out_path = super().save(file, return_path=True, with_opt=with_opt)\n",
        "        if config and out_path:\n",
        "            state = torch.load(out_path)\n",
        "            state['config'] = config\n",
        "            torch.save(state, out_path)\n",
        "            del state\n",
        "            gc.collect()\n",
        "        return out_path\n",
        "\n",
        "    def beam_search(self, xb:Tensor, n_words:int, top_k:int=10, beam_sz:int=10, temperature:float=1.,\n",
        "                    ):\n",
        "        \"Return the `n_words` that come after `text` using beam search.\"\n",
        "        self.model.reset()\n",
        "        self.model.eval()\n",
        "        xb_length = xb.shape[-1]\n",
        "        if xb.shape[0] > 1: xb = xb[0][None]\n",
        "        yb = torch.ones_like(xb)\n",
        "\n",
        "        nodes = None\n",
        "        xb = xb.repeat(top_k, 1)\n",
        "        nodes = xb.clone()\n",
        "        scores = xb.new_zeros(1).float()\n",
        "        with torch.no_grad():\n",
        "            for k in progress_bar(range(n_words), leave=False):\n",
        "                out = F.log_softmax(self.model(xb)[0][:,-1], dim=-1)\n",
        "                values, indices = out.topk(top_k, dim=-1)\n",
        "                scores = (-values + scores[:,None]).view(-1)\n",
        "                indices_idx = torch.arange(0,nodes.size(0))[:,None].expand(nodes.size(0), top_k).contiguous().view(-1)\n",
        "                sort_idx = scores.argsort()[:beam_sz]\n",
        "                scores = scores[sort_idx]\n",
        "                nodes = torch.cat([nodes[:,None].expand(nodes.size(0),top_k,nodes.size(1)),\n",
        "                                indices[:,:,None].expand(nodes.size(0),top_k,1),], dim=2)\n",
        "                nodes = nodes.view(-1, nodes.size(2))[sort_idx]\n",
        "                self.model[0].select_hidden(indices_idx[sort_idx])\n",
        "                xb = nodes[:,-1][:,None]\n",
        "        if temperature != 1.: scores.div_(temperature)\n",
        "        node_idx = torch.multinomial(torch.exp(-scores), 1).item()\n",
        "        return [i.item() for i in nodes[node_idx][xb_length:] ]\n",
        "\n",
        "    def predict(self, item:MusicItem, n_words:int=128,\n",
        "                     temperatures:float=(1.0,1.0,1.0), min_bars=4,\n",
        "                     top_k=30, top_p=0.6, allowed_ins:list = None):\n",
        "        \"Return the `n_words` that come after `text`.\"\n",
        "        self.model.reset()\n",
        "        new_idx = []\n",
        "        vocab = self.data.vocab\n",
        "        x, pos = item.to_tensor(), item.get_pos_tensor()\n",
        "        last_pos = pos[-1] if len(pos) else 0\n",
        "        y = torch.tensor([0])\n",
        "\n",
        "        start_pos = last_pos\n",
        "\n",
        "        sep_count = 0\n",
        "        bar_len = SAMPLE_FREQ * 4 # assuming 4/4 time\n",
        "        vocab = self.data.vocab\n",
        "\n",
        "        repeat_count = 0\n",
        "        if hasattr(self.model[0], 'encode_position'):\n",
        "            encode_position = self.model[0].encode_position\n",
        "        else: encode_position = False\n",
        "\n",
        "        #DONE\n",
        "        last_xxsep = False\n",
        "\n",
        "        for i in progress_bar(range(n_words), leave=True):\n",
        "            with torch.no_grad():\n",
        "                if encode_position:\n",
        "                    batch = { 'x': x[None], 'pos': pos[None] }\n",
        "                    logits = self.model(batch)[0][-1][-1]\n",
        "                else:\n",
        "                    logits = self.model(x[None])[0][-1][-1]\n",
        "\n",
        "            #DONE\n",
        "            # prev_idx = new_idx[-1] if len(new_idx) else vocab.pad_idx\n",
        "            # prev_idx = new_idx[-1] if len(new_idx) else item.data[-1]\n",
        "            \n",
        "            # if nothing is predicted then use the last token of input item \n",
        "            if len(new_idx):\n",
        "              prev_idx = new_idx[-1]\n",
        "            else:\n",
        "              prev_idx = item.data[-1]\n",
        "              print('Init prev_idx = ', prev_idx)\n",
        "\n",
        "            # set the last xxsep \n",
        "            # if prev_idx is xxsep then we want our model to predict d xxni as next two tokens \n",
        "            if prev_idx == vocab.sep_idx:\n",
        "              last_xxsep = True\n",
        "            elif vocab.is_ins(prev_idx):\n",
        "              if prev_idx == vocab.ni_idx:\n",
        "                last_xxsep = False\n",
        "\n",
        "            # Temperature\n",
        "            # Use first temperatures value if last prediction was duration\n",
        "            # temperature = temperatures[0] if vocab.is_duration_or_pad(prev_idx) else temperatures[1]\n",
        "            # temperature[0] -> note\n",
        "            # temperature[1] -> duration\n",
        "            # temperature[2] -> instrument\n",
        "\n",
        "            temperature = None\n",
        "\n",
        "            if vocab.is_duration(prev_idx):\n",
        "              temperature = temperatures[2]  \n",
        "            elif vocab.is_note(prev_idx):\n",
        "              temperature = temperatures[1]\n",
        "            elif vocab.is_ins(prev_idx) or prev_idx == vocab.stoi[PAD]:\n",
        "              temperature = temperatures[0]\n",
        "\n",
        "            try:\n",
        "              assert temperature is not None\n",
        "            except:\n",
        "              print(f'Assertion error: prev_idx = {prev_idx}')\n",
        "              raise AssertionError\n",
        "            \n",
        "\n",
        "            # Temperature specific terms \n",
        "            repeat_penalty = max(0, np.log((repeat_count+1)/4)/5) * temperature\n",
        "            temperature += repeat_penalty\n",
        "            if temperature != 1.: logits = logits / temperature\n",
        "                \n",
        "\n",
        "            # Filter\n",
        "            # bar = 16 beats\n",
        "            filter_value = -float('Inf')\n",
        "            if ((last_pos - start_pos) // 16) <= min_bars: logits[vocab.bos_idx] = filter_value\n",
        "\n",
        "            # Call the filter invalid indexes function \n",
        "            logits = filter_invalid_indexes(logits, prev_idx, vocab, filter_value=filter_value, last_xxsep = last_xxsep, allowed_ins = allowed_ins)\n",
        "            logits = top_k_top_p(logits, top_k=top_k, top_p=top_p, filter_value=filter_value)\n",
        "            \n",
        "            # Sample\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            # Update repeat count\n",
        "            num_choices = len(probs.nonzero().view(-1))\n",
        "            if num_choices <= 2: repeat_count += 1\n",
        "            else: repeat_count = repeat_count // 2\n",
        "\n",
        "            if prev_idx==vocab.sep_idx: \n",
        "                duration = idx - vocab.dur_range[0]\n",
        "                last_pos = last_pos + duration\n",
        "\n",
        "                bars_pred = (last_pos - start_pos) // 16\n",
        "                abs_bar = last_pos // 16\n",
        "                # if (bars % 8 == 0) and (bars_pred > min_bars): break\n",
        "                if (i / n_words > 0.80) and (abs_bar % 4 == 0): break\n",
        "\n",
        "\n",
        "            if idx==vocab.bos_idx: \n",
        "                print('Predicted BOS token. Returning prediction...')\n",
        "                break\n",
        "\n",
        "            new_idx.append(idx)\n",
        "            x = x.new_tensor([idx])\n",
        "            pos = pos.new_tensor([last_pos])\n",
        "        #DONE\n",
        "        #pred = vocab.to_music_item(np.array(new_idx))\n",
        "        pred = vocab.to_music_item(np.array(new_idx), item.ins)\n",
        "        full = item.append(pred)\n",
        "        return pred, full\n",
        "    \n",
        "# High level prediction functions from midi file\n",
        "def predict_from_midi(learn, midi=None, n_words=400, \n",
        "                      temperatures=(1.0,1.0), top_k=30, top_p=0.6, seed_len=None, **kwargs):\n",
        "    vocab = learn.data.vocab\n",
        "    seed = MusicItem.from_file(midi, vocab) if not is_empty_midi(midi) else MusicItem.empty(vocab)\n",
        "    if seed_len is not None: seed = seed.trim_to_beat(seed_len)\n",
        "\n",
        "    pred, full = learn.predict(seed, n_words=n_words, temperatures=temperatures, top_k=top_k, top_p=top_p, **kwargs)\n",
        "    return full\n",
        "\n",
        "def filter_invalid_indexes(res, prev_idx, vocab, filter_value=-float('Inf'), last_xxsep = False, allowed_ins:list = None):\n",
        "    #DONE : Hardcoded piano to not be generated at all\n",
        "    #res[[vocab.ins_range[0]]] = filter_value\n",
        "\n",
        "    #DONE : Hardcoded every instrument other than violin to not be generated at all\n",
        "    if allowed_ins is not None:\n",
        "      res[ list( set(range(vocab.ins_range[0], vocab.ins_range[1])) - set([vocab.stoi[x] for x in allowed_ins]) ) ] = filter_value\n",
        "\n",
        "\n",
        "    #If the last predicted note was xxsep, then it should be impossible to predict instrument other than xxni\n",
        "    if last_xxsep is True:\n",
        "      res[list(range(*vocab.ins_range))] = filter_value\n",
        "    else:\n",
        "      res[[vocab.stoi[IN]]] = filter_value\n",
        "\n",
        "    if vocab.is_duration(prev_idx):\n",
        "        res[list(range(*vocab.dur_range))] = filter_value\n",
        "        res[list(range(*vocab.note_range))] = filter_value\n",
        "        res[ list( set([vocab.stoi[x] for x in SPECIAL_TOKS])\n",
        "         - {vocab.stoi[IN]} ) ] = filter_value\n",
        "\n",
        "    elif vocab.is_ins(prev_idx) or prev_idx == vocab.stoi[PAD]:\n",
        "        res[list(range(*vocab.ins_range))] = filter_value\n",
        "        res[list(range(*vocab.dur_range))] = filter_value\n",
        "        res[ list( set([vocab.stoi[x] for x in SPECIAL_TOKS])\n",
        "         - {vocab.stoi[SEP]} ) ] = filter_value   \n",
        "\n",
        "    else:\n",
        "        res[list(range(*vocab.note_range))] = filter_value\n",
        "        res[list(range(*vocab.ins_range))] = filter_value\n",
        "        res[ list( set([vocab.stoi[x] for x in SPECIAL_TOKS]))] = filter_value   \n",
        "        # res[[vocab.ni_idx]] = filter_value\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xf0Zsf0x84j"
      },
      "source": [
        "## **Model declaration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TXqkZpMBRJy"
      },
      "source": [
        "###**config** \n",
        "Change these functions for tweaking model hyper-parameters "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "cellView": "code",
        "id": "NGSIasuVBRBu"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#https://github.com/bearpelican/musicautobot/blob/master/musicautobot/config.py\n",
        "\n",
        "from fastai.text.models.transformer import tfmerXL_lm_config, Activation\n",
        "# from .vocab import MusicVocab\n",
        "\n",
        "# https://github.com/fastai/fastai1/blob/master/fastai/text/models/transformer.py#L175\n",
        "\n",
        "\n",
        "def default_config():\n",
        "    config = tfmerXL_lm_config.copy()\n",
        "    config['act'] = Activation.GeLU\n",
        "\n",
        "    config['mem_len'] = 512\n",
        "    config['d_model'] = 512\n",
        "    config['d_inner'] = 2048\n",
        "    config['n_layers'] = 6\n",
        "    config['n_heads'] = 8\n",
        "    config['d_head'] = 64\n",
        "\n",
        "    return config\n",
        "\n",
        "def music_config():\n",
        "    config = default_config()\n",
        "    config['encode_position'] = True\n",
        "    return config\n",
        "\n",
        "def musicm_config():\n",
        "    config = music_config()\n",
        "    config['d_model'] = 512\n",
        "    config['d_inner'] = 3072\n",
        "    config['n_heads'] = 12\n",
        "    config['d_head'] = 64\n",
        "    config['n_layers'] = 12\n",
        "    return config\n",
        "    \n",
        "def multitask_config():\n",
        "    config = default_config()\n",
        "    config['bias'] = True\n",
        "    config['enc_layers'] = 8\n",
        "    config['dec_layers'] = 8\n",
        "    del config['n_layers']\n",
        "    return config\n",
        "\n",
        "def multitaskm_config():\n",
        "    config = musicm_config()\n",
        "    config['bias'] = True\n",
        "    config['enc_layers'] = 12\n",
        "    config['dec_layers'] = 12\n",
        "    del config['n_layers']\n",
        "    return config\n",
        "\n",
        "def btp_phase1_config():\n",
        "  config = default_config()\n",
        "  config['act'] = Activation.GeLU\n",
        "\n",
        "  config['ctx_len'] = 512\n",
        "  config['d_model'] = 512\n",
        "  config['d_inner'] = 3072\n",
        "  config['n_heads'] = 12\n",
        "  config['d_head'] = 64\n",
        "  config['n_layers'] = 8\n",
        "  config['transpose_range'] = (0, 12)\n",
        "  config['mask_steps'] = 4\n",
        "  config['encode_position'] = False\n",
        "  return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "76MXo1LQSvQC"
      },
      "outputs": [],
      "source": [
        "config = btp_phase1_config()\n",
        "config['transpose_range'] = (0, 12)\n",
        "config['mask_steps'] = 4\n",
        "config['encode_position'] = False\n",
        "encode_position = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1LJKXFEap9YrQ7Md4S38CD5ergr1jRVML"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWgTHkja0CpW",
        "outputId": "b89cc9ad-5526-4722-ca9a-a9856357a7d9"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LJKXFEap9YrQ7Md4S38CD5ergr1jRVML\n",
            "To: /content/lakh_genre_model.pth\n",
            "100% 411M/411M [00:03<00:00, 111MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "jLHF5Dtd3OaF"
      },
      "outputs": [],
      "source": [
        "# load = False\n",
        "load = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "bArZC6jD9DCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7393223-725e-42c7-8979-702735018f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:156: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ],
      "source": [
        "if(load):\n",
        "  learner = music_model_learner(MusicDataBunch.empty(''), config=config.copy(), encode_position = encode_position, pretrained_path = '/content/lakh_genre_model.pth')\n",
        "else:\n",
        "  learner = music_model_learner(data, config=config.copy(), encode_position = encode_position)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq2Iwv72xsDj",
        "outputId": "dc4b28b9-89dc-45c8-9b7b-b6405fc5efe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". := models\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "print(learner.path, ':=', learner.model_dir)\n",
        "print(learner.model[0].encode_position)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "BkTvzY0DqU7x"
      },
      "outputs": [],
      "source": [
        "learner.to_fp16(dynamic=True, clip=0.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJpOhWhq4kIA"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsIqdbdHmJ7T"
      },
      "source": [
        "####Custom callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "YLI2lS2flhUv"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GenSamples(Callback):\n",
        "    learn:Learner\n",
        "    file_name: str #full path of file\n",
        "    n_pred: int #number of words to predict\n",
        "    cutoff_beat: int #number of beats to cut from \n",
        "    out_path : str #output path of file \n",
        "    \n",
        "    def on_epoch_end(self, epoch = -1, **kwargs:Any):\n",
        "      f = self.file_name\n",
        "\n",
        "      item = MusicItem.from_file(f, data.vocab)\n",
        "      seed_item = item.trim_to_beat(self.cutoff_beat)\n",
        "\n",
        "      print('Attempting to generate validation prediction')\n",
        "      pred, full = learner.predict(seed_item, n_words=self.n_pred, temperatures=(1.4,0.8), min_bars=12, top_k=30, top_p=0.7)\n",
        "      try:\n",
        "        #Currently hard-coded to work for generating 1000 pred words by manually eliminating xxbos and xxpad before .to_stream\n",
        "        full.data = full.data[2:(len(full.data) - 1)]\n",
        "        s = full.to_stream()\n",
        "        \n",
        "        s.write('midi', fp = (self.out_path + 'pred_epoch_' + str(epoch) + '.mid'))  \n",
        "      except:\n",
        "        print('Invalid prediction generate at epoch ' + str(epoch))\n",
        "      return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lcnk9iw-RSa"
      },
      "source": [
        "### fit one cycle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "SMx-dlTA_MIu"
      },
      "outputs": [],
      "source": [
        "def calc_net_weight_count(net):\n",
        "  net.train()\n",
        "  net_params = filter(lambda p: p.requires_grad, net.parameters())\n",
        "  weight_count = 0\n",
        "  for param in net_params:\n",
        "    weight_count += np.prod(param.size())\n",
        "  \n",
        "  return weight_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "EuUGcvOUvJ0c"
      },
      "outputs": [],
      "source": [
        "# learner.lr_find()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "aMJleW42vWSE"
      },
      "outputs": [],
      "source": [
        "# learner.recorder.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "EHOexYFH7XvW"
      },
      "outputs": [],
      "source": [
        "# Change no. of epochs accordingly \n",
        "epochs = 50\n",
        "lr = 5e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "kLcFzO2AxMGc"
      },
      "outputs": [],
      "source": [
        "from fastai.callbacks import EarlyStoppingCallback, SaveModelCallback, ReduceLROnPlateauCallback, OneCycleScheduler\n",
        "\n",
        "es_cb = EarlyStoppingCallback(learn = learner, monitor='valid_loss', patience = 5, min_delta = 0.01)\n",
        "save_cb = SaveModelCallback(learn = learner, name='lakh_genre_model')\n",
        "gen_samples_cb = GenSamples(learn = learner, file_name = '/content/drive/MyDrive/datasets/BTP/final_data/blues/budapest-George Ezra-kar_rt.mid', n_pred = 512, cutoff_beat = 32, out_path = '/content/drive/MyDrive/datasets/BTP/prediction/')\n",
        "# lr_cb = ReduceLROnPlateauCallback(learn = learner, patience = 2, min_delta = 0.001)\n",
        "# one_sch_cb =  OneCycleScheduler(learner, lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGr9b83s_SwS",
        "outputId": "7ad89dce-91f9-4097-f282-717699a4cc15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41107268"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "calc_net_weight_count(learner.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "bgfgNx2aThMS"
      },
      "outputs": [],
      "source": [
        "# learner.fit_one_cycle(epochs, lr, callbacks = [es_cb, save_cb, gen_samples_cb] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNk3PXx2jpXk"
      },
      "source": [
        "## **Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_vocab = MusicVocab.create()\n",
        "print(len(data_vocab))"
      ],
      "metadata": {
        "id": "jzLTT8Hh_xLQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f2fdb5a-d0d9-4460-dba2-d918fa2598bb"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose the seed file\n",
        "\n",
        "file_path = \"/content/Undertale_-_Megalovania.mid\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown This field control the lookback of relative attention (default 1024 tokens): \n",
        "men_len = 1024 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Cutoff beat for the seed item\n",
        "cutoff_beat = 64  #@param {type:\"integer\"}\n",
        "\n",
        "learner.model.mem_len = men_len\n",
        "\n",
        "item = MusicItem.from_file(file_path, data_vocab)\n",
        "seed_item = item.trim_to_beat(cutoff_beat)"
      ],
      "metadata": {
        "id": "8Wq5AlLWhd-Z"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose one of the genre \n",
        "# ELECTRONIC = 'xxelec'\n",
        "# FOLK = 'xxfolk'\n",
        "# FUNK = 'xxfunk'\n",
        "# JAZZ = 'xxjazz'\n",
        "# POP = 'xxpop'\n",
        "# ROCK = 'xxrock'"
      ],
      "metadata": {
        "id": "s1ZnnZoapL6Y"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_item.data[0] = data_vocab.stoi[ELECTRONIC]"
      ],
      "metadata": {
        "id": "FS9XLinboXER"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_item.to_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "4pzcQ-fzyQ5v",
        "outputId": "c2c95fce-40d8-46b4-f401-be0cf33a4614"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'xxelec xxpad n62 d2 i0 xxsep d2 xxni n62 d2 i0 xxsep d2 xxni n74 d4 i0 xxsep d4 xxni n69 d6 i0 xxsep d6 xxni n68 d4 i0 xxsep d4 xxni n67 d4 i0 xxsep d4 xxni n65 d4 i0 xxsep d4 xxni n62 d2 i0 xxsep d2 xxni n65 d2 i0 xxsep d2 xxni n67 d2 i0 xxsep d2 xxni n60 d2 i0 xxsep d2 xxni n60 d2 i0 xxsep d2 xxni n74 d4 i0 xxsep d4 xxni n69 d6 i0 xxsep d6 xxni n68 d4 i0 xxsep d4 xxni n67 d4 i0 xxsep d4 xxni n65 d4 i0 xxsep d4 xxni n62 d2 i0 xxsep d2 xxni n65 d2 i0 xxsep d2 xxni n67 d2 i0 xxsep d2 xxni n59 d2 i0 xxsep d2 xxni n59 d2 i0 xxsep d2 xxni n74 d4 i0 xxsep d4 xxni n69 d6 i0 xxsep d6 xxni n68 d4 i0 xxsep d4 xxni n67 d4 i0 xxsep d4 xxni n65 d4 i0 xxsep d4 xxni n62 d2 i0 xxsep d2 xxni n65 d2 i0 xxsep d2 xxni n67 d2 i0 xxsep d2 xxni n58 d2 i0 xxsep d2 xxni n58 d2 i0 xxsep d2 xxni n74 d4 i0 xxsep d4 xxni n69 d6 i0 xxsep d6 xxni n68 d4 i0 xxsep d4 xxni n67 d4 i0 xxsep d4 xxni n65 d4 i0 xxsep d4 xxni n62 d2 i0 xxsep d2 xxni n65 d2 i0 xxsep d2 xxni n67 d2 i0 xxsep d2 xxni n62 d2 i0 n50 d4 i0 xxsep d2 xxni n62 d2 i0 xxsep d2 xxni n74 d4 i0 n50 d4 i0 xxsep d4 xxni n69 d6 i0 n50 d2 i0 xxsep d2 xxni n50 d4 i0 xxsep d4 xxni n68 d4 i0 n50 d4 i0 xxsep d4 xxni n67 d4 i0 n50 d4 i0 xxsep d4 xxni n65 d4 i0 n50 d4 i0 xxsep d4 xxni n62 d2 i0 n50 d2 i0 xxsep d2 xxni n65 d2 i0 n50 d4 i0 xxsep d2 xxni n67 d2 i0 xxsep d2 xxni n60 d2 i0 n48 d4 i0 xxsep d2 xxni n60 d2 i0 xxsep d2 xxni n74 d4 i0 n48 d4 i0 xxsep d4 xxni n69 d6 i0 n48 d2 i0 xxsep d2 xxni n48 d4 i0 xxsep d4 xxni n68 d4 i0 n48 d4 i0 xxsep d4 xxni n67 d4 i0 n48 d4 i0 xxsep d4 xxni n65 d4 i0 n48 d4 i0 xxsep d4 xxni n62 d2 i0 n48 d2 i0 xxsep d2 xxni n65 d2 i0 n48 d4 i0 xxsep d2 xxni n67 d2 i0 xxsep d2 xxni n59 d2 i0 n47 d4 i0 xxsep d2 xxni n59 d2 i0 xxsep d2 xxni n74 d4 i0 n47 d4 i0 xxsep d4 xxni n69 d6 i0 n47 d2 i0 xxsep d2 xxni n47 d4 i0 xxsep d4 xxni n68 d4 i0 n47 d4 i0 xxsep d4 xxni n67 d4 i0 n47 d4 i0 xxsep d4 xxni n65 d4 i0 n47 d4 i0 xxsep d4 xxni n62 d2 i0 n47 d2 i0 xxsep d2 xxni n65 d2 i0 n47 d4 i0 xxsep d2 xxni n67 d2 i0 xxsep d2 xxni n58 d2 i0 n46 d4 i0 xxsep d2 xxni n58 d2 i0 xxsep d2 xxni n74 d4 i0 n46 d4 i0 xxsep d4 xxni n69 d6 i0 n46 d2 i0 xxsep d2 xxni n46 d4 i0 xxsep d4 xxni n68 d4 i0 n48 d4 i0 xxsep d2 xxni n55 d2 i0 xxsep d2 xxni n67 d4 i0 n48 d4 i0 xxsep d2 xxni n55 d2 i0 xxsep d2 xxni n65 d4 i0 n48 d4 i0 xxsep d2 xxni n55 d2 i0 xxsep d2 xxni n62 d2 i0 n48 d2 i0 xxsep d2 xxni n55 d2 i0 n65 d2 i0 n48 d4 i0 xxsep d2 xxni n67 d2 i0 xxsep d2 xxni'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Predict**"
      ],
      "metadata": {
        "id": "VhpDIsJp1Evn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "rbvg9xONOYHZ",
        "outputId": "217c27a5-6c2e-4a0e-b8ce-0e8ddeb335af"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='512' class='' max='512' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [512/512 00:06&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Init prev_idx =  10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:167: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:186: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:187: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        }
      ],
      "source": [
        "pred, full = learner.predict(seed_item, n_words=512, temperatures=(1.6,1.2,1.1), min_bars=12, top_k=20, top_p=0.85, allowed_ins = None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred.to_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "D77H2SJ4BScN",
        "outputId": "007d5040-13e6-4e1c-e712-41919a5e9015"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'n62 d2 i0 n48 d4 i0 xxsep d2 xxni n62 d2 i0 xxsep d2 xxni n69 d5 i0 n50 d2 i0 xxsep d2 xxni n50 d4 i0 xxsep d4 xxni n68 d4 i0 n49 d4 i0 xxsep d4 xxni n67 d4 i0 n50 d4 i0 xxsep d4 xxni n65 d4 i0 n52 d4 i0 xxsep d4 xxni n62 d2 i0 n47 d2 i0 xxsep d2 xxni n66 d2 i0 n50 d2 i0 xxsep d2 xxni n64 d2 i0 n42 d4 i0 xxsep d2 xxni n60 d2 i0 xxsep d2 xxni n67 d5 i0 xxsep d6 xxni n67 d4 i0 n62 d4 i0 n57 d2 i0 xxsep d2 xxni n60 d4 i0 xxsep d4 xxni n60 d4 i0 xxsep d2 xxni n60 d2 i0 xxsep d2 xxni n50 d4 i0 n36 d2 i0 n33 d4 i0 n33 d4 i0 xxsep d2 xxni n60 d2 i0 n57 d2 i0 n41 d4 i0 xxsep d2 xxni n42 d4 i0 n37 d4 i0 xxsep d2 xxni n60 d2 i0 n57 d2 i0 n36 d4 i0 xxsep d2 xxni n60 d2 i0 n57 d2 i0 n38 d4 i0 xxsep d2 xxni n42 d4 i0 xxsep d2 xxni n62 d2 i0 n57 d2 i0 n42 d4 i0 xxsep d2 xxni n60 d2 i0 n57 d2 i0 n38 d4 i0 xxsep d2 xxni n42 d4 i0 xxsep d2 xxni n62 d2 i0 n60 d2 i0 n57 d2 i0 n43 d4 i0 xxsep d2 xxni n42 d4 i0 xxsep d2 xxni n65 d2 i0 n62 d2 i0 n59 d2 i0 n55 d2 i0 xxsep d2 xxni n69 d5 i0 n50 d2 i0 n43 d2 i0 xxsep d2 xxni n50 d4 i0 n38 d4 i0 n30 d4 i0 xxsep d2 xxni n52 d2 i0 n42 d4 i0 xxsep d2 xxni n64 d2 i0 n50 d4 i0 n38 d4 i0 xxsep d2 xxni n66 d2 i0 n60 d2 i0 n57 d2 i0 n38 d4 i0 xxsep d2 xxni n64 d2 i0 n50 d4 i0 n38 d4 i0 xxsep d2 xxni n66 d2 i0 n64 d2 i0 n60 d2 i0 n57 d2 i0 n42 d4 i0 xxsep d2 xxni n64 d2 i0 n54 d2 i0 n42 d4 i0 xxsep d2 xxni n62 d2 i0 n60 d2 i0 n56 d2 i0 n35 d4 i0 xxsep d2 xxni n64 d2 i0 n57 d2 i0 n43 d4 i0 xxsep d2 xxni n67 d3 i0 n64 d2 i0 n59 d3 i0 n55 d3 i0 n36 d4 i0 xxsep d2 xxni n62 d2 i0 n60 d2 i0 n56 d2 i0 n40 d4 i0 xxsep d2 xxni n69 d6 i0 n67 d6 i0 n59 d6 i0 n57 d6 i0 n43 d4 i0 n38 d4 i0 xxsep d2 xxni n67 d2 i0 n64 d2 i0 n62 d2 i0 n59 d2 i0 n50 d4 i0 xxsep d2 xxni n69 d2 i0 n62 d2 i0 n64 d2 i0 n47 d4 i0 xxsep d2 xxni n64 d2 i0 n64 d2 i0 n62 d2 i0 n59 d2 i0 xxsep d2 xxni n67 d5 i0 xxsep d6 xxni n67 d4 i0 n62 d4 i0 n57 d2 i0 xxsep d2 xxni n64 d1 i0 n61 d1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full.to_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "4TFTBkzLxCc7",
        "outputId": "bafa3915-3498-45ec-cf9f-cfbcbe540e4a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'xxelec xxpad n62 d2 i0 xxsep d2 xxni n62 d2 i0 xxsep d2 xxni n74 d4 i0 xxsep d4 xxni n69 d6 i0 xxsep d6 xxni n68 d4 i0 xxsep d4 xxni n67 d4 i0 xxsep d4 xxni n65 d4 i0 xxsep d4 xxni n62 d2 i0 xxsep d2 xxni n65 d2 i0 xxsep d2 xxni n67 d2 i0 xxsep d2 xxni n60 d2 i0 xxsep d2 xxni n60 d2 i0 xxsep d2 xxni n74 d4 i0 xxsep d4 xxni n69 d6 i0 xxsep d6 xxni n68 d4 i0 xxsep d4 xxni n67 d4 i0 xxsep d4 xxni n65 d4 i0 xxsep d4 xxni n62 d2 i0 xxsep d2 xxni n65 d2 i0 xxsep d2 xxni n67 d2 i0 xxsep d2 xxni n59 d2 i0 xxsep d2 xxni n59 d2 i0 xxsep d2 xxni n74 d4 i0 xxsep d4 xxni n69 d6 i0 xxsep d6 xxni n68 d4 i0 xxsep d4 xxni n67 d4 i0 xxsep d4 xxni n65 d4 i0 xxsep d4 xxni n62 d2 i0 xxsep d2 xxni n65 d2 i0 xxsep d2 xxni n67 d2 i0 xxsep d2 xxni n58 d2 i0 xxsep d2 xxni n58 d2 i0 xxsep d2 xxni n74 d4 i0 xxsep d4 xxni n69 d6 i0 xxsep d6 xxni n68 d4 i0 xxsep d4 xxni n67 d4 i0 xxsep d4 xxni n65 d4 i0 xxsep d4 xxni n62 d2 i0 xxsep d2 xxni n65 d2 i0 xxsep d2 xxni n67 d2 i0 xxsep d2 xxni n62 d2 i0 n50 d4 i0 xxsep d2 xxni n62 d2 i0 xxsep d2 xxni n74 d4 i0 n50 d4 i0 xxsep d4 xxni n69 d6 i0 n50 d2 i0 xxsep d2 xxni n50 d4 i0 xxsep d4 xxni n68 d4 i0 n50 d4 i0 xxsep d4 xxni n67 d4 i0 n50 d4 i0 xxsep d4 xxni n65 d4 i0 n50 d4 i0 xxsep d4 xxni n62 d2 i0 n50 d2 i0 xxsep d2 xxni n65 d2 i0 n50 d4 i0 xxsep d2 xxni n67 d2 i0 xxsep d2 xxni n60 d2 i0 n48 d4 i0 xxsep d2 xxni n60 d2 i0 xxsep d2 xxni n74 d4 i0 n48 d4 i0 xxsep d4 xxni n69 d6 i0 n48 d2 i0 xxsep d2 xxni n48 d4 i0 xxsep d4 xxni n68 d4 i0 n48 d4 i0 xxsep d4 xxni n67 d4 i0 n48 d4 i0 xxsep d4 xxni n65 d4 i0 n48 d4 i0 xxsep d4 xxni n62 d2 i0 n48 d2 i0 xxsep d2 xxni n65 d2 i0 n48 d4 i0 xxsep d2 xxni n67 d2 i0 xxsep d2 xxni n59 d2 i0 n47 d4 i0 xxsep d2 xxni n59 d2 i0 xxsep d2 xxni n74 d4 i0 n47 d4 i0 xxsep d4 xxni n69 d6 i0 n47 d2 i0 xxsep d2 xxni n47 d4 i0 xxsep d4 xxni n68 d4 i0 n47 d4 i0 xxsep d4 xxni n67 d4 i0 n47 d4 i0 xxsep d4 xxni n65 d4 i0 n47 d4 i0 xxsep d4 xxni n62 d2 i0 n47 d2 i0 xxsep d2 xxni n65 d2 i0 n47 d4 i0 xxsep d2 xxni n67 d2 i0 xxsep d2 xxni n58 d2 i0 n46 d4 i0 xxsep d2 xxni n58 d2 i0 xxsep d2 xxni n74 d4 i0 n46 d4 i0 xxsep d4 xxni n69 d6 i0 n46 d2 i0 xxsep d2 xxni n46 d4 i0 xxsep d4 xxni n68 d4 i0 n48 d4 i0 xxsep d2 xxni n55 d2 i0 xxsep d2 xxni n67 d4 i0 n48 d4 i0 xxsep d2 xxni n55 d2 i0 xxsep d2 xxni n65 d4 i0 n48 d4 i0 xxsep d2 xxni n55 d2 i0 xxsep d2 xxni n62 d2 i0 n48 d2 i0 xxsep d2 xxni n55 d2 i0 n65 d2 i0 n48 d4 i0 xxsep d2 xxni n67 d2 i0 xxsep d2 xxni n62 d2 i0 n48 d4 i0 xxsep d2 xxni n62 d2 i0 xxsep d2 xxni n69 d5 i0 n50 d2 i0 xxsep d2 xxni n50 d4 i0 xxsep d4 xxni n68 d4 i0 n49 d4 i0 xxsep d4 xxni n67 d4 i0 n50 d4 i0 xxsep d4 xxni n65 d4 i0 n52 d4 i0 xxsep d4 xxni n62 d2 i0 n47 d2 i0 xxsep d2 xxni n66 d2 i0 n50 d2 i0 xxsep d2 xxni n64 d2 i0 n42 d4 i0 xxsep d2 xxni n60 d2 i0 xxsep d2 xxni n67 d5 i0 xxsep d6 xxni n67 d4 i0 n62 d4 i0 n57 d2 i0 xxsep d2 xxni n60 d4 i0 xxsep d4 xxni n60 d4 i0 xxsep d2 xxni n60 d2 i0 xxsep d2 xxni n50 d4 i0 n36 d2 i0 n33 d4 i0 n33 d4 i0 xxsep d2 xxni n60 d2 i0 n57 d2 i0 n41 d4 i0 xxsep d2 xxni n42 d4 i0 n37 d4 i0 xxsep d2 xxni n60 d2 i0 n57 d2 i0 n36 d4 i0 xxsep d2 xxni n60 d2 i0 n57 d2 i0 n38 d4 i0 xxsep d2 xxni n42 d4 i0 xxsep d2 xxni n62 d2 i0 n57 d2 i0 n42 d4 i0 xxsep d2 xxni n60 d2 i0 n57 d2 i0 n38 d4 i0 xxsep d2 xxni n42 d4 i0 xxsep d2 xxni n62 d2 i0 n60 d2 i0 n57 d2 i0 n43 d4 i0 xxsep d2 xxni n42 d4 i0 xxsep d2 xxni n65 d2 i0 n62 d2 i0 n59 d2 i0 n55 d2 i0 xxsep d2 xxni n69 d5 i0 n50 d2 i0 n43 d2 i0 xxsep d2 xxni n50 d4 i0 n38 d4 i0 n30 d4 i0 xxsep d2 xxni n52 d2 i0 n42 d4 i0 xxsep d2 xxni n64 d2 i0 n50 d4 i0 n38 d4 i0 xxsep d2 xxni n66 d2 i0 n60 d2 i0 n57 d2 i0 n38 d4 i0 xxsep d2 xxni n64 d2 i0 n50 d4 i0 n38 d4 i0 xxsep d2 xxni n66 d2 i0 n64 d2 i0 n60 d2 i0 n57 d2 i0 n42 d4 i0 xxsep d2 xxni n64 d2 i0 n54 d2 i0 n42 d4 i0 xxsep d2 xxni n62 d2 i0 n60 d2 i0 n56 d2 i0 n35 d4 i0 xxsep d2 xxni n64 d2 i0 n57 d2 i0 n43 d4 i0 xxsep d2 xxni n67 d3 i0 n64 d2 i0 n59 d3 i0 n55 d3 i0 n36 d4 i0 xxsep d2 xxni n62 d2 i0 n60 d2 i0 n56 d2 i0 n40 d4 i0 xxsep d2 xxni n69 d6 i0 n67 d6 i0 n59 d6 i0 n57 d6 i0 n43 d4 i0 n38 d4 i0 xxsep d2 xxni n67 d2 i0 n64 d2 i0 n62 d2 i0 n59 d2 i0 n50 d4 i0 xxsep d2 xxni n69 d2 i0 n62 d2 i0 n64 d2 i0 n47 d4 i0 xxsep d2 xxni n64 d2 i0 n64 d2 i0 n62 d2 i0 n59 d2 i0 xxsep d2 xxni n67 d5 i0 xxsep d6 xxni n67 d4 i0 n62 d4 i0 n57 d2 i0 xxsep d2 xxni n64 d1 i0 n61 d1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = pred.to_stream()\n",
        "t = full.to_stream()\n",
        "prefix = 'meglovania' #@param {type: \"string\"}\n",
        "s.write('midi', fp= f'/content/{prefix}_pred.mid')  \n",
        "t.write('midi', fp= f'/content/{prefix}_full.mid')  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dGOnV1qnxvPc",
        "outputId": "c994d73f-a73d-4e0e-91d7-47974c0c2be4"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/meglovania_full.mid'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kWTsbbkUwu0C"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}